{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c063f6cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heatmaps showing where a prototype is found will not be generated because OpenCV is not installed.\n"
     ]
    }
   ],
   "source": [
    "from pipnet.pipnet import PIPNet, get_network\n",
    "from util.log import Log\n",
    "import torch.nn as nn\n",
    "from util.args import get_args, save_args, get_optimizer_nn\n",
    "from util.data import get_dataloaders\n",
    "from util.func import init_weights_xavier\n",
    "from pipnet.train import train_pipnet, test_pipnet\n",
    "# from pipnet.test import eval_pipnet, get_thresholds, eval_ood\n",
    "from util.eval_cub_csv import eval_prototypes_cub_parts_csv, get_topk_cub, get_proto_patches_cub\n",
    "import torch\n",
    "from util.vis_pipnet import visualize, visualize_topk\n",
    "from util.visualize_prediction import vis_pred, vis_pred_experiments\n",
    "import sys, os\n",
    "import random\n",
    "import numpy as np\n",
    "from shutil import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "from util.node import Node\n",
    "import shutil\n",
    "from util.phylo_utils import construct_phylo_tree, construct_discretized_phylo_tree\n",
    "import pickle\n",
    "from util.func import get_patch_size\n",
    "import random\n",
    "from util.data import ModifiedLabelLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0b0fc25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------- No discretization -------------------------\n"
     ]
    }
   ],
   "source": [
    "# run_path = '/home/harishbabu/projects/PIPNet/runs/010-CUB-27-imgnet_OOD_cnext26_img=224_nprotos=20'\n",
    "# run_path = '/home/harishbabu/projects/PIPNet/runs/031-CUB-18-imgnet_cnext26_img=224_nprotos=20_orth-on-rel'\n",
    "# run_path = '/home/harishbabu/projects/PIPNet/runs/032-CUB-18-imgnet_cnext26_img=224_nprotos=20_orth-on-rel'\n",
    "run_path = '/home/harishbabu/projects/PIPNet/runs/035-CUB-18-imgnet_OOD_cnext26_img=224_nprotos=20_orth-on-rel'\n",
    "args_file = open(os.path.join(run_path, 'metadata', 'args.pickle'), 'rb')\n",
    "args = pickle.load(args_file)\n",
    "\n",
    "if args.phylo_config:\n",
    "    phylo_config = OmegaConf.load(args.phylo_config)\n",
    "\n",
    "if args.phylo_config:\n",
    "    # construct the phylo tree\n",
    "    if phylo_config.phyloDistances_string == 'None':\n",
    "        if '031' in run_path: # this run uses a different phylogeny file that had an extra root node which is a mistake\n",
    "            root = construct_phylo_tree('/home/harishbabu/data/phlyogenyCUB/18Species-with-extra-root-node/1_tree-consensus-Hacket-18Species-modified_cub-names_v1.phy')\n",
    "        else:\n",
    "            root = construct_phylo_tree(phylo_config.phylogeny_path)\n",
    "        print('-'*25 + ' No discretization ' + '-'*25)\n",
    "    else:\n",
    "        root = construct_discretized_phylo_tree(phylo_config.phylogeny_path, phylo_config.phyloDistances_string)\n",
    "        print('-'*25 + ' Discretized ' + '-'*25)\n",
    "else:\n",
    "    # construct the tree (original hierarchy as described in the paper)\n",
    "    root = Node(\"root\")\n",
    "    root.add_children(['animal','vehicle','everyday_object','weapon','scuba_diver'])\n",
    "    root.add_children_to('animal',['non_primate','primate'])\n",
    "    root.add_children_to('non_primate',['African_elephant','giant_panda','lion'])\n",
    "    root.add_children_to('primate',['capuchin','gibbon','orangutan'])\n",
    "    root.add_children_to('vehicle',['ambulance','pickup','sports_car'])\n",
    "    root.add_children_to('everyday_object',['laptop','sandal','wine_bottle'])\n",
    "    root.add_children_to('weapon',['assault_rifle','rifle'])\n",
    "    # flat root\n",
    "    # root.add_children(['scuba_diver','African_elephant','giant_panda','lion','capuchin','gibbon','orangutan','ambulance','pickup','sports_car','laptop','sandal','wine_bottle','assault_rifle','rifle'])\n",
    "root.assign_all_descendents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70d410eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      "\t052+053\n",
      "\t\tcub_052_Pied_billed_Grebe\n",
      "\t\t053+050\n",
      "\t\t\tcub_053_Western_Grebe\n",
      "\t\t\t050+051\n",
      "\t\t\t\tcub_050_Eared_Grebe\n",
      "\t\t\t\tcub_051_Horned_Grebe\n",
      "\t004+086\n",
      "\t\t004+032\n",
      "\t\t\tcub_004_Groove_billed_Ani\n",
      "\t\t\t032+033\n",
      "\t\t\t\tcub_032_Mangrove_Cuckoo\n",
      "\t\t\t\t033+031\n",
      "\t\t\t\t\tcub_033_Yellow_billed_Cuckoo\n",
      "\t\t\t\t\tcub_031_Black_billed_Cuckoo\n",
      "\t\t086+045\n",
      "\t\t\tcub_086_Pacific_Loon\n",
      "\t\t\t045+101\n",
      "\t\t\t\t045+003\n",
      "\t\t\t\t\tcub_045_Northern_Fulmar\n",
      "\t\t\t\t\t003+002\n",
      "\t\t\t\t\t\tcub_003_Sooty_Albatross\n",
      "\t\t\t\t\t\t002+001\n",
      "\t\t\t\t\t\t\tcub_002_Laysan_Albatross\n",
      "\t\t\t\t\t\t\tcub_001_Black_footed_Albatross\n",
      "\t\t\t\t101+023\n",
      "\t\t\t\t\t101+100\n",
      "\t\t\t\t\t\tcub_101_White_Pelican\n",
      "\t\t\t\t\t\tcub_100_Brown_Pelican\n",
      "\t\t\t\t\t023+025\n",
      "\t\t\t\t\t\tcub_023_Brandt_Cormorant\n",
      "\t\t\t\t\t\t025+024\n",
      "\t\t\t\t\t\t\tcub_025_Pelagic_Cormorant\n",
      "\t\t\t\t\t\t\tcub_024_Red_faced_Cormorant\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f055a7da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num classes (k) =  18 ['cub_001_Black_footed_Albatross', 'cub_002_Laysan_Albatross', 'cub_003_Sooty_Albatross', 'cub_004_Groove_billed_Ani', 'cub_023_Brandt_Cormorant'] etc.\n",
      "Classes:  {'cub_001_Black_footed_Albatross': 0, 'cub_002_Laysan_Albatross': 1, 'cub_003_Sooty_Albatross': 2, 'cub_004_Groove_billed_Ani': 3, 'cub_023_Brandt_Cormorant': 4, 'cub_024_Red_faced_Cormorant': 5, 'cub_025_Pelagic_Cormorant': 6, 'cub_031_Black_billed_Cuckoo': 7, 'cub_032_Mangrove_Cuckoo': 8, 'cub_033_Yellow_billed_Cuckoo': 9, 'cub_045_Northern_Fulmar': 10, 'cub_050_Eared_Grebe': 11, 'cub_051_Horned_Grebe': 12, 'cub_052_Pied_billed_Grebe': 13, 'cub_053_Western_Grebe': 14, 'cub_086_Pacific_Loon': 15, 'cub_100_Brown_Pelican': 16, 'cub_101_White_Pelican': 17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harishbabu/.conda/envs/hpnet1/lib/python3.8/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of prototypes:  20\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Node' object has no attribute 'num_descendants'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClasses: \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mstr\u001b[39m(classes), flush\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Create a convolutional network based on arguments and add 1x1 conv layer\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m feature_net, add_on_layers, pool_layer, classification_layers, num_prototypes \u001b[38;5;241m=\u001b[39m \u001b[43mget_network\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mclasses\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mroot\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Create a PIP-Net\u001b[39;00m\n\u001b[1;32m     26\u001b[0m net \u001b[38;5;241m=\u001b[39m PIPNet(num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(classes),\n\u001b[1;32m     27\u001b[0m                     num_prototypes\u001b[38;5;241m=\u001b[39mnum_prototypes,\n\u001b[1;32m     28\u001b[0m                     feature_net \u001b[38;5;241m=\u001b[39m feature_net,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     34\u001b[0m                     root \u001b[38;5;241m=\u001b[39m root\n\u001b[1;32m     35\u001b[0m                     )\n",
      "File \u001b[0;32m~/projects/PIPNet/pipnet/pipnet.py:209\u001b[0m, in \u001b[0;36mget_network\u001b[0;34m(num_classes, args, root)\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;66;03m# for classifying leaf nodes from parent node prototypes\u001b[39;00m\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m node\u001b[38;5;241m.\u001b[39mnum_children() \u001b[38;5;241m!=\u001b[39m node\u001b[38;5;241m.\u001b[39mnum_descendents():\n\u001b[0;32m--> 209\u001b[0m         classification_layers[node\u001b[38;5;241m.\u001b[39mname\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_descendant\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m NonNegLinear(num_prototypes, \u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_descendants\u001b[49m(), bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m features, add_on_layers, pool_layer, classification_layers, num_prototypes\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Node' object has no attribute 'num_descendants'"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    device_ids = [torch.cuda.current_device()]\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    device_ids = []\n",
    "\n",
    "args_file = open(os.path.join(run_path, 'metadata', 'args.pickle'), 'rb')\n",
    "args = pickle.load(args_file)\n",
    "\n",
    "ckpt_path = os.path.join(run_path, 'checkpoints', 'net_trained_last')\n",
    "checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "\n",
    "# Obtain the dataset and dataloaders\n",
    "trainloader, trainloader_pretraining, trainloader_normal, trainloader_normal_augment, projectloader, testloader, test_projectloader, classes = get_dataloaders(args, device)\n",
    "if len(classes)<=20:\n",
    "    if args.validation_size == 0.:\n",
    "        print(\"Classes: \", testloader.dataset.class_to_idx, flush=True)\n",
    "    else:\n",
    "        print(\"Classes: \", str(classes), flush=True)\n",
    "\n",
    "# Create a convolutional network based on arguments and add 1x1 conv layer\n",
    "feature_net, add_on_layers, pool_layer, classification_layers, num_prototypes = get_network(len(classes), args, root=root)\n",
    "   \n",
    "# Create a PIP-Net\n",
    "net = PIPNet(num_classes=len(classes),\n",
    "                    num_prototypes=num_prototypes,\n",
    "                    feature_net = feature_net,\n",
    "                    args = args,\n",
    "                    add_on_layers = add_on_layers,\n",
    "                    pool_layer = pool_layer,\n",
    "                    classification_layers = classification_layers,\n",
    "                    num_parent_nodes = len(root.nodes_with_children()),\n",
    "                    root = root\n",
    "                    )\n",
    "net = net.to(device=device)\n",
    "net = nn.DataParallel(net, device_ids = device_ids)    \n",
    "net.load_state_dict(checkpoint['model_state_dict'],strict=True)\n",
    "net.eval()\n",
    "criterion = nn.NLLLoss(reduction='mean').to(device)\n",
    "\n",
    "# Forward one batch through the backbone to get the latent output size\n",
    "# with torch.no_grad():\n",
    "#     xs1, _, _ = next(iter(trainloader))\n",
    "#     xs1 = xs1.to(device)\n",
    "#     proto_features, _, _ = net(xs1)\n",
    "#     wshape = proto_features['root'].shape[-1]\n",
    "#     args.wshape = wshape #needed for calculating image patch size\n",
    "#     print(\"Output shape: \", proto_features['root'].shape, flush=True)\n",
    "    \n",
    "args.wshape = 26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "620e9099",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_patch_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m     heapq\u001b[38;5;241m.\u001b[39mheapify(list_)\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m list_\n\u001b[0;32m---> 21\u001b[0m patchsize, skip \u001b[38;5;241m=\u001b[39m \u001b[43mget_patch_size\u001b[49m(args)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# node = root.get_node('045+101')\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# proto_idx = 1 #8\u001b[39;00m\n\u001b[1;32m     26\u001b[0m node \u001b[38;5;241m=\u001b[39m root\u001b[38;5;241m.\u001b[39mget_node(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m004+086\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_patch_size' is not defined"
     ]
    }
   ],
   "source": [
    "# Proto activations on leaf descendents - topk images\n",
    "\n",
    "from util.data import ModifiedLabelLoader\n",
    "from collections import defaultdict\n",
    "import heapq\n",
    "import pdb\n",
    "from util.vis_pipnet import get_img_coordinates\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image, ImageDraw as D\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "topk = 10\n",
    "save_images = False\n",
    "\n",
    "def get_heap():\n",
    "    list_ = []\n",
    "    heapq.heapify(list_)\n",
    "    return list_\n",
    "\n",
    "patchsize, skip = get_patch_size(args)\n",
    "\n",
    "# node = root.get_node('045+101')\n",
    "# proto_idx = 1 #8\n",
    "\n",
    "node = root.get_node('004+086')\n",
    "proto_idx = 17 #8\n",
    "\n",
    "non_leaf_children_names = [child.name for child in node.children if not child.is_leaf()]\n",
    "\n",
    "name2label = projectloader.dataset.class_to_idx\n",
    "label2name = {label:name for name, label in name2label.items()}\n",
    "modifiedLabelLoader = ModifiedLabelLoader(projectloader, node)\n",
    "coarse_label2name = modifiedLabelLoader.modifiedlabel2name\n",
    "node_label_to_children = {label: name for name, label in node.children_to_labels.items()}\n",
    "\n",
    "imgs = modifiedLabelLoader.filtered_imgs\n",
    "\n",
    "img_iter = tqdm(enumerate(modifiedLabelLoader),\n",
    "                total=len(modifiedLabelLoader),\n",
    "                mininterval=50.,\n",
    "                desc='Collecting topk',\n",
    "                ncols=0)\n",
    "\n",
    "classification_weights = getattr(net.module, '_'+node.name+'_classification').weight\n",
    "\n",
    "# maps proto_number -> grand_child_name (or descendant leaf name) -> list of top-k activations\n",
    "proto_mean_activations = defaultdict(lambda: defaultdict(get_heap))\n",
    "\n",
    "# maps class names to the prototypes that belong to that\n",
    "class_and_prototypes = defaultdict(set)\n",
    "\n",
    "for i, (xs, orig_y, ys) in img_iter:\n",
    "    if coarse_label2name[ys.item()] not in non_leaf_children_names:\n",
    "        continue\n",
    "\n",
    "    xs, ys = xs.to(device), ys.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        softmaxes, pooled, _ = net(xs, inference=False)\n",
    "        pooled = pooled[node.name].squeeze(0) \n",
    "        softmaxes = softmaxes[node.name]#.squeeze(0)\n",
    "\n",
    "        for p in range(pooled.shape[0]): # pooled.shape -> [768] (== num of prototypes)\n",
    "            c_weight = torch.max(classification_weights[:,p]) # classification_weights[:,p].shape -> [200] (== num of classes)\n",
    "            relevant_proto_classes = torch.nonzero(classification_weights[:, p] > 1e-3)\n",
    "            relevant_proto_class_names = [node_label_to_children[class_idx.item()] for class_idx in relevant_proto_classes]\n",
    "\n",
    "            # Take the max per prototype.                             \n",
    "            max_per_prototype, max_idx_per_prototype = torch.max(softmaxes, dim=0)\n",
    "            max_per_prototype_h, max_idx_per_prototype_h = torch.max(max_per_prototype, dim=1)\n",
    "            max_per_prototype_w, max_idx_per_prototype_w = torch.max(max_per_prototype_h, dim=1) #shape (num_prototypes)\n",
    "\n",
    "            h_idx = max_idx_per_prototype_h[p, max_idx_per_prototype_w[p]]\n",
    "            w_idx = max_idx_per_prototype_w[p]\n",
    "\n",
    "            if len(relevant_proto_class_names) == 0:\n",
    "                continue\n",
    "\n",
    "            if (len(relevant_proto_class_names) == 1) and (relevant_proto_class_names[0] not in non_leaf_children_names):\n",
    "                continue\n",
    "\n",
    "            h_coor_min, h_coor_max, w_coor_min, w_coor_max = get_img_coordinates(args.image_size, softmaxes.shape, patchsize, skip, h_idx, w_idx)\n",
    "\n",
    "            if (coarse_label2name[ys.item()] in relevant_proto_class_names):\n",
    "                child_node = root.get_node(coarse_label2name[ys.item()])\n",
    "                leaf_descendent = label2name[orig_y.item()][4:7]\n",
    "                img_to_open = imgs[i][0] # it is a tuple of (path to image, lable)\n",
    "                img_tensor = xs\n",
    "                if topk and (len(proto_mean_activations[p][leaf_descendent]) > topk):\n",
    "                    heapq.heappushpop(proto_mean_activations[p][leaf_descendent], (pooled[p].item(), img_to_open, img_tensor, (h_coor_min, h_coor_max, w_coor_min, w_coor_max)))\n",
    "                else:\n",
    "                    heapq.heappush(proto_mean_activations[p][leaf_descendent], (pooled[p].item(), img_to_open, img_tensor, (h_coor_min, h_coor_max, w_coor_min, w_coor_max)))\n",
    "\n",
    "            class_and_prototypes[', '.join(relevant_proto_class_names)].add(p)\n",
    "\n",
    "\n",
    "print('Node', node.name)\n",
    "\n",
    "contributions = []\n",
    "\n",
    "for leaf_descendent in proto_mean_activations[proto_idx]:\n",
    "    # [0] coz only taking the top most image from each descendent\n",
    "    activation, img_to_open, img_tensor, (h_coor_min, h_coor_max, w_coor_min, w_coor_max) = proto_mean_activations[proto_idx][leaf_descendent][0] \n",
    "    \n",
    "    features = net.module._net(img_tensor)\n",
    "    \n",
    "    proto_features = getattr(net.module, '_'+node.name+'_add_on')(features)\n",
    "    softmaxes = net.module._softmax(proto_features)\n",
    "    \n",
    "    # Take the max per prototype.                             \n",
    "    max_per_prototype, max_idx_per_prototype = torch.max(softmaxes, dim=0)\n",
    "    max_per_prototype_h, max_idx_per_prototype_h = torch.max(max_per_prototype, dim=1)\n",
    "    max_per_prototype_w, max_idx_per_prototype_w = torch.max(max_per_prototype_h, dim=1) #shape (num_prototypes)\n",
    "    h_idx = max_idx_per_prototype_h[proto_idx, max_idx_per_prototype_w[proto_idx]]\n",
    "    w_idx = max_idx_per_prototype_w[proto_idx]\n",
    "    \n",
    "    proto_vector = getattr(net.module, '_'+node.name+'_add_on').weight[proto_idx]\n",
    "    feature_patch = features[:, :, h_idx, w_idx].squeeze()\n",
    "    proto_vector = proto_vector.squeeze()\n",
    "    # proto_vector_bias = getattr(net.module, '_'+node.name+'_add_on').bias[proto_idx]\n",
    "    mul = torch.dot(feature_patch.squeeze(), proto_vector.squeeze()) # feature_patch @ proto_vector\n",
    "    # proto_features[:, proto_idx, h_idx, w_idx]\n",
    "    # print((mul+proto_vector_bias) == proto_features[:, proto_idx, h_idx, w_idx])\n",
    "    contributions.append(feature_patch * proto_vector)\n",
    "    \n",
    "    # print(leaf_descendent)\n",
    "    \n",
    "    # Plot the tensor as an image\n",
    "    # plot_tensor = (feature_patch * proto_vector).view(1, -1).repeat(100, 1).detach().cpu().numpy()\n",
    "    c = (feature_patch * proto_vector)\n",
    "    c = (c - c.min()) / (c.max() - c.min())\n",
    "    plot_tensor = torch.repeat_interleave(c.view(1, -1).repeat(100, 1), 2, dim=1).detach().cpu().numpy() # .repeat(100, 1)\n",
    "    fig = plt.figure(figsize=(12, 5))\n",
    "    plt.imshow(plot_tensor, cmap='cool') # jet, hot, plasma\n",
    "    plt.title(leaf_descendent + \" \" + str(mul.item()))\n",
    "    plt.axis('off')  # Turn off the axis labels\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "#     mean_activation = round(np.mean([activation for activation, *_ in proto_mean_activations[p][leaf_descendent]]), 4)\n",
    "#     num_images = len(proto_mean_activations[p][leaf_descendent])\n",
    "#     logstr += f'{leaf_descendent}:({mean_activation}) '\n",
    "# print(logstr)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c054c2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "            if save_images:\n",
    "                patches = []\n",
    "                right_descriptions = []\n",
    "                text_region_width = 7 # 7x the width of a patch\n",
    "                for leaf_descendent, heap in proto_mean_activations[p].items():\n",
    "                    heap = sorted(heap)[::-1]\n",
    "                    mean_activation = round(np.mean([activation for activation, *_ in proto_mean_activations[p][leaf_descendent]]), 4)\n",
    "                    for ele in heap:\n",
    "                        activation, img_to_open, img_tensor, (h_coor_min, h_coor_max, w_coor_min, w_coor_max) = ele\n",
    "                        image = transforms.Resize(size=(args.image_size, args.image_size))(Image.open(img_to_open))\n",
    "                        img_tensor = transforms.ToTensor()(image)#.unsqueeze_(0) #shape (1, 3, h, w)\n",
    "                        img_tensor_patch = img_tensor[:, h_coor_min:h_coor_max, w_coor_min:w_coor_max]\n",
    "                        patches.append(img_tensor_patch)\n",
    "\n",
    "                    # description on the right hand side\n",
    "                    text = f'{mean_activation}, {leaf_descendent}'\n",
    "                    txtimage = Image.new(\"RGB\", (patches[0].shape[-2]*text_region_width,patches[0].shape[-1]), (0, 0, 0))\n",
    "                    draw = D.Draw(txtimage)\n",
    "                    draw.text((5, patches[0].shape[1]//2), text, anchor='mm', fill=\"white\")\n",
    "                    txttensor = transforms.ToTensor()(txtimage)#.unsqueeze_(0)\n",
    "                    right_descriptions.append(txttensor)\n",
    "\n",
    "                grid = torchvision.utils.make_grid(patches, nrow=topk+1, padding=1)\n",
    "                grid_right_descriptions = torchvision.utils.make_grid(right_descriptions, nrow=1, padding=1)\n",
    "\n",
    "                # merging right description with the grid of images\n",
    "                grid = torch.cat([grid, grid_right_descriptions], dim=-1)\n",
    "\n",
    "                # description on the top\n",
    "                text = f'Node:{node.name}, p{p}, Child:{child_classname}'\n",
    "                txtimage = Image.new(\"RGB\", (grid.shape[-1], args.wshape), (0, 0, 0))\n",
    "                draw = D.Draw(txtimage)\n",
    "                draw.text((5, patches[0].shape[1]//2), text, anchor='mm', fill=\"white\")\n",
    "                txttensor = transforms.ToTensor()(txtimage)#.unsqueeze_(0)\n",
    "\n",
    "                # merging top description with the grid of images\n",
    "                grid = torch.cat([grid, txttensor], dim=1)\n",
    "\n",
    "                os.makedirs(os.path.join(run_path, 'descendent_specific_topk', node.name), exist_ok=True)\n",
    "                torchvision.utils.save_image(grid, os.path.join(run_path, 'descendent_specific_topk', node.name, f'{child_classname}-p{p}.png'))\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
