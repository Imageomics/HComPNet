{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_path = '/home/harishbabu/projects/PIPNet/runs/010-CUB-27-imgnet_OOD_cnext26_img=224_nprotos=20'\n",
    "# run_path = '/home/harishbabu/projects/PIPNet/runs/031-CUB-18-imgnet_cnext26_img=224_nprotos=20_orth-on-rel'\n",
    "# run_path = '/home/harishbabu/projects/PIPNet/runs/032-CUB-18-imgnet_cnext26_img=224_nprotos=20_orth-on-rel'\n",
    "\n",
    "# run_path = '/home/harishbabu/projects/PIPNet/runs/035-CUB-18-imgnet_OOD_cnext26_img=224_nprotos=20_orth-on-rel'\n",
    "\n",
    "# run_path = '/home/harishbabu/projects/PIPNet/runs/043-035_clone-CUB-18-imgnet_OOD_cnext26_img=224_nprotos=20_orth-on-rel'\n",
    "# run_path = \"/home/harishbabu/projects/PIPNet/runs/036-CUB-18-imgnet_OOD_cnext26_img=224_nprotos=20_orth-on-rel_uniformity\"\n",
    "# run_path = \"/home/harishbabu/projects/PIPNet/runs/041-035_clone-CUB-18-imgnet_OOD_cnext26_img=224_nprotos=20_orth-on-rel\"\n",
    "# run_path = \"/home/harishbabu/projects/PIPNet/runs/042-035_clone-CUB-18-imgnet_OOD_cnext26_img=224_nprotos=20_orth-on-rel\"\n",
    "\n",
    "# run_path = \"/home/harishbabu/projects/PIPNet/runs/044-CUB-18-imgnet_OOD_cnext26_img=224_nprotos=20-or-4per-desc_orth-on-rel\"\n",
    "\n",
    "# run_path = \"/home/harishbabu/projects/PIPNet/runs/046-CUB-18-imgnet_OOD_cnext26_img=224_nprotos=10per-desc_orth-on-rel\"\n",
    "# run_path = \"/home/harishbabu/projects/PIPNet/runs/047-CUB-18-imgnet_OOD_cnext26_img=224_nprotos=5per-desc_tanh-desc\"\n",
    "# run_path = \"/home/harishbabu/projects/PIPNet/runs/048-CUB-18-imgnet_OOD_cnext26_img=224_nprotos=5per-desc_tanh-desc_unit-sphere\"\n",
    "# run_path = \"/home/harishbabu/projects/PIPNet/runs/051-CUB-18-imgnet_cnext26_img=224_nprotos=4per-desc_tanh-desc_unit-sphere_AW=5-TW=2-UW=2-CW=2\"\n",
    "# run_path = \"/home/harishbabu/projects/PIPNet/runs/052-CUB-18-imgnet_OOD_cnext26_img=224_nprotos=4per-desc_tanh-desc_unit-sphere_AW=5-TW=2-UW=2-CW=2\"\n",
    "# run_path = \"/home/harishbabu/projects/PIPNet/runs/055-CUB-18_cnext26_img=224_nprotos=4per-desc_unit-sphere_no-softmax_AW=3-TW=2-UW=3-CW=2\"\n",
    "# run_path = \"/home/harishbabu/projects/PIPNet/runs/056-CUB-18-imgnet_cnext26_img=224_nprotos=4per-desc_unit-sphere_no-softmax_AW=3-TW=2-UW=3-CW=2\"\n",
    "# run_path = \"/home/harishbabu/projects/PIPNet/runs/057-CUB-18-imgnet_cnext26_img=224_nprotos=4per-desc_unit-sphere_no-meanpool_no-softmax_AW=3-TW=2-UW=3-CW=2\"\n",
    "# run_path = \"/home/harishbabu/projects/PIPNet/runs/058-CUB-18-imgnet_with-equalize-aug_cnext26_img=224_nprotos=4per-desc_unit-sphere_no-meanpool_no-softmax_AW=3-TW=2-UW=3-CW=2\"\n",
    "\n",
    "# with unit sphere\n",
    "# run_path = \"/home/harishbabu/projects/PIPNet/runs/059-CUB-18-imgnet_with-equalize-aug_cnext26_img=224_nprotos=4per-desc_unit-sphere_finetune=5_no-meanpool_no-softmax_AW=3-TW=2-UW=3-CW=2_batch=20\"\n",
    "\n",
    "# unit sphere with softmax\n",
    "# run_path = \"/home/harishbabu/projects/PIPNet/runs/065-CUB-18-imgnet_with-equalize-aug_cnext26_img=224_nprotos=4per-desc_unit-sphere_finetune=5_no-meanpool_with-softmax_AW=3-TW=2-UW=3-CW=2_batch=20\"\n",
    "\n",
    "# original hpipnet with 20 protos per node no KO, no OOD, no tanh-desc\n",
    "run_path = \"/home/harishbabu/projects/PIPNet/runs/062-CUB-18-imgnet_with-equalize-aug_cnext26_img=224_nprotos=20_no-KO_no-OOD\"\n",
    "\n",
    "# original hpipnet with 20 protos per node no KO, no OOD, WITH tanh-desc\n",
    "# run_path = \"/home/harishbabu/projects/PIPNet/runs/063-CUB-18-imgnet_with-equalize-aug_cnext26_img=224_nprotos=20_no-KO_no-OOD_tanh-desc\"\n",
    "\n",
    "# with unit sphere but no AL+UNI\n",
    "# run_path = \"/home/harishbabu/projects/PIPNet/runs/066-CUB-18-imgnet_with-equalize-aug_cnext26_img=224_nprotos=4per-desc_unit-sphere_finetune=5_no-meanpool_no-softmax_no-align_no-uni_AW=3-TW=2-UW=3-CW=2_batch=20\"\n",
    "\n",
    "# with unit sphere, protopool, with softmax, no tanh-desc\n",
    "# run_path = \"/home/harishbabu/projects/PIPNet/runs/067-CUB-18-imgnet_with-equalize-aug_cnext26_img=224_nprotos=4per-desc_unit-sphere-protopool_finetune=5_no-meanpool_with-softmax_AW=3-TW=2-UW=3-CW=2_batch=20\"\n",
    "\n",
    "# with unit sphere, protopool, with softmax, no tanh-desc, INCORRECT\n",
    "# run_path = \"/home/harishbabu/projects/PIPNet/runs/067-incorrect-CUB-18-imgnet_with-equalize-aug_cnext26_img=224_nprotos=4per-desc_unit-sphere-protopool_finetune=5_no-meanpool_with-softmax_AW=3-TW=2-UW=3-CW=2_batch=20\"\n",
    "\n",
    "# with unit sphere, protopool, no softmax, no tanh-desc\n",
    "# run_path = \"/home/harishbabu/projects/PIPNet/runs/068-CUB-18-imgnet_with-equalize-aug_cnext26_img=224_nprotos=4per-desc_unit-sphere-protopool_finetune=5_no-meanpool_no-softmax_AW=3-TW=2-UW=3-CW=2_batch=20\"\n",
    "\n",
    "# 071 with bias\n",
    "# run_path = \"/home/harishbabu/projects/PIPNet/runs/071-CUB-18-imgnet_with-equalize-aug_cnext26_img=224_nprotos=4per-desc_unit-sphere-protopool_finetune=5_no-meanpool_with-softmax_with-addon-bias_AW=3-TW=2-UW=3-CW=2_batch=20\"\n",
    "\n",
    "# 072 gumbel softmax\n",
    "# run_path = \"/home/harishbabu/projects/PIPNet/runs/072-CUB-18-imgnet_with-equalize-aug_cnext26_img=224_nprotos=4per-desc_unit-sphere-protopool_finetune=5_no-meanpool_with-gumbel-softmax_no-addon-bias_AW=3-TW=2-UW=3-CW=2_batch=20\"\n",
    "\n",
    "# 073 gumbel softmax, tau-1.0\n",
    "# run_path = \"/home/harishbabu/projects/PIPNet/runs/073-CUB-18-imgnet_with-equalize-aug_cnext26_img=224_nprotos=4per-desc_unit-sphere-protopool_finetune=5_no-meanpool_with-gumbel-softmax-tau=1_no-addon-bias_AW=3-TW=2-UW=3-CW=2_batch=20\"\n",
    "\n",
    "# 075 068 with focal loss\n",
    "# run_path = \"/home/harishbabu/projects/PIPNet/runs/075-068-with-focal_CUB-18-imgnet_with-equalize-aug_cnext26_img=224_nprotos=4per-desc_unit-sphere-protopool_finetune=5_no-meanpool_no-softmax_no-addon-bias_AW=3-TW=2-UW=3-CW=2_batch=20\"\n",
    "\n",
    "# 076 cs followed by softmax. Uses align_pf along with align+uni\n",
    "# run_path = \"/home/harishbabu/projects/PIPNet/runs/076_CUB-18-imgnet_with-equalize-aug_cnext26_img=224_nprotos=4per-desc_unit-sphere-protopool_finetune=5_align-pf-during-training_no-meanpool_no-softmax_no-addon-bias_AW=3-TW=2-UW=3-CW=2-APW=5_batch=20\"\n",
    "\n",
    "# 074 multiply_cs_softmax\n",
    "# run_path = \"/home/harishbabu/projects/PIPNet/runs/074-CUB-18-imgnet_with-equalize-aug_cnext26_img=224_nprotos=4per-desc_unit-sphere-protopool_finetune=5_no-meanpool_with-softmax_multi-cs-softmax_no-addon-bias_AW=3-TW=2-UW=3-CW=2_batch=20\"\n",
    "\n",
    "# 077 unit sphere protopool with cosin no softmax constant 20 protos per node\n",
    "# run_path = \"/home/harishbabu/projects/PIPNet/runs/077_CUB-18-imgnet_with-equalize-aug_cnext26_img=224_nprotos=20-sphere-protopool_finetune=5_align-pf-during-training_no-meanpool_no-softmax_no-addon-bias_AW=3-TW=2-UW=3-CW=2_batch=20\"\n",
    "\n",
    "# 082 unit sphere cs followed by softmax with minmazimize loss\n",
    "# run_path = \"/home/harishbabu/projects/PIPNet/runs/082-CUB-18-imgnet_with-equalize-aug_cnext26_img=224_nprotos=4per-leaf-desc_unit-sphere_finetune=5_no-meanpool_with-softmax_no-addon-bias_AW=3-TW=2-MMW=2-UW=3-CW=2_mm-loss_batch=48\"\n",
    "\n",
    "# 083 unit sphere cs followed by softmax with minmazimize loss\n",
    "# run_path = \"/home/harishbabu/projects/PIPNet/runs/083-CUB-18-imgnet_with-equalize-aug_cnext26_img=224_nprotos=4per-leaf-desc_unit-sphere_finetune=5_no-meanpool_with-softmax_no-addon-bias_AW=3-TW=2-MMW=2-UW=3-CW=2_no-align_no-uni_no-mm-loss_batch=48\"\n",
    "\n",
    "# 085 unit sphere cs followed by softmax-with-tau with minmazimize loss\n",
    "# run_path = \"/home/harishbabu/projects/PIPNet/runs/085-notebook-CUB-18-imgnet_with-equalize-aug_cnext26_img=224_nprotos=4per-leaf-desc_unit-sphere_finetune=5_no-meanpool_with-softmax-tau=0.2_no-addon-bias_AW=3-TW=2-MMW=2-UW=3-CW=2_mm-loss_batch=12\"\n",
    "\n",
    "# 091 basic gaussian multiplier on stage 4\n",
    "# run_path = \"/home/harishbabu/projects/PIPNet/runs/091-CUB-18-imgnet_with-equalize-aug_cnext26_BGM=4|1.0|50_img=224_latent-dim=256_nprotos=4per-leaf-desc_unit-sphere_finetune=5_no-meanpool_with-softmax-tau=0.2_no-addon-bias_AW=3-TW=2-MMW=2-UW=3-CW=2_mm-loss_batch=48\"\n",
    "\n",
    "# 092 basic gaussian multiplier on stage 3, 4\n",
    "# run_path = \"/home/harishbabu/projects/PIPNet/runs/092-CUB-18-imgnet_with-equalize-aug_cnext26_BGM=3,4|1.0|50_img=224_latent-dim=256_nprotos=4per-leaf-desc_unit-sphere_finetune=5_no-meanpool_with-softmax-tau=0.2_no-addon-bias_AW=3-TW=2-MMW=2-UW=3-CW=2_mm-loss_batch=48\"\n",
    "\n",
    "# 093 128 dim linear\n",
    "# run_path = \"/home/harishbabu/projects/PIPNet/runs/093-CUB-18-imgnet_with-equalize-aug_cnext26_BGM=4|1.0|50_img=224_latent-dim=128_nprotos=20_unit-sphere_finetune=5_no-meanpool_with-softmax-tau=0.2_no-addon-bias_AW=3-TW=2-MMW=2-UW=1-CW=2_mm-loss_batch=48\"\n",
    "\n",
    "# 094 128 dim linear\n",
    "# run_path = \"/home/harishbabu/projects/PIPNet/runs/094-CUB-18-imgnet_with-equalize-aug_cnext26_BGM=4|1.0|50_img=224_latent-dim=128_nprotos=20_unit-sphere_finetune=5_no-meanpool_with-softmax-tau=0.2_no-addon-bias_AW=3-TW=2-MMW=2-UW=1-CW=2_mm-loss_batch=48\"\n",
    "\n",
    "# 095 ablation 091 without AL+UNI\n",
    "# run_path = \"/home/harishbabu/projects/PIPNet/runs/095-091-woALUNI-CUB-18-imgnet_with-equalize-aug_cnext26_BGM=4|1.0|50_img=224_nprotos=4per-leaf-desc_unit-sphere_finetune=5_no-meanpool_with-softmax-tau=0.2_no-addon-bias_AW=3-TW=2-MMW=2-UW=3-CW=2_no-AL_no-UNI_mm-loss_batch=48\"\n",
    "\n",
    "# 096 ablation 091 without AL+UNI\n",
    "# run_path = \"/home/harishbabu/projects/PIPNet/runs/096-091-wfocal-CUB-18-imgnet_with-equalize-aug_cnext26_BGM=4|1.0|50_img=224_nprotos=4per-leaf-desc_unit-sphere_finetune=5_no-meanpool_with-softmax-tau=0.2_no-addon-bias_AW=3-TW=2-MMW=2-UW=3-CW=2_mm-loss_batch=48\"\n",
    "\n",
    "# 097 - 091 with bg\n",
    "# run_path = \"/home/harishbabu/projects/PIPNet/runs/097-091-wbg-CUB-18_with-equalize-aug_cnext26_BGM=4|1.0|50_img=224_nprotos=4per-leaf-desc_unit-sphere_finetune=5_no-meanpool_with-softmax-tau=0.2_no-addon-bias_AW=3-TW=2-MMW=2-UW=3-CW=2_mm-loss_batch=48\"\n",
    "\n",
    "# 0100 cub29 with 20 per node\n",
    "# run_path = \"/home/harishbabu/projects/PIPNet/runs/100_CUB-29-imgnet_with-equalize-aug_cnext26_BGM=4|1.0|50_img=224_nprotos=20_unit-sphere-protopool_no-meanpool_with-softmax-tau=0.2_no-addon-bias_AW=3-TW=2-MMW=2-UW=3-CW=2_mm-loss_batch=48\"\n",
    "\n",
    "# 0101 baseline with 4 per desc per node\n",
    "# run_path = \"/home/harishbabu/projects/PIPNet/runs/101-baseline-CUB-18-imgnet_with-equalize-aug_cnext26_img=224_nprotos=4per-desc_no-KO_no-OOD\"\n",
    "\n",
    "# 0103 091 with 20 per node\n",
    "# run_path = \"/home/harishbabu/projects/PIPNet/runs/103-091-wProtoPool20PerNode_CUB-18-imgnet_with-equalize-aug_cnext26_BGM=4|1.0|50_img=224_nprotos=20_unit-sphere-protopool_no-meanpool_with-softmax-tau=0.2_no-addon-bias_AW=3-TW=2-MMW=2-UW=3-CW=2_batch=48\"\n",
    "\n",
    "# 098 091 without AL + UNI\n",
    "# run_path = '/home/harishbabu/projects/PIPNet/runs/098-091-woALUNI_finetune=0_CUB-18-imgnet_with-equalize-aug_cnext26_BGM=4|1.0|50_img=224_nprotos=4per-leaf-desc_unit-sphere_no-meanpool_with-softmax-tau=0.2_no-addon-bias_AW=3-TW=2-MMW=2-UW=3-CW=2_mm-loss_batch=48'\n",
    "\n",
    "# 0107 091 with 20 per node\n",
    "# run_path = \"/home/harishbabu/projects/PIPNet/runs/107-baseline_LOU_CUB-18-imgnet_with-equalize-aug_cnext26_img=224_nprotos=20_no-KO_no-OOD\"\n",
    "\n",
    "# 109 flat structure 18 species - HPIPNet\n",
    "# run_path = '/home/harishbabu/projects/PIPNet/runs/109-FlatStructure180protos_CUB-18-imgnet-bg_with-equalize-aug_cnext26_BGM=4|1.0|50_img=224_nprotos=180_unit-sphere-protopool_no-meanpool_with-softmax-tau=0.2_no-addon-bias_AW=3-TW=2-MMW=2-UW=3-CW=2_batch=48'\n",
    "\n",
    "# 110 flat structure 18 species - HPIPNet no AL+UNI\n",
    "# run_path = '/home/harishbabu/projects/PIPNet/runs/110-FlatStructure180protosNoALUNI_CUB-18-imgnet-bg_with-equalize-aug_cnext26_BGM=4|1.0|50_img=224_nprotos=180_unit-sphere-protopool_no-meanpool_with-softmax-tau=0.2_no-addon-bias_AW=3-TW=2-MMW=2-UW=3-CW=2_batch=48'\n",
    "\n",
    "# 111 flat structure 18 species - Naive-HPIPNet\n",
    "# run_path = '/home/harishbabu/projects/PIPNet/runs/111-NaiveHPIPNetFlatStructure180-baseline_CUB-18-imgnet_with-equalize-aug_cnext26_img=224_nprotos=180_no-KO_no-OOD'\n",
    "\n",
    "try:\n",
    "    sys.path.remove('/home/harishbabu/projects/PIPNet')\n",
    "except:\n",
    "    pass\n",
    "sys.path.insert(0, os.path.join(run_path, 'source_clone'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harishbabu/.local/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/harishbabu/.local/lib/python3.8/site-packages/torchvision/image.so: undefined symbol: _ZN3c104cuda9SetDeviceEi'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "import sys, os\n",
    "import random\n",
    "import numpy as np\n",
    "from shutil import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "import shutil\n",
    "import pickle\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torchvision.datasets.folder import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "from skimage.filters import threshold_local, gaussian\n",
    "import ntpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/harishbabu/projects/PIPNet/runs/062-CUB-18-imgnet_with-equalize-aug_cnext26_img=224_nprotos=20_no-KO_no-OOD/source_clone', '/home/harishbabu/.conda/envs/hpnet3/lib/python38.zip', '/home/harishbabu/.conda/envs/hpnet3/lib/python3.8', '/home/harishbabu/.conda/envs/hpnet3/lib/python3.8/lib-dynload', '', '/home/harishbabu/.local/lib/python3.8/site-packages', '/home/harishbabu/.conda/envs/hpnet3/lib/python3.8/site-packages']\n"
     ]
    }
   ],
   "source": [
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipnet.pipnet import PIPNet, get_network\n",
    "from util.log import Log\n",
    "from util.args import get_args, save_args, get_optimizer_nn\n",
    "from util.data import get_dataloaders\n",
    "from util.func import init_weights_xavier\n",
    "from pipnet.train import train_pipnet, test_pipnet\n",
    "# from pipnet.test import eval_pipnet, get_thresholds, eval_ood\n",
    "from util.eval_cub_csv import eval_prototypes_cub_parts_csv, get_topk_cub, get_proto_patches_cub\n",
    "from util.vis_pipnet import visualize, visualize_topk\n",
    "from util.visualize_prediction import vis_pred, vis_pred_experiments\n",
    "from util.node import Node\n",
    "from util.phylo_utils import construct_phylo_tree, construct_discretized_phylo_tree\n",
    "from util.func import get_patch_size\n",
    "from util.data import ModifiedLabelLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/harishbabu/projects/PIPNet/runs/062-CUB-18-imgnet_with-equalize-aug_cnext26_img=224_nprotos=20_no-KO_no-OOD/source_clone/pipnet/pipnet.py\n"
     ]
    }
   ],
   "source": [
    "from pipnet import pipnet\n",
    "print(pipnet.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pdb\n",
    "\n",
    "def get_heatmap(latent_activation, input_image):\n",
    "    image_a = latent_activation.cpu().numpy()\n",
    "    image_a = (image_a - image_a.min()) / (image_a.max() - image_a.min())\n",
    "\n",
    "    input_image = (input_image - input_image.min()) / (input_image.max() - input_image.min())\n",
    "    image_b = input_image.permute(1, 2, 0).cpu().numpy()\n",
    "    \n",
    "    reshaped_image_a = np.array(Image.fromarray((image_a[0] * 255).astype('uint8')).resize((input_image.shape[-1], input_image.shape[-1])))\n",
    "    normalized_heatmap = (reshaped_image_a - np.min(reshaped_image_a)) / (np.max(reshaped_image_a) - np.min(reshaped_image_a))\n",
    "    \n",
    "    heatmap_colormap = plt.get_cmap('jet')\n",
    "    heatmap_colored = heatmap_colormap(normalized_heatmap)\n",
    "    \n",
    "    heatmap_colored_uint8 = (heatmap_colored[:, :, :3] * 255).astype(np.uint8)\n",
    "    image_a_heatmap_pillow = Image.fromarray(heatmap_colored_uint8)\n",
    "    image_b_pillow = Image.fromarray((image_b * 255).astype('uint8'))\n",
    "    \n",
    "    result_image = Image.blend(image_b_pillow, image_a_heatmap_pillow, alpha=0.3)\n",
    "    \n",
    "    return np.array(result_image)\n",
    "\n",
    "\n",
    "def get_heatmap_uninterpolated(latent_activation, input_image):\n",
    "    image_a = latent_activation.cpu().numpy()\n",
    "    image_a = (image_a - image_a.min()) / (image_a.max() - image_a.min())\n",
    "\n",
    "    input_image = (input_image - input_image.min()) / (input_image.max() - input_image.min())\n",
    "    image_b = input_image.permute(1, 2, 0).cpu().numpy()\n",
    "    \n",
    "    reshaped_image_a = np.array(Image.fromarray((image_a[0] * 255).astype('uint8')).resize((input_image.shape[-1], input_image.shape[-1]), \\\n",
    "                                                                                          resample=Image.NEAREST ))\n",
    "    normalized_heatmap = (reshaped_image_a - np.min(reshaped_image_a)) / (np.max(reshaped_image_a) - np.min(reshaped_image_a))\n",
    "    \n",
    "    heatmap_colormap = plt.get_cmap('jet')\n",
    "    heatmap_colored = heatmap_colormap(normalized_heatmap)\n",
    "    \n",
    "    heatmap_colored_uint8 = (heatmap_colored[:, :, :3] * 255).astype(np.uint8)\n",
    "    image_a_heatmap_pillow = Image.fromarray(heatmap_colored_uint8)\n",
    "    image_b_pillow = Image.fromarray((image_b * 255).astype('uint8'))\n",
    "    \n",
    "    result_image = Image.blend(image_b_pillow, image_a_heatmap_pillow, alpha=0.3)\n",
    "    \n",
    "    return np.array(result_image)\n",
    "\n",
    "def get_bb_gaussian_threshold(latent_activation, sigma=1.0, percentile=97, extend_h=0, extend_w=0):\n",
    "    # latent_activation -> []\n",
    "    upscaled_similarity = get_upscaled_activation_uninterpolated(latent_activation, \\\n",
    "                                                                 image_size=(args.image_size, args.image_size))\n",
    "    upscaled_similarity = minmaxscale(upscaled_similarity)\n",
    "    upscaled_similarity = gaussian(upscaled_similarity, sigma=sigma)\n",
    "    upscaled_similarity = threshold_local(upscaled_similarity, block_size=15, method='mean')\n",
    "    h_min, h_max, w_min, w_max = find_top_percentile_bbox(upscaled_similarity ,percentile=97)\n",
    "    h_min = max(0, h_min-extend_h)\n",
    "    h_max = min(upscaled_similarity.shape[0], h_max+extend_h)\n",
    "    w_min = max(0, w_min-extend_w)\n",
    "    w_max = min(upscaled_similarity.shape[1], w_max+extend_w)\n",
    "    return h_min, h_max, w_min, w_max\n",
    "\n",
    "\n",
    "def minmaxscale(tensor):\n",
    "    return (tensor - tensor.min()) / (tensor.max() - tensor.min())\n",
    "\n",
    "from torch.utils.data import DataLoader, SequentialSampler\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def unshuffle_dataloader(dataloader):\n",
    "    if type(dataloader.dataset) == ImageFolder:\n",
    "        dataset = dataloader.dataset\n",
    "    else:\n",
    "        dataset = dataloader.dataset.dataset.dataset\n",
    "    new_dataloader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=dataloader.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=dataloader.num_workers,\n",
    "        pin_memory=dataloader.pin_memory,\n",
    "        drop_last=dataloader.drop_last,\n",
    "        timeout=dataloader.timeout,\n",
    "        worker_init_fn=dataloader.worker_init_fn,\n",
    "        multiprocessing_context=dataloader.multiprocessing_context,\n",
    "        generator=dataloader.generator,\n",
    "        prefetch_factor=dataloader.prefetch_factor,\n",
    "        persistent_workers=dataloader.persistent_workers\n",
    "    )\n",
    "    return new_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------- No discretization -------------------------\n"
     ]
    }
   ],
   "source": [
    "args_file = open(os.path.join(run_path, 'metadata', 'args.pickle'), 'rb')\n",
    "args = pickle.load(args_file)\n",
    "\n",
    "if args.phylo_config:\n",
    "    phylo_config = OmegaConf.load(args.phylo_config)\n",
    "\n",
    "if args.phylo_config:\n",
    "    # construct the phylo tree\n",
    "    if phylo_config.phyloDistances_string == 'None':\n",
    "        if '031' in run_path: # this run uses a different phylogeny file that had an extra root node which is a mistake\n",
    "            root = construct_phylo_tree('/home/harishbabu/data/phlyogenyCUB/18Species-with-extra-root-node/1_tree-consensus-Hacket-18Species-modified_cub-names_v1.phy')\n",
    "        else:\n",
    "            root = construct_phylo_tree(phylo_config.phylogeny_path)\n",
    "        print('-'*25 + ' No discretization ' + '-'*25)\n",
    "    else:\n",
    "        root = construct_discretized_phylo_tree(phylo_config.phylogeny_path, phylo_config.phyloDistances_string)\n",
    "        print('-'*25 + ' Discretized ' + '-'*25)\n",
    "else:\n",
    "    # construct the tree (original hierarchy as described in the paper)\n",
    "    root = Node(\"root\")\n",
    "    root.add_children(['animal','vehicle','everyday_object','weapon','scuba_diver'])\n",
    "    root.add_children_to('animal',['non_primate','primate'])\n",
    "    root.add_children_to('non_primate',['African_elephant','giant_panda','lion'])\n",
    "    root.add_children_to('primate',['capuchin','gibbon','orangutan'])\n",
    "    root.add_children_to('vehicle',['ambulance','pickup','sports_car'])\n",
    "    root.add_children_to('everyday_object',['laptop','sandal','wine_bottle'])\n",
    "    root.add_children_to('weapon',['assault_rifle','rifle'])\n",
    "    # flat root\n",
    "    # root.add_children(['scuba_diver','African_elephant','giant_panda','lion','capuchin','gibbon','orangutan','ambulance','pickup','sports_car','laptop','sandal','wine_bottle','assault_rifle','rifle'])\n",
    "root.assign_all_descendents()\n",
    "\n",
    "exp_no = int(os.path.basename(run_path)[:3])\n",
    "\n",
    "if exp_no < 77:\n",
    "    if ('num_protos_per_descendant' in args) and (args.num_protos_per_descendant > 0):\n",
    "        for node in root.nodes_with_children():\n",
    "            node.set_num_protos(args.num_protos_per_descendant)\n",
    "else:\n",
    "    if ('num_protos_per_descendant' in args):\n",
    "        # update num of protos per node based on num_protos_per_descendant\n",
    "        if args.num_features == 0 and args.num_protos_per_descendant == 0:\n",
    "            raise Exception('Either of num_features or num_protos_per_descendant must be greater than zero')\n",
    "        for node in root.nodes_with_children():\n",
    "            node.set_num_protos(num_protos_per_descendant=args.num_protos_per_descendant,\\\n",
    "                                min_protos=args.num_features,\\\n",
    "                                split_protos=('protopool' in args) and (args.protopool == 'n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "args.batch_size = 1\n",
    "\n",
    "print(args.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping 0 samples from trainloader\n",
      "Dropping 0 samples from trainloader_normal\n",
      "Dropping 0 samples from trainloader_normal_augment\n",
      "Num classes (k) =  18 ['cub_001_Black_footed_Albatross', 'cub_002_Laysan_Albatross', 'cub_003_Sooty_Albatross', 'cub_004_Groove_billed_Ani', 'cub_023_Brandt_Cormorant'] etc.\n",
      "1 1\n",
      "Classes:  {'cub_001_Black_footed_Albatross': 0, 'cub_002_Laysan_Albatross': 1, 'cub_003_Sooty_Albatross': 2, 'cub_004_Groove_billed_Ani': 3, 'cub_023_Brandt_Cormorant': 4, 'cub_024_Red_faced_Cormorant': 5, 'cub_025_Pelagic_Cormorant': 6, 'cub_031_Black_billed_Cuckoo': 7, 'cub_032_Mangrove_Cuckoo': 8, 'cub_033_Yellow_billed_Cuckoo': 9, 'cub_045_Northern_Fulmar': 10, 'cub_050_Eared_Grebe': 11, 'cub_051_Horned_Grebe': 12, 'cub_052_Pied_billed_Grebe': 13, 'cub_053_Western_Grebe': 14, 'cub_086_Pacific_Loon': 15, 'cub_100_Brown_Pelican': 16, 'cub_101_White_Pelican': 17}\n",
      "Number of prototypes:  20\n",
      "Assigned 20 protos to node root\n",
      "Assigned 20 protos to node 052+053\n",
      "Assigned 20 protos to node 004+086\n",
      "Assigned 20 protos to node 053+050\n",
      "Assigned 20 protos to node 004+032\n",
      "Assigned 20 protos to node 086+045\n",
      "Assigned 20 protos to node 050+051\n",
      "Assigned 20 protos to node 032+033\n",
      "Assigned 20 protos to node 045+101\n",
      "Assigned 20 protos to node 033+031\n",
      "Assigned 20 protos to node 045+003\n",
      "Assigned 20 protos to node 101+023\n",
      "Assigned 20 protos to node 003+002\n",
      "Assigned 20 protos to node 101+100\n",
      "Assigned 20 protos to node 023+025\n",
      "Assigned 20 protos to node 002+001\n",
      "Assigned 20 protos to node 025+024\n",
      "DataParallel(\n",
      "  (module): PIPNet(\n",
      "    (_net): ConvNeXt(\n",
      "      (features): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))\n",
      "          (1): LayerNorm2d((96,), eps=1e-06, elementwise_affine=True)\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): CNBlock(\n",
      "            (block): Sequential(\n",
      "              (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
      "              (1): Permute()\n",
      "              (2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
      "              (3): Linear(in_features=96, out_features=384, bias=True)\n",
      "              (4): GELU(approximate='none')\n",
      "              (5): Linear(in_features=384, out_features=96, bias=True)\n",
      "              (6): Permute()\n",
      "            )\n",
      "            (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
      "          )\n",
      "          (1): CNBlock(\n",
      "            (block): Sequential(\n",
      "              (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
      "              (1): Permute()\n",
      "              (2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
      "              (3): Linear(in_features=96, out_features=384, bias=True)\n",
      "              (4): GELU(approximate='none')\n",
      "              (5): Linear(in_features=384, out_features=96, bias=True)\n",
      "              (6): Permute()\n",
      "            )\n",
      "            (stochastic_depth): StochasticDepth(p=0.0058823529411764705, mode=row)\n",
      "          )\n",
      "          (2): CNBlock(\n",
      "            (block): Sequential(\n",
      "              (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
      "              (1): Permute()\n",
      "              (2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
      "              (3): Linear(in_features=96, out_features=384, bias=True)\n",
      "              (4): GELU(approximate='none')\n",
      "              (5): Linear(in_features=384, out_features=96, bias=True)\n",
      "              (6): Permute()\n",
      "            )\n",
      "            (stochastic_depth): StochasticDepth(p=0.011764705882352941, mode=row)\n",
      "          )\n",
      "        )\n",
      "        (2): Sequential(\n",
      "          (0): LayerNorm2d((96,), eps=1e-06, elementwise_affine=True)\n",
      "          (1): Conv2d(96, 192, kernel_size=(2, 2), stride=(2, 2))\n",
      "        )\n",
      "        (3): Sequential(\n",
      "          (0): CNBlock(\n",
      "            (block): Sequential(\n",
      "              (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
      "              (1): Permute()\n",
      "              (2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "              (3): Linear(in_features=192, out_features=768, bias=True)\n",
      "              (4): GELU(approximate='none')\n",
      "              (5): Linear(in_features=768, out_features=192, bias=True)\n",
      "              (6): Permute()\n",
      "            )\n",
      "            (stochastic_depth): StochasticDepth(p=0.017647058823529415, mode=row)\n",
      "          )\n",
      "          (1): CNBlock(\n",
      "            (block): Sequential(\n",
      "              (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
      "              (1): Permute()\n",
      "              (2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "              (3): Linear(in_features=192, out_features=768, bias=True)\n",
      "              (4): GELU(approximate='none')\n",
      "              (5): Linear(in_features=768, out_features=192, bias=True)\n",
      "              (6): Permute()\n",
      "            )\n",
      "            (stochastic_depth): StochasticDepth(p=0.023529411764705882, mode=row)\n",
      "          )\n",
      "          (2): CNBlock(\n",
      "            (block): Sequential(\n",
      "              (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
      "              (1): Permute()\n",
      "              (2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "              (3): Linear(in_features=192, out_features=768, bias=True)\n",
      "              (4): GELU(approximate='none')\n",
      "              (5): Linear(in_features=768, out_features=192, bias=True)\n",
      "              (6): Permute()\n",
      "            )\n",
      "            (stochastic_depth): StochasticDepth(p=0.029411764705882353, mode=row)\n",
      "          )\n",
      "        )\n",
      "        (4): Sequential(\n",
      "          (0): LayerNorm2d((192,), eps=1e-06, elementwise_affine=True)\n",
      "          (1): Conv2d(192, 384, kernel_size=(2, 2), stride=(1, 1))\n",
      "        )\n",
      "        (5): Sequential(\n",
      "          (0): CNBlock(\n",
      "            (block): Sequential(\n",
      "              (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "              (1): Permute()\n",
      "              (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "              (3): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (4): GELU(approximate='none')\n",
      "              (5): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (6): Permute()\n",
      "            )\n",
      "            (stochastic_depth): StochasticDepth(p=0.03529411764705883, mode=row)\n",
      "          )\n",
      "          (1): CNBlock(\n",
      "            (block): Sequential(\n",
      "              (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "              (1): Permute()\n",
      "              (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "              (3): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (4): GELU(approximate='none')\n",
      "              (5): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (6): Permute()\n",
      "            )\n",
      "            (stochastic_depth): StochasticDepth(p=0.0411764705882353, mode=row)\n",
      "          )\n",
      "          (2): CNBlock(\n",
      "            (block): Sequential(\n",
      "              (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "              (1): Permute()\n",
      "              (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "              (3): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (4): GELU(approximate='none')\n",
      "              (5): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (6): Permute()\n",
      "            )\n",
      "            (stochastic_depth): StochasticDepth(p=0.047058823529411764, mode=row)\n",
      "          )\n",
      "          (3): CNBlock(\n",
      "            (block): Sequential(\n",
      "              (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "              (1): Permute()\n",
      "              (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "              (3): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (4): GELU(approximate='none')\n",
      "              (5): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (6): Permute()\n",
      "            )\n",
      "            (stochastic_depth): StochasticDepth(p=0.052941176470588235, mode=row)\n",
      "          )\n",
      "          (4): CNBlock(\n",
      "            (block): Sequential(\n",
      "              (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "              (1): Permute()\n",
      "              (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "              (3): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (4): GELU(approximate='none')\n",
      "              (5): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (6): Permute()\n",
      "            )\n",
      "            (stochastic_depth): StochasticDepth(p=0.058823529411764705, mode=row)\n",
      "          )\n",
      "          (5): CNBlock(\n",
      "            (block): Sequential(\n",
      "              (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "              (1): Permute()\n",
      "              (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "              (3): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (4): GELU(approximate='none')\n",
      "              (5): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (6): Permute()\n",
      "            )\n",
      "            (stochastic_depth): StochasticDepth(p=0.06470588235294118, mode=row)\n",
      "          )\n",
      "          (6): CNBlock(\n",
      "            (block): Sequential(\n",
      "              (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "              (1): Permute()\n",
      "              (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "              (3): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (4): GELU(approximate='none')\n",
      "              (5): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (6): Permute()\n",
      "            )\n",
      "            (stochastic_depth): StochasticDepth(p=0.07058823529411766, mode=row)\n",
      "          )\n",
      "          (7): CNBlock(\n",
      "            (block): Sequential(\n",
      "              (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "              (1): Permute()\n",
      "              (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "              (3): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (4): GELU(approximate='none')\n",
      "              (5): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (6): Permute()\n",
      "            )\n",
      "            (stochastic_depth): StochasticDepth(p=0.07647058823529412, mode=row)\n",
      "          )\n",
      "          (8): CNBlock(\n",
      "            (block): Sequential(\n",
      "              (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "              (1): Permute()\n",
      "              (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "              (3): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (4): GELU(approximate='none')\n",
      "              (5): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (6): Permute()\n",
      "            )\n",
      "            (stochastic_depth): StochasticDepth(p=0.0823529411764706, mode=row)\n",
      "          )\n",
      "        )\n",
      "        (6): Sequential(\n",
      "          (0): LayerNorm2d((384,), eps=1e-06, elementwise_affine=True)\n",
      "          (1): Conv2d(384, 768, kernel_size=(2, 2), stride=(1, 1))\n",
      "        )\n",
      "        (7): Sequential(\n",
      "          (0): CNBlock(\n",
      "            (block): Sequential(\n",
      "              (0): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
      "              (1): Permute()\n",
      "              (2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "              (3): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (4): GELU(approximate='none')\n",
      "              (5): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (6): Permute()\n",
      "            )\n",
      "            (stochastic_depth): StochasticDepth(p=0.08823529411764706, mode=row)\n",
      "          )\n",
      "          (1): CNBlock(\n",
      "            (block): Sequential(\n",
      "              (0): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
      "              (1): Permute()\n",
      "              (2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "              (3): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (4): GELU(approximate='none')\n",
      "              (5): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (6): Permute()\n",
      "            )\n",
      "            (stochastic_depth): StochasticDepth(p=0.09411764705882353, mode=row)\n",
      "          )\n",
      "          (2): CNBlock(\n",
      "            (block): Sequential(\n",
      "              (0): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
      "              (1): Permute()\n",
      "              (2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "              (3): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (4): GELU(approximate='none')\n",
      "              (5): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (6): Permute()\n",
      "            )\n",
      "            (stochastic_depth): StochasticDepth(p=0.1, mode=row)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (avgpool): Identity()\n",
      "      (classifier): Identity()\n",
      "    )\n",
      "    (_root_add_on): Conv2d(768, 20, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (_052+053_add_on): Conv2d(768, 20, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (_004+086_add_on): Conv2d(768, 20, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (_053+050_add_on): Conv2d(768, 20, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (_004+032_add_on): Conv2d(768, 20, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (_086+045_add_on): Conv2d(768, 20, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (_050+051_add_on): Conv2d(768, 20, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (_032+033_add_on): Conv2d(768, 20, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (_045+101_add_on): Conv2d(768, 20, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (_033+031_add_on): Conv2d(768, 20, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (_045+003_add_on): Conv2d(768, 20, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (_101+023_add_on): Conv2d(768, 20, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (_003+002_add_on): Conv2d(768, 20, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (_101+100_add_on): Conv2d(768, 20, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (_023+025_add_on): Conv2d(768, 20, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (_002+001_add_on): Conv2d(768, 20, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (_025+024_add_on): Conv2d(768, 20, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (_pool): Sequential(\n",
      "      (0): AdaptiveMaxPool2d(output_size=(1, 1))\n",
      "      (1): Flatten(start_dim=1, end_dim=-1)\n",
      "    )\n",
      "    (_root_classification): NonNegLinear()\n",
      "    (_052+053_classification): NonNegLinear()\n",
      "    (_004+086_classification): NonNegLinear()\n",
      "    (_053+050_classification): NonNegLinear()\n",
      "    (_004+032_classification): NonNegLinear()\n",
      "    (_086+045_classification): NonNegLinear()\n",
      "    (_050+051_classification): NonNegLinear()\n",
      "    (_032+033_classification): NonNegLinear()\n",
      "    (_045+101_classification): NonNegLinear()\n",
      "    (_033+031_classification): NonNegLinear()\n",
      "    (_045+003_classification): NonNegLinear()\n",
      "    (_101+023_classification): NonNegLinear()\n",
      "    (_003+002_classification): NonNegLinear()\n",
      "    (_101+100_classification): NonNegLinear()\n",
      "    (_023+025_classification): NonNegLinear()\n",
      "    (_002+001_classification): NonNegLinear()\n",
      "    (_025+024_classification): NonNegLinear()\n",
      "    (_softmax): Softmax(dim=1)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    device_ids = [torch.cuda.current_device()]\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    device_ids = []\n",
    "\n",
    "# args_file = open(os.path.join(run_path, 'metadata', 'args.pickle'), 'rb')\n",
    "# args = pickle.load(args_file)\n",
    "\n",
    "# ckpt_file_name = 'net_overspecific_pruned_replaced_thresh=0.5_last'\n",
    "ckpt_file_name = 'net_trained_last'\n",
    "# ckpt_file_name = 'net_trained_10'\n",
    "# ckpt_file_name = 'net_pretrained'\n",
    "epoch = ckpt_file_name.split('_')[-1]\n",
    "\n",
    "ckpt_path = os.path.join(run_path, 'checkpoints', ckpt_file_name)\n",
    "checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "\n",
    "if ckpt_file_name != 'net_trained_last':\n",
    "    print('\\n', (10*'-')+'WARNING: Not using the final trained model'+(10*'-'), '\\n')\n",
    "\n",
    "# Obtain the dataset and dataloaders\n",
    "trainloader, trainloader_pretraining, trainloader_normal, trainloader_normal_augment, projectloader, testloader, test_projectloader, classes = get_dataloaders(args, device)\n",
    "\n",
    "print(args.batch_size, trainloader.batch_size)\n",
    "\n",
    "if len(classes)<=20:\n",
    "    if args.validation_size == 0.:\n",
    "        print(\"Classes: \", testloader.dataset.class_to_idx, flush=True)\n",
    "    else:\n",
    "        print(\"Classes: \", str(classes), flush=True)\n",
    "\n",
    "# Create a convolutional network based on arguments and add 1x1 conv layer\n",
    "feature_net, add_on_layers, pool_layer, classification_layers, num_prototypes = get_network(len(classes), args, root=root)\n",
    "   \n",
    "# Create a PIP-Net\n",
    "net = PIPNet(num_classes=len(classes),\n",
    "                    num_prototypes=num_prototypes,\n",
    "                    feature_net = feature_net,\n",
    "                    args = args,\n",
    "                    add_on_layers = add_on_layers,\n",
    "                    pool_layer = pool_layer,\n",
    "                    classification_layers = classification_layers,\n",
    "                    num_parent_nodes = len(root.nodes_with_children()),\n",
    "                    root = root\n",
    "                    )\n",
    "net = net.to(device=device)\n",
    "net = nn.DataParallel(net, device_ids = device_ids)    \n",
    "net.load_state_dict(checkpoint['model_state_dict'],strict=True)\n",
    "print(net.eval())\n",
    "criterion = nn.NLLLoss(reduction='mean').to(device)\n",
    "\n",
    "# Forward one batch through the backbone to get the latent output size\n",
    "# with torch.no_grad():\n",
    "#     xs1, _, _ = next(iter(trainloader))\n",
    "#     xs1 = xs1.to(device)\n",
    "#     proto_features, _, _ = net(xs1)\n",
    "#     wshape = proto_features['root'].shape[-1]\n",
    "#     args.wshape = wshape #needed for calculating image patch size\n",
    "#     print(\"Output shape: \", proto_features['root'].shape, flush=True)\n",
    "    \n",
    "args.wshape = 26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "11.7\n",
      "2.0.0+cu117\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.version.cuda)\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Namespace' object has no attribute 'basic_cnext_gaussian_multiplier'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbasic_cnext_gaussian_multiplier\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Namespace' object has no attribute 'basic_cnext_gaussian_multiplier'"
     ]
    }
   ],
   "source": [
    "args.basic_cnext_gaussian_multiplier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proto activations on leaf descendents - topk images using either NAIVE-HPIPNET or UNIT-SPACE-PROTOPOOL with HEATMAP (CANON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 540it [00:22, 24.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node root\n",
      "Num protos for 052+053 3\n",
      "Num protos for 004+086 56\n",
      "\t Child: 052+053\n",
      "\t\tProto:10 050:(0.0178) 051:(0.0188) 052:(0.0174) 053:(0.0206) \n",
      "\t\tSkipping proto 10 of root\n",
      "\t\tProto:4 050:(0.5023) 051:(0.498) 052:(0.0596) 053:(0.4306) \n",
      "\t\tProto:12 050:(0.2899) 051:(0.3175) 052:(0.4172) 053:(0.321) \n",
      "\t Child: 004+086\n",
      "\t\tProto:16 001:(0.0235) 002:(0.0228) 003:(0.0226) 004:(0.0227) 023:(0.0222) 024:(0.0222) 025:(0.0212) 031:(0.0247) 032:(0.0219) 033:(0.0244) 045:(0.022) 086:(0.022) 100:(0.0318) 101:(0.0318) \n",
      "\t\tSkipping proto 16 of root\n",
      "\t\tProto:17 001:(0.0206) 002:(0.02) 003:(0.0193) 004:(0.0213) 023:(0.0216) 024:(0.0218) 025:(0.0216) 031:(0.0197) 032:(0.0205) 033:(0.0229) 045:(0.0202) 086:(0.0209) 100:(0.0181) 101:(0.0187) \n",
      "\t\tSkipping proto 17 of root\n",
      "\t\tProto:18 001:(0.0379) 002:(0.0396) 003:(0.0362) 004:(0.0225) 023:(0.0224) 024:(0.0272) 025:(0.026) 031:(0.026) 032:(0.0261) 033:(0.0261) 045:(0.0329) 086:(0.0216) 100:(0.027) 101:(0.0262) \n",
      "\t\tSkipping proto 18 of root\n",
      "\t\tProto:19 001:(0.021) 002:(0.0182) 003:(0.0283) 004:(0.0262) 023:(0.0228) 024:(0.0248) 025:(0.0237) 031:(0.0244) 032:(0.0227) 033:(0.0246) 045:(0.0189) 086:(0.0188) 100:(0.0232) 101:(0.0195) \n",
      "\t\tSkipping proto 19 of root\n",
      "\t\tProto:20 001:(0.0196) 002:(0.0196) 003:(0.0193) 004:(0.0195) 023:(0.0183) 024:(0.0191) 025:(0.0189) 031:(0.0198) 032:(0.021) 033:(0.0197) 045:(0.0174) 086:(0.0177) 100:(0.0199) 101:(0.019) \n",
      "\t\tSkipping proto 20 of root\n",
      "\t\tProto:21 001:(0.0306) 002:(0.0201) 003:(0.0188) 004:(0.0197) 023:(0.0224) 024:(0.0213) 025:(0.0204) 031:(0.0191) 032:(0.0219) 033:(0.0206) 045:(0.0204) 086:(0.0191) 100:(0.018) 101:(0.0189) \n",
      "\t\tSkipping proto 21 of root\n",
      "\t\tProto:22 001:(0.0174) 002:(0.0164) 003:(0.0183) 004:(0.0206) 023:(0.0205) 024:(0.0199) 025:(0.021) 031:(0.0206) 032:(0.0197) 033:(0.0194) 045:(0.0168) 086:(0.0182) 100:(0.0203) 101:(0.0174) \n",
      "\t\tSkipping proto 22 of root\n",
      "\t\tProto:23 001:(0.0177) 002:(0.0183) 003:(0.0181) 004:(0.0187) 023:(0.0192) 024:(0.0191) 025:(0.0199) 031:(0.0198) 032:(0.0179) 033:(0.0185) 045:(0.0189) 086:(0.0187) 100:(0.0188) 101:(0.0197) \n",
      "\t\tSkipping proto 23 of root\n",
      "\t\tProto:24 001:(0.0188) 002:(0.0201) 003:(0.019) 004:(0.0247) 023:(0.0222) 024:(0.0192) 025:(0.0259) 031:(0.0242) 032:(0.0245) 033:(0.0231) 045:(0.0215) 086:(0.0229) 100:(0.0217) 101:(0.0213) \n",
      "\t\tSkipping proto 24 of root\n",
      "\t\tProto:25 001:(0.0198) 002:(0.0365) 003:(0.0201) 004:(0.025) 023:(0.0262) 024:(0.0278) 025:(0.0256) 031:(0.0211) 032:(0.0213) 033:(0.0221) 045:(0.0344) 086:(0.0205) 100:(0.0252) 101:(0.0381) \n",
      "\t\tSkipping proto 25 of root\n",
      "\t\tProto:26 001:(0.0187) 002:(0.019) 003:(0.0187) 004:(0.0186) 023:(0.0195) 024:(0.0215) 025:(0.0196) 031:(0.0213) 032:(0.0224) 033:(0.0234) 045:(0.0198) 086:(0.0191) 100:(0.0178) 101:(0.0187) \n",
      "\t\tSkipping proto 26 of root\n",
      "\t\tProto:27 001:(0.0205) 002:(0.0234) 003:(0.0228) 004:(0.0268) 023:(0.0213) 024:(0.0203) 025:(0.0213) 031:(0.0217) 032:(0.022) 033:(0.023) 045:(0.023) 086:(0.0223) 100:(0.0217) 101:(0.0212) \n",
      "\t\tSkipping proto 27 of root\n",
      "\t\tProto:28 001:(0.0186) 002:(0.0189) 003:(0.0189) 004:(0.0191) 023:(0.0194) 024:(0.02) 025:(0.0192) 031:(0.0181) 032:(0.0217) 033:(0.0178) 045:(0.0173) 086:(0.0184) 100:(0.0175) 101:(0.0201) \n",
      "\t\tSkipping proto 28 of root\n",
      "\t\tProto:29 001:(0.0261) 002:(0.0242) 003:(0.0257) 004:(0.0213) 023:(0.0219) 024:(0.0209) 025:(0.0231) 031:(0.0194) 032:(0.0216) 033:(0.022) 045:(0.0224) 086:(0.022) 100:(0.0231) 101:(0.0217) \n",
      "\t\tSkipping proto 29 of root\n",
      "\t\tProto:30 001:(0.0263) 002:(0.0231) 003:(0.0242) 004:(0.0272) 023:(0.0281) 024:(0.0279) 025:(0.0273) 031:(0.0234) 032:(0.0218) 033:(0.024) 045:(0.0238) 086:(0.0259) 100:(0.0246) 101:(0.0245) \n",
      "\t\tSkipping proto 30 of root\n",
      "\t\tProto:31 001:(0.0247) 002:(0.0275) 003:(0.0243) 004:(0.0229) 023:(0.0237) 024:(0.0243) 025:(0.0235) 031:(0.0286) 032:(0.026) 033:(0.0268) 045:(0.0243) 086:(0.023) 100:(0.0257) 101:(0.026) \n",
      "\t\tSkipping proto 31 of root\n",
      "\t\tProto:32 001:(0.0237) 002:(0.0285) 003:(0.0239) 004:(0.0264) 023:(0.0264) 024:(0.0267) 025:(0.0265) 031:(0.0359) 032:(0.0302) 033:(0.0354) 045:(0.0319) 086:(0.0253) 100:(0.0216) 101:(0.0365) \n",
      "\t\tSkipping proto 32 of root\n",
      "\t\tProto:33 001:(0.027) 002:(0.0226) 003:(0.0226) 004:(0.0211) 023:(0.0205) 024:(0.0199) 025:(0.0211) 031:(0.0248) 032:(0.0202) 033:(0.0226) 045:(0.0226) 086:(0.0227) 100:(0.0217) 101:(0.029) \n",
      "\t\tSkipping proto 33 of root\n",
      "\t\tProto:34 001:(0.0232) 002:(0.0275) 003:(0.027) 004:(0.03) 023:(0.026) 024:(0.0227) 025:(0.0217) 031:(0.0365) 032:(0.0358) 033:(0.037) 045:(0.0268) 086:(0.0217) 100:(0.0229) 101:(0.0291) \n",
      "\t\tSkipping proto 34 of root\n",
      "\t\tProto:35 001:(0.0399) 002:(0.0409) 003:(0.0406) 004:(0.0243) 023:(0.0601) 024:(0.0209) 025:(0.0343) 031:(0.0489) 032:(0.0353) 033:(0.0326) 045:(0.0313) 086:(0.0548) 100:(0.0249) 101:(0.0225) \n",
      "\t\tSkipping proto 35 of root\n",
      "\t\tProto:36 001:(0.0198) 002:(0.0208) 003:(0.0193) 004:(0.0199) 023:(0.0208) 024:(0.0208) 025:(0.0208) 031:(0.02) 032:(0.0198) 033:(0.019) 045:(0.0202) 086:(0.0189) 100:(0.0208) 101:(0.0213) \n",
      "\t\tSkipping proto 36 of root\n",
      "\t\tProto:37 001:(0.0255) 002:(0.0244) 003:(0.0202) 004:(0.0182) 023:(0.0227) 024:(0.0195) 025:(0.0205) 031:(0.0199) 032:(0.0201) 033:(0.0207) 045:(0.0251) 086:(0.0208) 100:(0.0211) 101:(0.0224) \n",
      "\t\tSkipping proto 37 of root\n",
      "\t\tProto:38 001:(0.0213) 002:(0.0208) 003:(0.0216) 004:(0.0185) 023:(0.0209) 024:(0.0295) 025:(0.0291) 031:(0.0262) 032:(0.0274) 033:(0.0236) 045:(0.0243) 086:(0.0227) 100:(0.0201) 101:(0.0265) \n",
      "\t\tSkipping proto 38 of root\n",
      "\t\tProto:39 001:(0.023) 002:(0.0226) 003:(0.0254) 004:(0.0232) 023:(0.0248) 024:(0.0253) 025:(0.0256) 031:(0.0227) 032:(0.0257) 033:(0.0236) 045:(0.024) 086:(0.0216) 100:(0.0198) 101:(0.0205) \n",
      "\t\tSkipping proto 39 of root\n",
      "\t\tProto:40 001:(0.0203) 002:(0.018) 003:(0.0198) 004:(0.0188) 023:(0.0205) 024:(0.0201) 025:(0.0204) 031:(0.0173) 032:(0.0156) 033:(0.017) 045:(0.0196) 086:(0.017) 100:(0.0193) 101:(0.0162) \n",
      "\t\tSkipping proto 40 of root\n",
      "\t\tProto:41 001:(0.0294) 002:(0.0257) 003:(0.0195) 004:(0.0196) 023:(0.0199) 024:(0.0203) 025:(0.019) 031:(0.0224) 032:(0.0186) 033:(0.0241) 045:(0.0241) 086:(0.0208) 100:(0.0211) 101:(0.0182) \n",
      "\t\tSkipping proto 41 of root\n",
      "\t\tProto:42 001:(0.0245) 002:(0.0216) 003:(0.0214) 004:(0.0247) 023:(0.0298) 024:(0.0232) 025:(0.0304) 031:(0.0249) 032:(0.0195) 033:(0.0242) 045:(0.0232) 086:(0.0252) 100:(0.0244) 101:(0.0199) \n",
      "\t\tSkipping proto 42 of root\n",
      "\t\tProto:43 001:(0.0211) 002:(0.0228) 003:(0.0215) 004:(0.0196) 023:(0.0218) 024:(0.0198) 025:(0.0197) 031:(0.0212) 032:(0.0204) 033:(0.0213) 045:(0.0231) 086:(0.0206) 100:(0.0215) 101:(0.0241) \n",
      "\t\tSkipping proto 43 of root\n",
      "\t\tProto:44 001:(0.0248) 002:(0.0245) 003:(0.0246) 004:(0.0179) 023:(0.0209) 024:(0.0194) 025:(0.0212) 031:(0.0214) 032:(0.0252) 033:(0.0199) 045:(0.0245) 086:(0.0211) 100:(0.0191) 101:(0.0208) \n",
      "\t\tSkipping proto 44 of root\n",
      "\t\tProto:45 001:(0.0269) 002:(0.0221) 003:(0.026) 004:(0.0202) 023:(0.02) 024:(0.0201) 025:(0.0207) 031:(0.0225) 032:(0.0211) 033:(0.0226) 045:(0.0259) 086:(0.0195) 100:(0.02) 101:(0.0194) \n",
      "\t\tSkipping proto 45 of root\n",
      "\t\tProto:46 001:(0.0177) 002:(0.0172) 003:(0.0198) 004:(0.016) 023:(0.0199) 024:(0.0181) 025:(0.0185) 031:(0.0167) 032:(0.0178) 033:(0.0185) 045:(0.0181) 086:(0.018) 100:(0.0182) 101:(0.019) \n",
      "\t\tSkipping proto 46 of root\n",
      "\t\tProto:47 001:(0.0243) 002:(0.0213) 003:(0.0225) 004:(0.0257) 023:(0.0261) 024:(0.0264) 025:(0.0261) 031:(0.021) 032:(0.0193) 033:(0.0205) 045:(0.0229) 086:(0.0248) 100:(0.0191) 101:(0.0192) \n",
      "\t\tSkipping proto 47 of root\n",
      "\t\tProto:48 001:(0.0238) 002:(0.027) 003:(0.0248) 004:(0.0214) 023:(0.022) 024:(0.0208) 025:(0.0214) 031:(0.0204) 032:(0.0208) 033:(0.0224) 045:(0.0286) 086:(0.0227) 100:(0.0206) 101:(0.0292) \n",
      "\t\tSkipping proto 48 of root\n",
      "\t\tProto:49 001:(0.0332) 002:(0.033) 003:(0.0351) 004:(0.0287) 023:(0.0404) 024:(0.026) 025:(0.0378) 031:(0.0299) 032:(0.0293) 033:(0.0272) 045:(0.0308) 086:(0.0391) 100:(0.0313) 101:(0.0269) \n",
      "\t\tSkipping proto 49 of root\n",
      "\t\tProto:50 001:(0.0226) 002:(0.0223) 003:(0.0186) 004:(0.0179) 023:(0.0216) 024:(0.0189) 025:(0.0203) 031:(0.0198) 032:(0.0191) 033:(0.0198) 045:(0.0201) 086:(0.0189) 100:(0.0202) 101:(0.0176) \n",
      "\t\tSkipping proto 50 of root\n",
      "\t\tProto:51 001:(0.0238) 002:(0.0256) 003:(0.0241) 004:(0.0213) 023:(0.0218) 024:(0.0212) 025:(0.0219) 031:(0.0196) 032:(0.0202) 033:(0.019) 045:(0.0224) 086:(0.0188) 100:(0.0215) 101:(0.0176) \n",
      "\t\tSkipping proto 51 of root\n",
      "\t\tProto:52 001:(0.0228) 002:(0.0237) 003:(0.0275) 004:(0.0233) 023:(0.0223) 024:(0.0307) 025:(0.0218) 031:(0.0323) 032:(0.0259) 033:(0.0345) 045:(0.0202) 086:(0.0216) 100:(0.0508) 101:(0.0197) \n",
      "\t\tSkipping proto 52 of root\n",
      "\t\tProto:53 001:(0.0225) 002:(0.0233) 003:(0.0228) 004:(0.0171) 023:(0.0172) 024:(0.0169) 025:(0.0171) 031:(0.0199) 032:(0.0188) 033:(0.0184) 045:(0.022) 086:(0.0178) 100:(0.0185) 101:(0.0185) \n",
      "\t\tSkipping proto 53 of root\n",
      "\t\tProto:54 001:(0.0234) 002:(0.0223) 003:(0.0215) 004:(0.0301) 023:(0.0228) 024:(0.0224) 025:(0.0233) 031:(0.0254) 032:(0.0252) 033:(0.0266) 045:(0.0235) 086:(0.021) 100:(0.0214) 101:(0.0194) \n",
      "\t\tSkipping proto 54 of root\n",
      "\t\tProto:55 001:(0.0207) 002:(0.0193) 003:(0.0196) 004:(0.0182) 023:(0.0203) 024:(0.0225) 025:(0.02) 031:(0.0208) 032:(0.0183) 033:(0.0213) 045:(0.0191) 086:(0.0189) 100:(0.0216) 101:(0.0185) \n",
      "\t\tSkipping proto 55 of root\n",
      "\t\tProto:56 001:(0.0249) 002:(0.0248) 003:(0.0514) 004:(0.0876) 023:(0.0834) 024:(0.0959) 025:(0.0872) 031:(0.0527) 032:(0.0352) 033:(0.0423) 045:(0.0343) 086:(0.0329) 100:(0.0647) 101:(0.0541) \n",
      "\t\tSkipping proto 56 of root\n",
      "\t\tProto:57 001:(0.0188) 002:(0.0196) 003:(0.0194) 004:(0.0176) 023:(0.0208) 024:(0.0192) 025:(0.019) 031:(0.0182) 032:(0.0197) 033:(0.0184) 045:(0.0194) 086:(0.02) 100:(0.019) 101:(0.0181) \n",
      "\t\tSkipping proto 57 of root\n",
      "\t\tProto:58 001:(0.0218) 002:(0.0231) 003:(0.021) 004:(0.0175) 023:(0.0191) 024:(0.0247) 025:(0.022) 031:(0.0183) 032:(0.0195) 033:(0.0184) 045:(0.0232) 086:(0.0184) 100:(0.0174) 101:(0.0178) \n",
      "\t\tSkipping proto 58 of root\n",
      "\t\tProto:59 001:(0.0213) 002:(0.0198) 003:(0.0198) 004:(0.0198) 023:(0.0242) 024:(0.0245) 025:(0.0242) 031:(0.021) 032:(0.022) 033:(0.0242) 045:(0.0209) 086:(0.0218) 100:(0.0222) 101:(0.0228) \n",
      "\t\tSkipping proto 59 of root\n",
      "\t\tProto:60 001:(0.0229) 002:(0.0195) 003:(0.0209) 004:(0.026) 023:(0.025) 024:(0.0255) 025:(0.0254) 031:(0.0211) 032:(0.0206) 033:(0.0194) 045:(0.0208) 086:(0.0233) 100:(0.0202) 101:(0.0192) \n",
      "\t\tSkipping proto 60 of root\n",
      "\t\tProto:61 001:(0.0216) 002:(0.0222) 003:(0.0245) 004:(0.0228) 023:(0.023) 024:(0.0235) 025:(0.0226) 031:(0.0262) 032:(0.0261) 033:(0.0271) 045:(0.0207) 086:(0.0223) 100:(0.022) 101:(0.0238) \n",
      "\t\tSkipping proto 61 of root\n",
      "\t\tProto:62 001:(0.0238) 002:(0.0231) 003:(0.0239) 004:(0.0199) 023:(0.0196) 024:(0.0213) 025:(0.0201) 031:(0.0182) 032:(0.0196) 033:(0.0197) 045:(0.024) 086:(0.017) 100:(0.018) 101:(0.0224) \n",
      "\t\tSkipping proto 62 of root\n",
      "\t\tProto:63 001:(0.0229) 002:(0.0652) 003:(0.027) 004:(0.0249) 023:(0.0246) 024:(0.0388) 025:(0.0224) 031:(0.0303) 032:(0.0314) 033:(0.0345) 045:(0.0563) 086:(0.0216) 100:(0.0228) 101:(0.069) \n",
      "\t\tSkipping proto 63 of root\n",
      "\t\tProto:64 001:(0.0251) 002:(0.0251) 003:(0.0241) 004:(0.0225) 023:(0.0249) 024:(0.0245) 025:(0.0235) 031:(0.0259) 032:(0.0224) 033:(0.0246) 045:(0.0241) 086:(0.0261) 100:(0.0258) 101:(0.0246) \n",
      "\t\tSkipping proto 64 of root\n",
      "\t\tProto:65 001:(0.0213) 002:(0.0222) 003:(0.0203) 004:(0.0242) 023:(0.0238) 024:(0.0196) 025:(0.0238) 031:(0.0195) 032:(0.0209) 033:(0.0232) 045:(0.023) 086:(0.0224) 100:(0.0219) 101:(0.0233) \n",
      "\t\tSkipping proto 65 of root\n",
      "\t\tProto:66 001:(0.0236) 002:(0.022) 003:(0.0214) 004:(0.0257) 023:(0.0269) 024:(0.0323) 025:(0.0223) 031:(0.0201) 032:(0.0242) 033:(0.0199) 045:(0.0211) 086:(0.0199) 100:(0.0216) 101:(0.0209) \n",
      "\t\tSkipping proto 66 of root\n",
      "\t\tProto:67 001:(0.0264) 002:(0.0271) 003:(0.0259) 004:(0.0381) 023:(0.0305) 024:(0.0303) 025:(0.0305) 031:(0.0454) 032:(0.0363) 033:(0.0455) 045:(0.0232) 086:(0.0249) 100:(0.0545) 101:(0.0536) \n",
      "\t\tSkipping proto 67 of root\n",
      "\t\tProto:68 001:(0.0216) 002:(0.0199) 003:(0.0211) 004:(0.0213) 023:(0.024) 024:(0.0199) 025:(0.0185) 031:(0.0222) 032:(0.0217) 033:(0.0194) 045:(0.0199) 086:(0.0217) 100:(0.0201) 101:(0.0184) \n",
      "\t\tSkipping proto 68 of root\n",
      "\t\tProto:69 001:(0.0247) 002:(0.0256) 003:(0.0265) 004:(0.0243) 023:(0.0212) 024:(0.019) 025:(0.0236) 031:(0.0238) 032:(0.0222) 033:(0.0263) 045:(0.0272) 086:(0.0201) 100:(0.0214) 101:(0.0198) \n",
      "\t\tSkipping proto 69 of root\n",
      "\t\tProto:70 001:(0.0188) 002:(0.017) 003:(0.0231) 004:(0.0174) 023:(0.0199) 024:(0.0192) 025:(0.0216) 031:(0.019) 032:(0.0195) 033:(0.0208) 045:(0.0191) 086:(0.0196) 100:(0.0195) 101:(0.0205) \n",
      "\t\tSkipping proto 70 of root\n",
      "\t\tProto:71 001:(0.0197) 002:(0.0241) 003:(0.0191) 004:(0.02) 023:(0.0189) 024:(0.0195) 025:(0.0193) 031:(0.0229) 032:(0.0205) 033:(0.0234) 045:(0.0204) 086:(0.0192) 100:(0.0204) 101:(0.0186) \n",
      "\t\tSkipping proto 71 of root\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 120it [00:03, 36.57it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 052+053\n",
      "Num protos for cub_052_Pied_billed_Grebe 2\n",
      "Num protos for 053+050 4\n",
      "\t Child: cub_052_Pied_billed_Grebe\n",
      "\t\tProto:0 052:(0.8833) \n",
      "\t\tProto:2 052:(0.0821) \n",
      "\t\tSkipping proto 2 of 052+053\n",
      "\t Child: 053+050\n",
      "\t\tProto:8 050:(0.7096) 051:(0.7131) 053:(0.6858) \n",
      "\t\tProto:9 050:(0.7265) 051:(0.752) 053:(0.8067) \n",
      "\t\tProto:10 050:(0.0833) 051:(0.0828) 053:(0.0809) \n",
      "\t\tSkipping proto 10 of 052+053\n",
      "\t\tProto:6 050:(0.5429) 051:(0.5379) 053:(0.4806) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 420it [00:11, 35.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 004+086\n",
      "Num protos for 004+032 8\n",
      "Num protos for 086+045 9\n",
      "\t Child: 004+032\n",
      "\t\tProto:0 004:(0.0275) 031:(0.0312) 032:(0.0375) 033:(0.0262) \n",
      "\t\tSkipping proto 0 of 004+086\n",
      "\t\tProto:2 004:(0.024) 031:(0.0245) 032:(0.0302) 033:(0.025) \n",
      "\t\tSkipping proto 2 of 004+086\n",
      "\t\tProto:3 004:(0.0231) 031:(0.0253) 032:(0.0489) 033:(0.0238) \n",
      "\t\tSkipping proto 3 of 004+086\n",
      "\t\tProto:7 004:(0.0222) 031:(0.0235) 032:(0.0226) 033:(0.0234) \n",
      "\t\tSkipping proto 7 of 004+086\n",
      "\t\tProto:8 004:(0.0293) 031:(0.0278) 032:(0.0221) 033:(0.0259) \n",
      "\t\tSkipping proto 8 of 004+086\n",
      "\t\tProto:10 004:(0.0447) 031:(0.0466) 032:(0.0491) 033:(0.0492) \n",
      "\t\tSkipping proto 10 of 004+086\n",
      "\t\tProto:11 004:(0.021) 031:(0.0227) 032:(0.0244) 033:(0.0252) \n",
      "\t\tSkipping proto 11 of 004+086\n",
      "\t\tProto:12 004:(0.0251) 031:(0.0289) 032:(0.026) 033:(0.0283) \n",
      "\t\tSkipping proto 12 of 004+086\n",
      "\t Child: 086+045\n",
      "\t\tProto:35 001:(0.0254) 002:(0.0278) 003:(0.0243) 023:(0.025) 024:(0.0304) 025:(0.0235) 045:(0.0263) 086:(0.0246) 100:(0.0255) 101:(0.0278) \n",
      "\t\tSkipping proto 35 of 004+086\n",
      "\t\tProto:38 001:(0.0305) 002:(0.0264) 003:(0.0289) 023:(0.0315) 024:(0.0299) 025:(0.0302) 045:(0.0335) 086:(0.0282) 100:(0.0235) 101:(0.0231) \n",
      "\t\tSkipping proto 38 of 004+086\n",
      "\t\tProto:43 001:(0.0401) 002:(0.0296) 003:(0.0318) 023:(0.0309) 024:(0.0523) 025:(0.0395) 045:(0.0298) 086:(0.0391) 100:(0.0335) 101:(0.0386) \n",
      "\t\tSkipping proto 43 of 004+086\n",
      "\t\tProto:44 001:(0.0308) 002:(0.0328) 003:(0.0254) 023:(0.0304) 024:(0.0258) 025:(0.0266) 045:(0.0314) 086:(0.0256) 100:(0.0343) 101:(0.0344) \n",
      "\t\tSkipping proto 44 of 004+086\n",
      "\t\tProto:47 001:(0.0334) 002:(0.0365) 003:(0.0348) 023:(0.0308) 024:(0.0305) 025:(0.0306) 045:(0.0283) 086:(0.0268) 100:(0.0325) 101:(0.0328) \n",
      "\t\tSkipping proto 47 of 004+086\n",
      "\t\tProto:48 001:(0.0258) 002:(0.0268) 003:(0.026) 023:(0.0228) 024:(0.0233) 025:(0.0251) 045:(0.0241) 086:(0.0227) 100:(0.0228) 101:(0.0241) \n",
      "\t\tSkipping proto 48 of 004+086\n",
      "\t\tProto:52 001:(0.0333) 002:(0.0255) 003:(0.0261) 023:(0.0279) 024:(0.0282) 025:(0.0279) 045:(0.0265) 086:(0.0249) 100:(0.0281) 101:(0.0261) \n",
      "\t\tSkipping proto 52 of 004+086\n",
      "\t\tProto:25 001:(0.0365) 002:(0.0314) 003:(0.035) 023:(0.0351) 024:(0.0366) 025:(0.0332) 045:(0.031) 086:(0.04) 100:(0.0323) 101:(0.0316) \n",
      "\t\tSkipping proto 25 of 004+086\n",
      "\t\tProto:26 001:(0.0277) 002:(0.03) 003:(0.0266) 023:(0.0288) 024:(0.0295) 025:(0.0295) 045:(0.0307) 086:(0.0285) 100:(0.0252) 101:(0.0348) \n",
      "\t\tSkipping proto 26 of 004+086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 90it [00:02, 34.39it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 053+050\n",
      "Num protos for cub_053_Western_Grebe 3\n",
      "Num protos for 050+051 4\n",
      "\t Child: cub_053_Western_Grebe\n",
      "\t\tProto:0 053:(0.1081) \n",
      "\t\tSkipping proto 0 of 053+050\n",
      "\t\tProto:1 053:(0.103) \n",
      "\t\tSkipping proto 1 of 053+050\n",
      "\t\tProto:2 053:(0.5235) \n",
      "\t Child: 050+051\n",
      "\t\tProto:11 050:(0.1224) 051:(0.1154) \n",
      "\t\tSkipping proto 11 of 053+050\n",
      "\t\tProto:4 050:(0.1346) 051:(0.1247) \n",
      "\t\tSkipping proto 4 of 053+050\n",
      "\t\tProto:5 050:(0.1163) 051:(0.1254) \n",
      "\t\tSkipping proto 5 of 053+050\n",
      "\t\tProto:6 050:(0.8862) 051:(0.9064) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 120it [00:03, 37.00it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 004+032\n",
      "Num protos for cub_004_Groove_billed_Ani 1\n",
      "Num protos for 032+033 2\n",
      "\t Child: cub_004_Groove_billed_Ani\n",
      "\t\tProto:3 004:(0.0781) \n",
      "\t\tSkipping proto 3 of 004+032\n",
      "\t Child: 032+033\n",
      "\t\tProto:10 031:(0.0872) 032:(0.101) 033:(0.0895) \n",
      "\t\tSkipping proto 10 of 004+032\n",
      "\t\tProto:12 031:(0.8162) 032:(0.8076) 033:(0.8222) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 300it [00:08, 34.57it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 086+045\n",
      "Num protos for cub_086_Pacific_Loon 0\n",
      "Num protos for 045+101 27\n",
      "\t Child: 045+101\n",
      "\t\tProto:4 001:(0.0328) 002:(0.0353) 003:(0.0312) 023:(0.0309) 024:(0.0304) 025:(0.0315) 045:(0.0438) 100:(0.0332) 101:(0.0627) \n",
      "\t\tSkipping proto 4 of 086+045\n",
      "\t\tProto:5 001:(0.0385) 002:(0.0384) 003:(0.0387) 023:(0.0401) 024:(0.0327) 025:(0.0357) 045:(0.0353) 100:(0.0388) 101:(0.0337) \n",
      "\t\tSkipping proto 5 of 086+045\n",
      "\t\tProto:6 001:(0.5728) 002:(0.5738) 003:(0.571) 023:(0.157) 024:(0.2887) 025:(0.2968) 045:(0.52) 100:(0.3532) 101:(0.2811) \n",
      "\t\tProto:7 001:(0.0328) 002:(0.0351) 003:(0.0362) 023:(0.031) 024:(0.0304) 025:(0.0306) 045:(0.0366) 100:(0.0407) 101:(0.0437) \n",
      "\t\tSkipping proto 7 of 086+045\n",
      "\t\tProto:8 001:(0.0376) 002:(0.0363) 003:(0.0402) 023:(0.0321) 024:(0.0423) 025:(0.0341) 045:(0.0346) 100:(0.0702) 101:(0.0443) \n",
      "\t\tSkipping proto 8 of 086+045\n",
      "\t\tProto:11 001:(0.0627) 002:(0.0631) 003:(0.0638) 023:(0.0363) 024:(0.0313) 025:(0.0431) 045:(0.0624) 100:(0.032) 101:(0.0363) \n",
      "\t\tSkipping proto 11 of 086+045\n",
      "\t\tProto:12 001:(0.0407) 002:(0.0522) 003:(0.0414) 023:(0.0354) 024:(0.043) 025:(0.0408) 045:(0.0468) 100:(0.0401) 101:(0.0597) \n",
      "\t\tSkipping proto 12 of 086+045\n",
      "\t\tProto:16 001:(0.043) 002:(0.0409) 003:(0.0424) 023:(0.038) 024:(0.0379) 025:(0.0469) 045:(0.0393) 100:(0.0347) 101:(0.0314) \n",
      "\t\tSkipping proto 16 of 086+045\n",
      "\t\tProto:17 001:(0.0464) 002:(0.0439) 003:(0.0487) 023:(0.074) 024:(0.0722) 025:(0.0741) 045:(0.0383) 100:(0.0741) 101:(0.0374) \n",
      "\t\tSkipping proto 17 of 086+045\n",
      "\t\tProto:18 001:(0.0827) 002:(0.0931) 003:(0.0741) 023:(0.0371) 024:(0.0367) 025:(0.0456) 045:(0.045) 100:(0.1316) 101:(0.1433) \n",
      "\t\tSkipping proto 18 of 086+045\n",
      "\t\tProto:19 001:(0.0517) 002:(0.0372) 003:(0.0525) 023:(0.0963) 024:(0.0989) 025:(0.0958) 045:(0.0363) 100:(0.0931) 101:(0.0385) \n",
      "\t\tSkipping proto 19 of 086+045\n",
      "\t\tProto:20 001:(0.0439) 002:(0.0401) 003:(0.0411) 023:(0.0359) 024:(0.0405) 025:(0.0398) 045:(0.041) 100:(0.0333) 101:(0.0326) \n",
      "\t\tSkipping proto 20 of 086+045\n",
      "\t\tProto:21 001:(0.0543) 002:(0.0549) 003:(0.0541) 023:(0.0383) 024:(0.0368) 025:(0.0419) 045:(0.0355) 100:(0.0453) 101:(0.0407) \n",
      "\t\tSkipping proto 21 of 086+045\n",
      "\t\tProto:22 001:(0.0401) 002:(0.0408) 003:(0.0406) 023:(0.0359) 024:(0.0378) 025:(0.0353) 045:(0.0429) 100:(0.0416) 101:(0.0422) \n",
      "\t\tSkipping proto 22 of 086+045\n",
      "\t\tProto:23 001:(0.0311) 002:(0.0392) 003:(0.0341) 023:(0.0365) 024:(0.0358) 025:(0.0364) 045:(0.0399) 100:(0.0543) 101:(0.056) \n",
      "\t\tSkipping proto 23 of 086+045\n",
      "\t\tProto:24 001:(0.0455) 002:(0.0431) 003:(0.0435) 023:(0.0528) 024:(0.0535) 025:(0.0529) 045:(0.0379) 100:(0.0562) 101:(0.0381) \n",
      "\t\tSkipping proto 24 of 086+045\n",
      "\t\tProto:25 001:(0.0529) 002:(0.0531) 003:(0.0478) 023:(0.0533) 024:(0.052) 025:(0.0499) 045:(0.0495) 100:(0.0407) 101:(0.0362) \n",
      "\t\tSkipping proto 25 of 086+045\n",
      "\t\tProto:26 001:(0.0489) 002:(0.0432) 003:(0.0475) 023:(0.0356) 024:(0.0427) 025:(0.0435) 045:(0.0462) 100:(0.0327) 101:(0.0402) \n",
      "\t\tSkipping proto 26 of 086+045\n",
      "\t\tProto:27 001:(0.0364) 002:(0.0341) 003:(0.0323) 023:(0.037) 024:(0.0444) 025:(0.0342) 045:(0.0363) 100:(0.0326) 101:(0.0373) \n",
      "\t\tSkipping proto 27 of 086+045\n",
      "\t\tProto:28 001:(0.032) 002:(0.0361) 003:(0.0317) 023:(0.034) 024:(0.0347) 025:(0.0359) 045:(0.0346) 100:(0.0334) 101:(0.0335) \n",
      "\t\tSkipping proto 28 of 086+045\n",
      "\t\tProto:29 001:(0.0324) 002:(0.0349) 003:(0.0406) 023:(0.0339) 024:(0.0317) 025:(0.0341) 045:(0.0365) 100:(0.054) 101:(0.0588) \n",
      "\t\tSkipping proto 29 of 086+045\n",
      "\t\tProto:30 001:(0.0407) 002:(0.0329) 003:(0.0329) 023:(0.0346) 024:(0.0297) 025:(0.0351) 045:(0.0346) 100:(0.0326) 101:(0.0344) \n",
      "\t\tSkipping proto 30 of 086+045\n",
      "\t\tProto:31 001:(0.0324) 002:(0.0308) 003:(0.031) 023:(0.0331) 024:(0.0308) 025:(0.0331) 045:(0.0307) 100:(0.0326) 101:(0.0318) \n",
      "\t\tSkipping proto 31 of 086+045\n",
      "\t\tProto:34 001:(0.0363) 002:(0.0313) 003:(0.032) 023:(0.0372) 024:(0.0377) 025:(0.036) 045:(0.0322) 100:(0.0336) 101:(0.032) \n",
      "\t\tSkipping proto 34 of 086+045\n",
      "\t\tProto:35 001:(0.0378) 002:(0.0358) 003:(0.0355) 023:(0.0393) 024:(0.0373) 025:(0.039) 045:(0.0375) 100:(0.0351) 101:(0.0342) \n",
      "\t\tSkipping proto 35 of 086+045\n",
      "\t\tProto:38 001:(0.0502) 002:(0.0411) 003:(0.0548) 023:(0.063) 024:(0.0655) 025:(0.0648) 045:(0.0439) 100:(0.041) 101:(0.0452) \n",
      "\t\tSkipping proto 38 of 086+045\n",
      "\t\tProto:39 001:(0.0404) 002:(0.0441) 003:(0.0459) 023:(0.0463) 024:(0.0497) 025:(0.0379) 045:(0.0459) 100:(0.0369) 101:(0.0402) \n",
      "\t\tSkipping proto 39 of 086+045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 100% 60/60 [00:01<00:00, 30.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 050+051\n",
      "Num protos for cub_050_Eared_Grebe 4\n",
      "Num protos for cub_051_Horned_Grebe 3\n",
      "\t Child: cub_050_Eared_Grebe\n",
      "\t\tProto:0 050:(0.8965) \n",
      "\t\tProto:1 050:(0.5544) \n",
      "\t\tProto:2 050:(0.9088) \n",
      "\t\tProto:3 050:(0.8808) \n",
      "\t Child: cub_051_Horned_Grebe\n",
      "\t\tProto:4 051:(0.1499) \n",
      "\t\tSkipping proto 4 of 050+051\n",
      "\t\tProto:5 051:(0.9157) \n",
      "\t\tProto:6 051:(0.157) \n",
      "\t\tSkipping proto 6 of 050+051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 90it [00:02, 34.31it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 032+033\n",
      "Num protos for cub_032_Mangrove_Cuckoo 3\n",
      "Num protos for 033+031 4\n",
      "\t Child: cub_032_Mangrove_Cuckoo\n",
      "\t\tProto:0 032:(0.8797) \n",
      "\t\tProto:1 032:(0.9113) \n",
      "\t\tProto:2 032:(0.106) \n",
      "\t\tSkipping proto 2 of 032+033\n",
      "\t Child: 033+031\n",
      "\t\tProto:8 031:(0.7527) 033:(0.6906) \n",
      "\t\tProto:10 031:(0.3833) 033:(0.2276) \n",
      "\t\tProto:5 031:(0.1159) 033:(0.1121) \n",
      "\t\tSkipping proto 5 of 032+033\n",
      "\t\tProto:6 031:(0.8894) 033:(0.8765) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 270it [00:06, 38.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 045+101\n",
      "Num protos for 045+003 5\n",
      "Num protos for 101+023 12\n",
      "\t Child: 045+003\n",
      "\t\tProto:2 001:(0.7293) 002:(0.6664) 003:(0.7455) 045:(0.7484) \n",
      "\t\tProto:4 001:(0.7608) 002:(0.7598) 003:(0.7595) 045:(0.7472) \n",
      "\t\tProto:7 001:(0.3054) 002:(0.3054) 003:(0.3144) 045:(0.3455) \n",
      "\t\tProto:11 001:(0.1401) 002:(0.1422) 003:(0.0364) 045:(0.0871) \n",
      "\t\tSkipping proto 11 of 045+101\n",
      "\t\tProto:13 001:(0.0581) 002:(0.3632) 003:(0.0659) 045:(0.5077) \n",
      "\t Child: 101+023\n",
      "\t\tProto:32 023:(0.0408) 024:(0.0358) 025:(0.0425) 100:(0.0369) 101:(0.046) \n",
      "\t\tSkipping proto 32 of 045+101\n",
      "\t\tProto:33 023:(0.0388) 024:(0.0489) 025:(0.0425) 100:(0.0425) 101:(0.2651) \n",
      "\t\tProto:16 023:(0.0386) 024:(0.0337) 025:(0.0327) 100:(0.0626) 101:(0.0679) \n",
      "\t\tSkipping proto 16 of 045+101\n",
      "\t\tProto:17 023:(0.0387) 024:(0.0403) 025:(0.0399) 100:(0.0396) 101:(0.0385) \n",
      "\t\tSkipping proto 17 of 045+101\n",
      "\t\tProto:18 023:(0.0374) 024:(0.0376) 025:(0.0372) 100:(0.0396) 101:(0.0362) \n",
      "\t\tSkipping proto 18 of 045+101\n",
      "\t\tProto:21 023:(0.0617) 024:(0.0662) 025:(0.065) 100:(0.063) 101:(0.0746) \n",
      "\t\tSkipping proto 21 of 045+101\n",
      "\t\tProto:23 023:(0.0878) 024:(0.0908) 025:(0.0818) 100:(0.6366) 101:(0.6608) \n",
      "\t\tProto:24 023:(0.0388) 024:(0.0376) 025:(0.0377) 100:(0.0363) 101:(0.0348) \n",
      "\t\tSkipping proto 24 of 045+101\n",
      "\t\tProto:25 023:(0.0415) 024:(0.041) 025:(0.0416) 100:(0.0513) 101:(0.0507) \n",
      "\t\tSkipping proto 25 of 045+101\n",
      "\t\tProto:29 023:(0.0429) 024:(0.0601) 025:(0.0409) 100:(0.0772) 101:(0.0443) \n",
      "\t\tSkipping proto 29 of 045+101\n",
      "\t\tProto:30 023:(0.0395) 024:(0.0361) 025:(0.0364) 100:(0.0754) 101:(0.0979) \n",
      "\t\tSkipping proto 30 of 045+101\n",
      "\t\tProto:31 023:(0.0385) 024:(0.0377) 025:(0.0363) 100:(0.0365) 101:(0.0858) \n",
      "\t\tSkipping proto 31 of 045+101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 100% 60/60 [00:02<00:00, 29.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 033+031\n",
      "Num protos for cub_033_Yellow_billed_Cuckoo 3\n",
      "Num protos for cub_031_Black_billed_Cuckoo 3\n",
      "\t Child: cub_033_Yellow_billed_Cuckoo\n",
      "\t\tProto:0 033:(0.9301) \n",
      "\t\tProto:2 033:(0.152) \n",
      "\t\tSkipping proto 2 of 033+031\n",
      "\t\tProto:3 033:(0.1637) \n",
      "\t\tSkipping proto 3 of 033+031\n",
      "\t Child: cub_031_Black_billed_Cuckoo\n",
      "\t\tProto:5 031:(0.1627) \n",
      "\t\tSkipping proto 5 of 033+031\n",
      "\t\tProto:6 031:(0.5268) \n",
      "\t\tProto:7 031:(0.9121) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 120it [00:03, 37.05it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 045+003\n",
      "Num protos for cub_045_Northern_Fulmar 1\n",
      "Num protos for 003+002 4\n",
      "\t Child: cub_045_Northern_Fulmar\n",
      "\t\tProto:3 045:(0.0807) \n",
      "\t\tSkipping proto 3 of 045+003\n",
      "\t Child: 003+002\n",
      "\t\tProto:8 001:(0.1809) 002:(0.0932) 003:(0.1085) \n",
      "\t\tSkipping proto 8 of 045+003\n",
      "\t\tProto:11 001:(0.0805) 002:(0.0787) 003:(0.0799) \n",
      "\t\tSkipping proto 11 of 045+003\n",
      "\t\tProto:12 001:(0.7509) 002:(0.7598) 003:(0.7556) \n",
      "\t\tProto:13 001:(0.1016) 002:(0.0844) 003:(0.3142) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 150it [00:03, 38.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 101+023\n",
      "Num protos for 101+100 2\n",
      "Num protos for 023+025 2\n",
      "\t Child: 101+100\n",
      "\t\tProto:0 100:(0.6591) 101:(0.6969) \n",
      "\t\tProto:5 100:(0.0667) 101:(0.0698) \n",
      "\t\tSkipping proto 5 of 101+023\n",
      "\t Child: 023+025\n",
      "\t\tProto:16 023:(0.7932) 024:(0.7899) 025:(0.791) \n",
      "\t\tProto:12 023:(0.8357) 024:(0.8372) 025:(0.8368) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 90it [00:02, 33.77it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 003+002\n",
      "Num protos for cub_003_Sooty_Albatross 2\n",
      "Num protos for 002+001 3\n",
      "\t Child: cub_003_Sooty_Albatross\n",
      "\t\tProto:0 003:(0.1079) \n",
      "\t\tSkipping proto 0 of 003+002\n",
      "\t\tProto:1 003:(0.1216) \n",
      "\t\tSkipping proto 1 of 003+002\n",
      "\t Child: 002+001\n",
      "\t\tProto:10 001:(0.8908) 002:(0.8939) \n",
      "\t\tProto:4 001:(0.7802) 002:(0.8349) \n",
      "\t\tProto:5 001:(0.879) 002:(0.1729) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 100% 60/60 [00:01<00:00, 31.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 101+100\n",
      "Num protos for cub_101_White_Pelican 4\n",
      "Num protos for cub_100_Brown_Pelican 3\n",
      "\t Child: cub_101_White_Pelican\n",
      "\t\tProto:0 101:(0.8888) \n",
      "\t\tProto:1 101:(0.151) \n",
      "\t\tSkipping proto 1 of 101+100\n",
      "\t\tProto:2 101:(0.9295) \n",
      "\t\tProto:3 101:(0.8451) \n",
      "\t Child: cub_100_Brown_Pelican\n",
      "\t\tProto:5 100:(0.1461) \n",
      "\t\tSkipping proto 5 of 101+100\n",
      "\t\tProto:6 100:(0.8839) \n",
      "\t\tProto:7 100:(0.9197) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 90it [00:02, 37.55it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 023+025\n",
      "Num protos for cub_023_Brandt_Cormorant 3\n",
      "Num protos for 025+024 2\n",
      "\t Child: cub_023_Brandt_Cormorant\n",
      "\t\tProto:0 023:(0.8643) \n",
      "\t\tProto:2 023:(0.8506) \n",
      "\t\tProto:3 023:(0.8563) \n",
      "\t Child: 025+024\n",
      "\t\tProto:10 024:(0.1164) 025:(0.1125) \n",
      "\t\tSkipping proto 10 of 023+025\n",
      "\t\tProto:6 024:(0.1863) 025:(0.1182) \n",
      "\t\tSkipping proto 6 of 023+025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 100% 60/60 [00:01<00:00, 30.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 002+001\n",
      "Num protos for cub_002_Laysan_Albatross 2\n",
      "Num protos for cub_001_Black_footed_Albatross 3\n",
      "\t Child: cub_002_Laysan_Albatross\n",
      "\t\tProto:0 002:(0.9329) \n",
      "\t\tProto:1 002:(0.1706) \n",
      "\t\tSkipping proto 1 of 002+001\n",
      "\t Child: cub_001_Black_footed_Albatross\n",
      "\t\tProto:4 001:(0.925) \n",
      "\t\tProto:5 001:(0.1551) \n",
      "\t\tSkipping proto 5 of 002+001\n",
      "\t\tProto:7 001:(0.8466) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 100% 60/60 [00:02<00:00, 29.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 025+024\n",
      "Num protos for cub_025_Pelagic_Cormorant 4\n",
      "Num protos for cub_024_Red_faced_Cormorant 2\n",
      "\t Child: cub_025_Pelagic_Cormorant\n",
      "\t\tProto:0 025:(0.1582) \n",
      "\t\tSkipping proto 0 of 025+024\n",
      "\t\tProto:1 025:(0.9217) \n",
      "\t\tProto:2 025:(0.1753) \n",
      "\t\tSkipping proto 2 of 025+024\n",
      "\t\tProto:3 025:(0.3016) \n",
      "\t Child: cub_024_Red_faced_Cormorant\n",
      "\t\tProto:6 024:(0.1621) \n",
      "\t\tSkipping proto 6 of 025+024\n",
      "\t\tProto:7 024:(0.9493) \n",
      "Done !!!\n"
     ]
    }
   ],
   "source": [
    "# Proto activations on leaf descendents - topk images\n",
    "\n",
    "from util.data import ModifiedLabelLoader\n",
    "from collections import defaultdict\n",
    "import heapq\n",
    "import pdb\n",
    "from util.vis_pipnet import get_img_coordinates\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import ImageFont, Image, ImageDraw as D\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "def find_top_percentile_bbox(image, percentile=95):\n",
    "    threshold = np.percentile(image.flatten(), percentile)\n",
    "    mask = image >= threshold\n",
    "    coords = np.argwhere(mask)\n",
    "    if coords.size == 0:\n",
    "        return None, None, None, None\n",
    "    h_min, w_min = coords.min(axis=0)\n",
    "    h_max, w_max = coords.max(axis=0)\n",
    "    h_min, h_max, w_min, w_max = map(int, [h_min, h_max, w_min, w_max])\n",
    "    return h_min, h_max, w_min, w_max\n",
    "\n",
    "def find_high_activation_crop(activation_map, percentile=95):\n",
    "    threshold = np.percentile(activation_map, percentile)\n",
    "    mask = np.ones(activation_map.shape)\n",
    "    mask[activation_map < threshold] = 0\n",
    "    lower_y, upper_y, lower_x, upper_x = 0, 0, 0, 0\n",
    "    for i in range(mask.shape[0]):\n",
    "        if np.amax(mask[i]) > 0.5:\n",
    "            lower_y = i\n",
    "            break\n",
    "    for i in reversed(range(mask.shape[0])):\n",
    "        if np.amax(mask[i]) > 0.5:\n",
    "            upper_y = i\n",
    "            break\n",
    "    for j in range(mask.shape[1]):\n",
    "        if np.amax(mask[:,j]) > 0.5:\n",
    "            lower_x = j\n",
    "            break\n",
    "    for j in reversed(range(mask.shape[1])):\n",
    "        if np.amax(mask[:,j]) > 0.5:\n",
    "            upper_x = j\n",
    "            break\n",
    "    return lower_y, upper_y+1, lower_x, upper_x+1\n",
    "\n",
    "def get_upscaled_activation_uninterpolated(latent_activation, image_size):\n",
    "    image_a = latent_activation.cpu().numpy()\n",
    "    min_image_a = image_a.min()\n",
    "    max_image_a = image_a.max()\n",
    "    image_a = (image_a - min_image_a) / (max_image_a - min_image_a)\n",
    "    reshaped_image_a = np.array(Image.fromarray((image_a[0] * 255).astype('uint8')).resize((image_size[-1], \\\n",
    "                                                                                            image_size[-2]), \\\n",
    "                                                                                          resample=Image.NEAREST ))\n",
    "    reshaped_image_a = (reshaped_image_a / 255).astype('float16')\n",
    "    reshaped_image_a = (reshaped_image_a * (max_image_a - min_image_a)) + min_image_a\n",
    "    return reshaped_image_a\n",
    "\n",
    "# added for NUMPY SAVING\n",
    "def get_upscaled_activation_interpolated(latent_activation, image_size):\n",
    "    image_a = latent_activation.cpu().numpy()\n",
    "    min_image_a = image_a.min()\n",
    "    max_image_a = image_a.max()\n",
    "    image_a = (image_a - min_image_a) / (max_image_a - min_image_a)\n",
    "    reshaped_image_a = np.array(Image.fromarray((image_a[0] * 255).astype('uint8')).resize((image_size[-1], \\\n",
    "                                                                                            image_size[-2])))    \n",
    "    reshaped_image_a = (reshaped_image_a / 255).astype('float16')\n",
    "    reshaped_image_a = (reshaped_image_a * (max_image_a - min_image_a)) + min_image_a\n",
    "    return reshaped_image_a\n",
    "\n",
    "def functional_UnitConv2D(in_features, weight, bias, stride = 1, padding=0):\n",
    "    normalized_weight = F.normalize(weight.data, p=2, dim=(1, 2, 3)) # Normalize the kernels to unit vectors\n",
    "    normalized_input = F.normalize(in_features, p=2, dim=1) # Normalize the input to unit vectors\n",
    "    if bias is not None:\n",
    "        normalized_bias = F.normalize(bias.data, p=2, dim=0) # Normalize the kernels to unit vectors\n",
    "    else:\n",
    "        normalized_bias = None\n",
    "    return F.conv2d(normalized_input, normalized_weight, normalized_bias, stride=stride, padding=padding)\n",
    "\n",
    "def findCorrespondingToMax(base, target):\n",
    "    output, indices = F.max_pool2d(base, kernel_size=(26, 26), return_indices=True)# these are logits\n",
    "    tensor_flattened = target.view(target.shape[0], target.shape[1], -1)\n",
    "    indices_flattened = indices.view(target.shape[0], target.shape[1], -1)\n",
    "    corresponding_values_in_target = torch.gather(tensor_flattened, 2, indices_flattened)\n",
    "    corresponding_values_in_target = corresponding_values_in_target.view(target.shape[0],\\\n",
    "                                     target.shape[1], 1, 1)\n",
    "    pooled_target = corresponding_values_in_target\n",
    "    return pooled_target\n",
    "\n",
    "def customForwardWithCSandSoftmax(net, xs,  inference=False):\n",
    "    features = net.module._net(xs) \n",
    "    proto_features = {}\n",
    "    proto_features_cs = {}\n",
    "    proto_features_softmaxed = {}\n",
    "    pooled = {}\n",
    "    pooled_cs = {}\n",
    "    pooled_softmaxed = {}\n",
    "    out = {}\n",
    "    for node in net.module.root.nodes_with_children():\n",
    "        # this may or may not be cosine similarity based on UniConv2D or Conv2d\n",
    "        proto_features[node.name] = getattr(net.module, '_'+node.name+'_add_on')(features)\n",
    "        \n",
    "        #calculating cosine similarity\n",
    "        prototypes = getattr(net.module, '_'+node.name+'_add_on')\n",
    "        proto_features_cs[node.name] = functional_UnitConv2D(features, prototypes.weight, prototypes.bias)\n",
    "\n",
    "        if net.module.args.softmax == 'y':\n",
    "            softmax_tau = 0.2\n",
    "            proto_features[node.name] = proto_features[node.name] / softmax_tau\n",
    "            proto_features_softmaxed[node.name] = net.module._softmax(proto_features[node.name])\n",
    "            proto_features[node.name] = proto_features_softmaxed[node.name] # will be overwritten if args.multiply_cs_softmax == 'y'\n",
    "        elif net.module.args.gumbel_softmax == 'y':\n",
    "            proto_features_softmaxed[node.name] = net.module._gumbel_softmax(proto_features[node.name])\n",
    "            proto_features[node.name] = proto_features_softmaxed[node.name] # will be overwritten if args.multiply_cs_softmax == 'y'\n",
    "\n",
    "        if net.module.args.multiply_cs_softmax == 'y':\n",
    "            proto_features[node.name] = proto_features_cs[node.name] * proto_features_softmaxed[node.name]\n",
    "        pooled[node.name] = net.module._pool(proto_features[node.name])\n",
    "        \n",
    "        # this could be softmax or cosine similarity\n",
    "        pooled_cs[node.name] = findCorrespondingToMax(base=proto_features[node.name], \\\n",
    "                                                     target=proto_features_cs[node.name])\n",
    "        \n",
    "        pooled_softmaxed[node.name] = findCorrespondingToMax(base=proto_features[node.name], \\\n",
    "                                                     target=proto_features_softmaxed[node.name])\n",
    "\n",
    "        if inference:\n",
    "            pooled[node.name] = torch.where(pooled[node.name] < 0.1, 0., pooled[node.name])  #during inference, ignore all prototypes that have 0.1 similarity or lower\n",
    "        out[node.name] = getattr(net.module, '_'+node.name+'_classification')(pooled[node.name]) #shape (bs*2, num_classes) # these are logits\n",
    "    \"\"\"\n",
    "    features -> Raw features generated by the backbone before the prototype layer\n",
    "    proto_features -> Output of prototype layer (UnitConv2D or Conv2D based on the network configuration used), and softmaxed\n",
    "    proto_features_cs -> Cosine similarity between features and prototypes\n",
    "    proto_features_softmaxed -> Same as proto_features\n",
    "    pooled -> max pooled on proto_features\n",
    "    pooled_cs -> cosine values corresponding to max indices in proto_features\n",
    "    \"\"\"\n",
    "    return features, proto_features, proto_features_cs, proto_features_softmaxed, pooled, pooled_cs, pooled_softmaxed, out\n",
    "#     return features, proto_features, pooled, pooled_cs, pooled_softmaxed, out\n",
    "\n",
    "find_non_descendants = False # True, False # param\n",
    "vizloader_name = 'projectloader'\n",
    "bbox_percentile = 97\n",
    "topk = 6 # param, args param\n",
    "save_images = False #True\n",
    "save_activation_as_npy_path = 'activation_as_npy' # activation_as_npy, added for NUMPY SAVING\n",
    "analysis_mode = True\n",
    "\n",
    "font = ImageFont.truetype(\"arial.ttf\", 50)\n",
    "font2 = ImageFont.truetype(\"arial.ttf\", 20)\n",
    "font3 = ImageFont.truetype(\"arial.ttf\", 30)\n",
    "\n",
    "from datetime import datetime\n",
    "txt_file = open(os.path.join(run_path, \"num_proto_details_\"+datetime.now().strftime(\"%m:%d:%H:%M:%S\")+\".txt\"), \"a\")\n",
    "txt_file.write('\\n')\n",
    "\n",
    "def write_num_proto_details(proto_mean_activations, node_name, net, threshold, txt_file, args):\n",
    "    \n",
    "    rand_input = torch.randn((1, 3, args.image_size, args.image_size))\n",
    "    with torch.no_grad():\n",
    "        *_, pooled, out = net(rand_input)\n",
    "    num_protos = pooled[node_name].shape[1]\n",
    "    used_protos = len(proto_mean_activations)\n",
    "    non_overspecific = 0\n",
    "    for p in proto_mean_activations:\n",
    "        logstr = '\\t'*2 + f'Proto:{p} '\n",
    "        protos_mean_for_all_leaf_descedants = []\n",
    "        for leaf_descendent in proto_mean_activations[p]:\n",
    "            mean_activation = round(np.mean([activation for activation, *_ in proto_mean_activations[p][leaf_descendent]]), 4)\n",
    "            protos_mean_for_all_leaf_descedants.append(mean_activation)\n",
    "            \n",
    "        if all([(mean_activation>0.2) for mean_activation in protos_mean_for_all_leaf_descedants]):\n",
    "            non_overspecific += 1\n",
    "            \n",
    "    txt_file.write(f\"Node:{node_name},Total:{num_protos},Used:{used_protos},Good:{non_overspecific},threshold={threshold}\\n\")\n",
    "\n",
    "\n",
    "def get_heap():\n",
    "    list_ = []\n",
    "    heapq.heapify(list_)\n",
    "    return list_\n",
    "\n",
    "patchsize, skip = get_patch_size(args)\n",
    "\n",
    "\n",
    "\n",
    "vizloader_dict = {'trainloader': trainloader,\n",
    "                 'projectloader': projectloader,\n",
    "                 'testloader': testloader,\n",
    "                 'test_projectloader': test_projectloader}\n",
    "vizloader_dict[vizloader_name] = unshuffle_dataloader(vizloader_dict[vizloader_name])\n",
    "\n",
    "\n",
    "if type(vizloader_dict[vizloader_name].dataset) == ImageFolder:\n",
    "    name2label = vizloader_dict[vizloader_name].dataset.class_to_idx\n",
    "    label2name = {label:name for name, label in name2label.items()}\n",
    "else:\n",
    "    name2label = vizloader_dict[vizloader_name].dataset.dataset.dataset.class_to_idx\n",
    "    label2name = {label:name for name, label in name2label.items()}\n",
    "\n",
    "for node in root.nodes_with_children():\n",
    "    \n",
    "#     if node.name == 'root':\n",
    "#         continue\n",
    "\n",
    "#     non_leaf_children_names = [child.name for child in node.children if not child.is_leaf()]\n",
    "#     if len(non_leaf_children_names) == 0: # if all the children are leaf nodes then skip this node\n",
    "#         continue\n",
    "\n",
    "#     name2label = projectloader.dataset.class_to_idx # param\n",
    "#     label2name = {label:name for name, label in name2label.items()}\n",
    "    modifiedLabelLoader = ModifiedLabelLoader(vizloader_dict[vizloader_name], node)\n",
    "    coarse_label2name = modifiedLabelLoader.modifiedlabel2name\n",
    "    node_label_to_children = {label: name for name, label in node.children_to_labels.items()}\n",
    "    \n",
    "    imgs = modifiedLabelLoader.filtered_imgs\n",
    "\n",
    "    img_iter = tqdm(enumerate(modifiedLabelLoader),\n",
    "                    total=len(modifiedLabelLoader),\n",
    "                    mininterval=50.,\n",
    "                    desc='Collecting topk',\n",
    "                    ncols=0)\n",
    "\n",
    "    classification_weights = getattr(net.module, '_'+node.name+'_classification').weight\n",
    "    \n",
    "    \n",
    "    \n",
    "    # maps proto_number -> grand_child_name (or descendant leaf name) -> list of top-k activations\n",
    "    proto_mean_activations = defaultdict(lambda: defaultdict(get_heap))\n",
    "\n",
    "    # maps class names to the prototypes that belong to that\n",
    "    class_and_prototypes = defaultdict(set)\n",
    "\n",
    "    for i, (xs, orig_y, ys) in img_iter:\n",
    "        # change\n",
    "#         if not find_non_descendants: \n",
    "#             # do only when finding descendants\n",
    "#             if coarse_label2name[ys.item()] not in non_leaf_children_names:\n",
    "#                 continue\n",
    "\n",
    "        xs, ys = xs.to(device), ys.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model_output = customForwardWithCSandSoftmax(net, xs, inference=False)\n",
    "            features, softmaxes, cosine_similarity, _, pooled, pooled_cs, pooled_softmaxed, out = model_output\n",
    "#             _, softmaxes, pooled, pooled_ip, pooled_softmax, _ = model_output # features, proto_features, pooled, pooled_cs, pooled_softmaxed, out\n",
    "#             model_output = net(xs, inference=False)\n",
    "#             if len(model_output) == 3:\n",
    "#                 softmaxes, pooled, _ = model_output\n",
    "#             elif len(model_output) == 4:\n",
    "#                 _, softmaxes, pooled, _ = model_output\n",
    "            pooled = pooled[node.name].squeeze(0)\n",
    "            pooled_cs = pooled_cs[node.name].squeeze(0) \n",
    "            softmaxes = softmaxes[node.name]#.squeeze(0)\n",
    "            cosine_similarity = cosine_similarity[node.name]\n",
    "\n",
    "            for p in range(pooled.shape[0]): # pooled.shape -> [768] (== num of prototypes)\n",
    "                c_weight = torch.max(classification_weights[:,p]) # classification_weights[:,p].shape -> [200] (== num of classes)\n",
    "                relevant_proto_classes = torch.nonzero(classification_weights[:, p] > 1e-3)\n",
    "                relevant_proto_class_names = [node_label_to_children[class_idx.item()] for class_idx in relevant_proto_classes]\n",
    "                \n",
    "                # Take the max per prototype.                             \n",
    "                max_per_prototype, max_idx_per_prototype = torch.max(softmaxes, dim=0)\n",
    "                max_per_prototype_h, max_idx_per_prototype_h = torch.max(max_per_prototype, dim=1)\n",
    "                max_per_prototype_w, max_idx_per_prototype_w = torch.max(max_per_prototype_h, dim=1) #shape (num_prototypes)\n",
    "                \n",
    "                h_idx = max_idx_per_prototype_h[p, max_idx_per_prototype_w[p]]\n",
    "                w_idx = max_idx_per_prototype_w[p]\n",
    "\n",
    "                if len(relevant_proto_class_names) == 0:\n",
    "                    continue\n",
    "                \n",
    "                # change\n",
    "#                 if (len(relevant_proto_class_names) == 1):# and (relevant_proto_class_names[0] not in non_leaf_children_names):\n",
    "#                     continue\n",
    "                \n",
    "                h_coor_min, h_coor_max, w_coor_min, w_coor_max = get_img_coordinates(args.image_size, softmaxes.shape, patchsize, skip, h_idx, w_idx)\n",
    "                latent_activation = softmaxes[:, p, :, :]\n",
    "                latent_activation_cs = cosine_similarity[:, p, :, :]\n",
    "                if not find_non_descendants:\n",
    "                    if (coarse_label2name[ys.item()] in relevant_proto_class_names):\n",
    "                        child_node = root.get_node(coarse_label2name[ys.item()])\n",
    "                        leaf_descendent = label2name[orig_y.item()][4:7] if analysis_mode else \\\n",
    "                                                label2name[orig_y.item()][4:]\n",
    "                        img_to_open = imgs[i][0] # it is a tuple of (path to image, lable)\n",
    "                        if topk and (len(proto_mean_activations[p][leaf_descendent]) >= topk):\n",
    "                            heapq.heappushpop(proto_mean_activations[p][leaf_descendent],\\\n",
    "                                              (pooled[p].item(), pooled_cs[p].item(), img_to_open,\\\n",
    "                                               (h_coor_min, h_coor_max, w_coor_min, w_coor_max), \\\n",
    "                                               latent_activation, latent_activation_cs))\n",
    "                        else:\n",
    "                            heapq.heappush(proto_mean_activations[p][leaf_descendent],\\\n",
    "                                           (pooled[p].item(), pooled_cs[p].item(), img_to_open,\\\n",
    "                                            (h_coor_min, h_coor_max, w_coor_min, w_coor_max), \\\n",
    "                                            latent_activation, latent_activation_cs))\n",
    "                else:\n",
    "                    if (coarse_label2name[ys.item()] not in relevant_proto_class_names):\n",
    "                        child_node = root.get_node(coarse_label2name[ys.item()])\n",
    "                        leaf_descendent = label2name[orig_y.item()][4:7] if analysis_mode else \\\n",
    "                                                label2name[orig_y.item()][4:]\n",
    "                        img_to_open = imgs[i][0] # it is a tuple of (path to image, lable)\n",
    "                        if topk and (len(proto_mean_activations[p][leaf_descendent]) >= topk):\n",
    "                            heapq.heappushpop(proto_mean_activations[p][leaf_descendent],\\\n",
    "                                              (pooled[p].item(), pooled_cs[p].item(), img_to_open,\\\n",
    "                                               (h_coor_min, h_coor_max, w_coor_min, w_coor_max), \\\n",
    "                                               latent_activation, latent_activation_cs))\n",
    "                        else:\n",
    "                            heapq.heappush(proto_mean_activations[p][leaf_descendent],\\\n",
    "                                           (pooled[p].item(), pooled_cs[p].item(), img_to_open,\\\n",
    "                                            (h_coor_min, h_coor_max, w_coor_min, w_coor_max), \\\n",
    "                                            latent_activation, latent_activation_cs))\n",
    "                class_and_prototypes[', '.join(relevant_proto_class_names)].add(p)\n",
    "                \n",
    "    write_num_proto_details(proto_mean_activations, node.name, net, threshold=0.2, txt_file=txt_file, args=args)\n",
    "    \n",
    "    print('Node', node.name)\n",
    "    for class_label in range(classification_weights.shape[0]):\n",
    "        child_name = (coarse_label2name[class_label])\n",
    "        print('Num protos for', child_name, torch.nonzero(classification_weights[class_label, :] > 1e-3).shape[0])\n",
    "        \n",
    "    for child_classname in class_and_prototypes:\n",
    "        \n",
    "        print('\\t'*1, 'Child:', child_classname)\n",
    "        for p in class_and_prototypes[child_classname]:\n",
    "            \n",
    "            logstr = '\\t'*2 + f'Proto:{p} '\n",
    "            mean_activation_of_every_leaf = []\n",
    "            for leaf_descendent in proto_mean_activations[p]:\n",
    "                mean_activation = round(np.mean([activation for activation, *_ in proto_mean_activations[p][leaf_descendent]]), 4)\n",
    "                num_images = len(proto_mean_activations[p][leaf_descendent])\n",
    "                logstr += f'{leaf_descendent}:({mean_activation}) '\n",
    "                mean_activation_of_every_leaf.append(mean_activation)\n",
    "            print(logstr)\n",
    "            \n",
    "            # if the mean_activation is less for all leaf descendants skip the node\n",
    "            if all([mean_act < 0.2 for mean_act in mean_activation_of_every_leaf]):\n",
    "                print('\\t'*2 + f'Skipping proto {p} of {node.name}')\n",
    "                continue\n",
    "            \n",
    "            # have this for NON descendants\n",
    "            if len(proto_mean_activations[p]) == 0:\n",
    "                continue\n",
    "            \n",
    "            if save_images or save_activation_as_npy_path:\n",
    "                patches = []\n",
    "                right_descriptions = []\n",
    "                text_region_width = 3 if analysis_mode else 2 # 3x the width of a patch\n",
    "                for leaf_descendent, heap in proto_mean_activations[p].items():\n",
    "                    heap = sorted(heap)[::-1]\n",
    "                    mean_activation = round(np.mean([activation for activation, *_ in proto_mean_activations[p][leaf_descendent]]), 2)\n",
    "                    least_activation = min([round(activation, 2) for activation, *_ in proto_mean_activations[p][leaf_descendent]])\n",
    "                    most_activation = max([round(activation, 2) for activation, *_ in proto_mean_activations[p][leaf_descendent]])\n",
    "                    mean_cosine_similarity = round(np.mean([activation_inner_product for _, activation_inner_product, *_ in proto_mean_activations[p][leaf_descendent]]), 2)\n",
    "                    # modified for NUMPY SAVING\n",
    "                    for rank, ele in enumerate(heap):\n",
    "                        \n",
    "                        activation, activation_inner_product, img_to_open, \\\n",
    "                        (h_coor_min, h_coor_max, w_coor_min, w_coor_max), \\\n",
    "                        latent_activation, latent_activation_cs = ele\n",
    "                        \n",
    "                        image = transforms.Resize(size=(args.image_size, args.image_size))(Image.open(img_to_open))\n",
    "                        img_tensor = transforms.ToTensor()(image)#.unsqueeze_(0) #shape (1, 3, h, w)\n",
    "                        img_tensor_patch = img_tensor[:, h_coor_min:h_coor_max, w_coor_min:w_coor_max]\n",
    "#                         overlayed_image_np = get_heatmap(latent_activation, img_tensor)\n",
    "#                         overlayed_image = torch.tensor(overlayed_image_np).permute(2, 0, 1).float() / 255.\n",
    "#                         patches.append(overlayed_image)\n",
    "                        \n",
    "                        overlayed_image_np = get_heatmap(latent_activation, img_tensor)\n",
    "                        if analysis_mode:\n",
    "                            overlayed_image_pil = Image.fromarray(overlayed_image_np)\n",
    "                            draw = D.Draw(overlayed_image_pil)\n",
    "                            text = f\"{round(activation, 2), round(activation_inner_product, 2)}\"\n",
    "    #                         text_width, text_height = draw.textsize(text, font2)\n",
    "                            bbox = draw.textbbox((0, 0), text, font2)\n",
    "                            text_width = bbox[2] - bbox[0]\n",
    "                            text_height = bbox[3] - bbox[1]\n",
    "                            x, y = 224 - text_width - 5, 5  # 10 pixels padding from right\n",
    "                            draw.text((x, y), text, font=font2, fill=(255, 255, 255))\n",
    "                            overlayed_image_np = np.array(overlayed_image_pil)\n",
    "                        \n",
    "                        overlayed_image = torch.tensor(overlayed_image_np).permute(2, 0, 1).float() / 255.\n",
    "                        \n",
    "                        if analysis_mode:\n",
    "                            upscaled_similarity = get_upscaled_activation_uninterpolated(latent_activation, image_size=(args.image_size, args.image_size))\n",
    "                            h_min, h_max, w_min, w_max = get_bb_gaussian_threshold(latent_activation, sigma=1.0, \\\n",
    "                                                                                   percentile=bbox_percentile, extend_h=0, extend_w=0)\n",
    "                            bbox_coords = torch.tensor([[w_min, h_min, w_max, h_max]])\n",
    "                            overlayed_image = torchvision.utils.draw_bounding_boxes((overlayed_image * 255).type(torch.uint8), \\\n",
    "                                                                                       bbox_coords, colors='red') / 255\n",
    "                        \n",
    "#                         plt_image = overlayed_bb_image.permute(1, 2, 0)# should be H, W, C with 0 to 1\n",
    "#                         plt.imshow(plt_image)\n",
    "#                         plt.show()\n",
    "#                         pdb.set_trace()\n",
    "                        patches.append(overlayed_image)\n",
    "                        # added for NUMPY SAVING\n",
    "                        if save_activation_as_npy_path:\n",
    "                            upscaled_similarity_interpolated = get_upscaled_activation_interpolated(latent_activation,\n",
    "                                                                                       image_size=(args.image_size, args.image_size))\n",
    "                            latent_activation_npy = latent_activation.squeeze().cpu().numpy()\n",
    "                            latent_activation_cs_npy = latent_activation_cs.squeeze().cpu().numpy()\n",
    "                            data = {'node_name': node.name,\n",
    "                                    'proto_num': p,\n",
    "                                    'leaf_desc': leaf_descendent,\n",
    "                                     'rank': rank,\n",
    "                                     'img_path': img_to_open,\n",
    "                                     'img_filename': ntpath.basename(img_to_open),\n",
    "                                     'activation': latent_activation_npy,\n",
    "                                     'activation_cs': latent_activation_cs_npy,\n",
    "                                     'max_activation': activation,\n",
    "                                     'model_type': 'HPIPNET'}\n",
    "                            filename = str(rank)+ '-' + ntpath.basename(img_to_open) + '.npy'\n",
    "                            save_path = os.path.join(run_path, save_activation_as_npy_path, \\\n",
    "                                                     node.name, str(p), leaf_descendent,\n",
    "                                                     filename)\n",
    "                            os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "                            np.save(save_path, data, allow_pickle=True)\n",
    "\n",
    "                    # description on the right hand side\n",
    "                    text = f'{mean_activation}, {leaf_descendent}' if analysis_mode else \\\n",
    "                                f'{leaf_descendent}'\n",
    "                    txtimage = Image.new(\"RGB\", (patches[0].shape[-2]*text_region_width,patches[0].shape[-1]), (0, 0, 0))\n",
    "                    draw = D.Draw(txtimage)\n",
    "                    draw.text((200, patches[0].shape[1]//2), text, anchor='mm', fill=\"white\", font=font3)\n",
    "                    txttensor = transforms.ToTensor()(txtimage)#.unsqueeze_(0)\n",
    "                    right_descriptions.append(txttensor)\n",
    "                \n",
    "                # weird thing padding should be zero for non descendants else it raises some error # change\n",
    "                if find_non_descendants or (len(patches) == topk): # (len(patches) == topk) means there is only one leaf descendant\n",
    "                    padding = 0\n",
    "                else:\n",
    "                    padding = 1\n",
    "\n",
    "                grid = torchvision.utils.make_grid(patches, nrow=topk, padding=padding)\n",
    "                grid_right_descriptions = torchvision.utils.make_grid(right_descriptions, nrow=1, padding=padding)\n",
    "\n",
    "                # merging right description with the grid of images\n",
    "                grid = torch.cat([grid_right_descriptions, grid], dim=-1)\n",
    "\n",
    "                # description on the top\n",
    "                text = f'Node:{node.name}, p{p}, Child:{child_classname}' if analysis_mode else \\\n",
    "                            f'Parent node:{node.name}, Child node:{child_classname}'\n",
    "                txtimage = Image.new(\"RGB\", (grid.shape[-1], 75), (0, 0, 0))\n",
    "                draw = D.Draw(txtimage)\n",
    "                draw.text((500, patches[0].shape[1]//2), text, anchor='mm', fill=\"white\", font=font)\n",
    "                txttensor = transforms.ToTensor()(txtimage)#.unsqueeze_(0)\n",
    "\n",
    "                # merging top description with the grid of images\n",
    "                grid = torch.cat([txttensor, grid], dim=1)\n",
    "                \n",
    "                if save_images:\n",
    "                    prefix = 'non_' if find_non_descendants else ''\n",
    "                    os.makedirs(os.path.join(run_path, prefix + f'descendent_specific_topk_heatmap_{vizloader_name}_{bbox_percentile}_ep={epoch}_analysis={analysis_mode}', node.name), exist_ok=True)\n",
    "                    torchvision.utils.save_image(grid, os.path.join(run_path, prefix + f'descendent_specific_topk_heatmap_{vizloader_name}_{bbox_percentile}_ep={epoch}_analysis={analysis_mode}', node.name, f'{child_classname}-p{p}.png'))\n",
    "\n",
    "txt_file.write('\\n')\n",
    "txt_file.close()\n",
    "print('Done !!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      "\tcub_052_Pied_billed_Grebe\n",
      "\tcub_053_Western_Grebe\n",
      "\tcub_050_Eared_Grebe\n",
      "\tcub_051_Horned_Grebe\n",
      "\tcub_004_Groove_billed_Ani\n",
      "\tcub_032_Mangrove_Cuckoo\n",
      "\tcub_033_Yellow_billed_Cuckoo\n",
      "\tcub_031_Black_billed_Cuckoo\n",
      "\tcub_086_Pacific_Loon\n",
      "\tcub_045_Northern_Fulmar\n",
      "\tcub_003_Sooty_Albatross\n",
      "\tcub_002_Laysan_Albatross\n",
      "\tcub_001_Black_footed_Albatross\n",
      "\tcub_101_White_Pelican\n",
      "\tcub_100_Brown_Pelican\n",
      "\tcub_023_Brandt_Cormorant\n",
      "\tcub_025_Pelagic_Cormorant\n",
      "\tcub_024_Red_faced_Cormorant\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/tmp/ipykernel_31177/857146827.py\u001b[0m(48)\u001b[0;36meval_prototypes_cub_parts\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     46 \u001b[0;31m            \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     47 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 48 \u001b[0;31m            \u001b[0mpart_activation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     49 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     50 \u001b[0;31m            \u001b[0;32mfor\u001b[0m \u001b[0mpartid\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> activation.max()\n",
      "0.856\n",
      "ipdb> activation.min()\n",
      "0.008\n",
      "ipdb> q\n"
     ]
    }
   ],
   "source": [
    "def eval_prototypes_cub_parts(npy_path, parts_loc_path, parts_name_path, imgs_id_path, imgs_sizes_path, args):\n",
    "    patchsize, _ = get_patch_size(args)\n",
    "    imgresize = float(args.image_size)\n",
    "    \n",
    "    img_filename_to_id = dict()\n",
    "    with open(imgs_id_path) as f:\n",
    "        for line in f:\n",
    "            id, path = line.split('\\n')[0].split(' ')\n",
    "            img_filename = ntpath.basename(path)\n",
    "            img_filename_to_id[img_filename] = int(id)\n",
    "            \n",
    "    img_id_to_size = dict()\n",
    "    with open(imgs_sizes_path) as f:\n",
    "        for line in f:\n",
    "            id, width, height = line.split('\\n')[0].split(' ')\n",
    "            img_id_to_size[int(id)] = (float(width), float(height))\n",
    "            \n",
    "    img_id_to_part_xy_vis = dict()\n",
    "    with open(parts_loc_path) as f:\n",
    "        for line in f:\n",
    "            img, partid, x, y, vis = line.split('\\n')[0].split(' ')\n",
    "            img, partid = int(img), int(partid)\n",
    "            width, height = img_id_to_size[img]\n",
    "            x = float(x) / width\n",
    "            y = float(y) / height\n",
    "            if img not in img_id_to_part_xy_vis.keys():\n",
    "                img_id_to_part_xy_vis[img]=dict()\n",
    "            if vis == '1':\n",
    "                img_id_to_part_xy_vis[img][partid]=(x,y)\n",
    "                \n",
    "    partid_to_pos_in_vector = {1: 0,\n",
    "                               2: 1,\n",
    "                               3: 2,\n",
    "                               4: 3,\n",
    "                               5: 4,\n",
    "                               6: 5,\n",
    "                               7: 6, # left eye\n",
    "                               8: 7, # left leg\n",
    "                               9: 8, # left wing\n",
    "                               10: 9,\n",
    "                               11: 6, # right eye\n",
    "                               12: 7, # right leg\n",
    "                               13: 8, # right wing\n",
    "                               14: 10,\n",
    "                               15: 11}\n",
    "    \n",
    "    all_part_activations = defaultdict(lambda: defaultdict(lambda: defaultdict(list)))\n",
    "                \n",
    "    for root_dir, dirs, files in os.walk(npy_path):\n",
    "        for file in files:\n",
    "            npy_filepath = os.path.join(root_dir, file)\n",
    "            data = np.load(npy_filepath, allow_pickle=True)\n",
    "            activation = data['activation']\n",
    "            img_id = img_filename_to_id[data['img_filename']]\n",
    "            part_activation = np.zeros(12)\n",
    "            partids_list = list(range(1, 16))\n",
    "            \n",
    "            # if both left eye and right eye are visible\n",
    "            if (7 in img_id_to_part_xy_vis[img_id]) and (11 in img_id_to_part_xy_vis[img_id]):\n",
    "                part_loc_x, part_loc_y = img_id_to_part_xy_vis[img_id][7]\n",
    "                part_loc_x, part_loc_y = int(part_loc_x * activation.shape[1]), int(part_loc_y * activation.shape[0])\n",
    "                activation_parta = activation[part_loc_y, part_loc_x]\n",
    "                part_loc_x, part_loc_y = img_id_to_part_xy_vis[img_id][11]\n",
    "                part_loc_x, part_loc_y = int(part_loc_x * activation.shape[1]), int(part_loc_y * activation.shape[0])\n",
    "                activation_partb = activation[part_loc_y, part_loc_x]\n",
    "                part_activation[partid_to_pos_in_vector[7]] = (activation_parta + activation_partb) / 2.\n",
    "                partids_list.remove(7)\n",
    "                partids_list.remove(11)\n",
    "            # if both left leg and right leg are visible\n",
    "            if (8 in img_id_to_part_xy_vis[img_id]) and (12 in img_id_to_part_xy_vis[img_id]):\n",
    "                part_loc_x, part_loc_y = img_id_to_part_xy_vis[img_id][8]\n",
    "                part_loc_x, part_loc_y = int(part_loc_x * activation.shape[1]), int(part_loc_y * activation.shape[0])\n",
    "                activation_parta = activation[part_loc_y, part_loc_x]\n",
    "                part_loc_x, part_loc_y = img_id_to_part_xy_vis[img_id][12]\n",
    "                part_loc_x, part_loc_y = int(part_loc_x * activation.shape[1]), int(part_loc_y * activation.shape[0])\n",
    "                activation_partb = activation[part_loc_y, part_loc_x]\n",
    "                part_activation[partid_to_pos_in_vector[8]] = (activation_parta + activation_partb) / 2.\n",
    "                partids_list.remove(8)\n",
    "                partids_list.remove(12)\n",
    "            # if both left wing and right wing are visible\n",
    "            if (9 in img_id_to_part_xy_vis[img_id]) and (13 in img_id_to_part_xy_vis[img_id]):\n",
    "                part_loc_x, part_loc_y = img_id_to_part_xy_vis[img_id][9]\n",
    "                part_loc_x, part_loc_y = int(part_loc_x * activation.shape[1]), int(part_loc_y * activation.shape[0])\n",
    "                activation_parta = activation[part_loc_y, part_loc_x]\n",
    "                part_loc_x, part_loc_y = img_id_to_part_xy_vis[img_id][13]\n",
    "                part_loc_x, part_loc_y = int(part_loc_x * activation.shape[1]), int(part_loc_y * activation.shape[0])\n",
    "                activation_partb = activation[part_loc_y, part_loc_x]\n",
    "                part_activation[partid_to_pos_in_vector[9]] = (activation_parta + activation_partb) / 2.\n",
    "                partids_list.remove(9)\n",
    "                partids_list.remove(13)\n",
    "            \n",
    "            for partid in partids_list:\n",
    "                if partid in img_id_to_part_xy_vis:\n",
    "                    part_loc_x, part_loc_y = img_id_to_part_xy_vis[img_id][partid]\n",
    "                    part_loc_x, part_loc_y = int(part_loc_x * activation.shape[1]), int(part_loc_y * activation.shape[0])\n",
    "                    part_activation[partid_to_pos_in_vector[partid]] = activation[part_loc_y, part_loc_x]\n",
    "            \n",
    "            all_part_activations[data['node_name']][data['proto_num']][data['leaf_desc']].append(part_activation)   \n",
    "            \n",
    "    for node_name\n",
    "\n",
    "eval_prototypes_cub_parts(npy_path=os.path.join(run_path, 'activation_as_npy'), \\\n",
    "                          parts_loc_path='/projects/ml4science/harishbabu/data/CUB_200_2011/parts/part_locs.txt', \\\n",
    "                          parts_name_path='/projects/ml4science/harishbabu/data/CUB_200_2011/parts/parts.txt', \\\n",
    "                          imgs_id_path='/projects/ml4science/harishbabu/data/CUB_200_2011/images.txt', \\\n",
    "                          imgs_sizes_path='/projects/ml4science/harishbabu/data/CUB_200_2011/image_sizes.txt', \\\n",
    "                          args=args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for root, dirs, files in os.walk(\"/home/harishbabu/projects/PIPNet/runs/103-091-wProtoPool20PerNode_CUB-18-imgnet_with-equalize-aug_cnext26_BGM=4|1.0|50_img=224_nprotos=20_unit-sphere-protopool_no-meanpool_with-softmax-tau=0.2_no-addon-bias_AW=3-TW=2-MMW=2-UW=3-CW=2_batch=48/activation_as_npy\", topdown=True):\n",
    "#     print(root)\n",
    "#     print(dirs)\n",
    "#     print(files)\n",
    "#     print('-*'*50)\n",
    "    \n",
    "# np.zeros(15)\n",
    "\n",
    "all_part_activations = defaultdict(lambda: defaultdict(lambda: defaultdict(list)))\n",
    "\n",
    "all_part_activations['node_name']['proto_num']['leaf_desc'].append(np.zeros(12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.<lambda>()>,\n",
       "            {'node_name': defaultdict(<function __main__.<lambda>.<locals>.<lambda>()>,\n",
       "                         {'proto_num': defaultdict(list,\n",
       "                                      {'leaf_desc': [array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])]})})})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_part_activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ImageFolder\n",
      "    Number of datapoints: 844\n",
      "    Root location: /projects/ml4science/harishbabu/data/CUB_29_pipnet_224/dataset_segmented_imgnet_pt/test_crop\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=warn)\n",
      "               ToTensor()\n",
      "               Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      "           )\n",
      "<torch.utils.data.sampler.SequentialSampler object at 0x2aaabcc523a0>\n",
      "<torch.utils.data.sampler.BatchSampler object at 0x2aaabcc52610>\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, SequentialSampler\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def unshuffle_dataloader(dataloader):\n",
    "    if type(dataloader.dataset) == ImageFolder:\n",
    "        dataset = dataloader.dataset\n",
    "    else:\n",
    "        dataset = dataloader.dataset.dataset.dataset\n",
    "    new_dataloader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=dataloader.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=dataloader.num_workers,\n",
    "        pin_memory=dataloader.pin_memory,\n",
    "        drop_last=dataloader.drop_last,\n",
    "        timeout=dataloader.timeout,\n",
    "        worker_init_fn=dataloader.worker_init_fn,\n",
    "        multiprocessing_context=dataloader.multiprocessing_context,\n",
    "        generator=dataloader.generator,\n",
    "        prefetch_factor=dataloader.prefetch_factor,\n",
    "        persistent_workers=dataloader.persistent_workers\n",
    "    )\n",
    "\n",
    "    return new_dataloader\n",
    "\n",
    "\n",
    "vizloader_name = 'testloader'\n",
    "vizloader_dict = {'trainloader': trainloader,\n",
    "                 'projectloader': projectloader,\n",
    "                 'testloader': testloader,\n",
    "                 'test_projectloader': test_projectloader}\n",
    "\n",
    "vizloader_dict[vizloader_name] = unshuffle_dataloader(vizloader_dict[vizloader_name])\n",
    "print(vizloader_dict[vizloader_name].dataset)\n",
    "\n",
    "print(vizloader_dict[vizloader_name].sampler)\n",
    "\n",
    "print(vizloader_dict[vizloader_name].batch_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ImageFolder\n",
      "    Number of datapoints: 844\n",
      "    Root location: /projects/ml4science/harishbabu/data/CUB_29_pipnet_224/dataset_segmented_imgnet_pt/test_crop\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=warn)\n",
      "               ToTensor()\n",
      "               Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      "           )\n"
     ]
    }
   ],
   "source": [
    "vizloader_name = 'testloader'\n",
    "vizloader_dict[vizloader_name] = unshuffle_dataloader(vizloader_dict[vizloader_name])\n",
    "print(vizloader_dict[vizloader_name].dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ImageFolder\n",
      "    Number of datapoints: 844\n",
      "    Root location: /projects/ml4science/harishbabu/data/CUB_29_pipnet_224/dataset_segmented_imgnet_pt/test_full\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=warn)\n",
      "               ToTensor()\n",
      "               Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      "           )\n"
     ]
    }
   ],
   "source": [
    "vizloader_name = 'test_projectloader'\n",
    "vizloader_dict[vizloader_name] = unshuffle_dataloader(vizloader_dict[vizloader_name])\n",
    "print(vizloader_dict[vizloader_name].dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ImageFolder\n",
      "    Number of datapoints: 869\n",
      "    Root location: /projects/ml4science/harishbabu/data/CUB_29_pipnet_224/dataset_segmented_imgnet_pt/train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=warn)\n",
      "               ToTensor()\n",
      "               Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      "           )\n"
     ]
    }
   ],
   "source": [
    "vizloader_name = 'projectloader'\n",
    "vizloader_dict[vizloader_name] = unshuffle_dataloader(vizloader_dict[vizloader_name])\n",
    "print(vizloader_dict[vizloader_name].dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ImageFolder\n",
      "    Number of datapoints: 869\n",
      "    Root location: /projects/ml4science/harishbabu/data/CUB_29_pipnet_224/dataset_segmented_imgnet_pt/train_crop\n"
     ]
    }
   ],
   "source": [
    "vizloader_name = 'trainloader'\n",
    "vizloader_dict[vizloader_name] = unshuffle_dataloader(vizloader_dict[vizloader_name])\n",
    "print(vizloader_dict[vizloader_name].dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proto activations on leaf descendents - topk images using  NAIVE-HPIPNET with HEATMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 540it [00:12, 43.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node root\n",
      "\t Child: 004+086\n",
      "\t\tProto:0 001:(0.4387) 002:(0.9556) 003:(0.6655) 004:(0.17) 023:(0.5825) 024:(0.2604) 025:(0.4885) 031:(0.9433) 032:(0.9985) 033:(0.9995) 045:(0.7229) 086:(0.7392) 100:(0.0796) 101:(0.1876) \n",
      "\t\tProto:1 001:(0.9697) 002:(0.9837) 003:(0.9768) 004:(0.893) 023:(0.8102) 024:(0.6288) 025:(0.8527) 031:(0.1947) 032:(0.2876) 033:(0.1525) 045:(0.5682) 086:(0.4189) 100:(0.9993) 101:(0.9893) \n",
      "\t\tProto:2 001:(0.2888) 002:(0.0461) 003:(0.2407) 004:(0.2767) 023:(0.9861) 024:(0.9961) 025:(0.9173) 031:(0.0071) 032:(0.2882) 033:(0.0346) 045:(0.0557) 086:(0.3827) 100:(0.9999) 101:(0.9983) \n",
      "\t\tProto:4 001:(0.0727) 002:(0.0254) 003:(0.1299) 004:(0.0059) 023:(0.3014) 024:(0.0115) 025:(0.0497) 031:(0.9358) 032:(0.9471) 033:(0.4799) 045:(0.0294) 086:(0.2171) 100:(0.0101) 101:(0.0152) \n",
      "\t\tProto:5 001:(0.9988) 002:(0.9276) 003:(0.9996) 004:(1.0) 023:(0.9974) 024:(0.8148) 025:(0.9991) 031:(0.999) 032:(0.9981) 033:(0.9956) 045:(0.9928) 086:(1.0) 100:(0.9888) 101:(0.8972) \n",
      "\t\tProto:6 001:(0.0238) 002:(0.1438) 003:(0.0241) 004:(0.0119) 023:(0.7712) 024:(0.604) 025:(0.4021) 031:(0.1534) 032:(1.0) 033:(1.0) 045:(0.1216) 086:(0.1614) 100:(0.0126) 101:(0.2336) \n",
      "\t\tProto:7 001:(0.1759) 002:(0.6532) 003:(0.2676) 004:(0.0041) 023:(0.0193) 024:(0.4904) 025:(0.0503) 031:(0.6616) 032:(0.1924) 033:(0.3657) 045:(0.6291) 086:(0.1009) 100:(0.1474) 101:(0.3142) \n",
      "\t\tProto:12 001:(0.9719) 002:(0.466) 003:(0.9518) 004:(0.1095) 023:(0.5488) 024:(0.0124) 025:(0.0148) 031:(0.9767) 032:(0.2064) 033:(0.4883) 045:(0.0738) 086:(0.1387) 100:(0.5254) 101:(0.1048) \n",
      "\t\tProto:13 001:(0.9997) 002:(1.0) 003:(0.9938) 004:(0.8379) 023:(0.9886) 024:(0.9999) 025:(0.9996) 031:(0.8672) 032:(0.0414) 033:(0.9013) 045:(0.9988) 086:(0.9963) 100:(0.2243) 101:(0.9974) \n",
      "\t\tProto:14 001:(0.481) 002:(0.8343) 003:(0.7323) 004:(0.9991) 023:(0.9472) 024:(0.18) 025:(0.7521) 031:(0.947) 032:(0.9889) 033:(0.9487) 045:(0.4137) 086:(0.7724) 100:(0.6864) 101:(0.4176) \n",
      "\t\tProto:16 001:(0.466) 002:(0.6932) 003:(0.7292) 004:(0.9817) 023:(0.9628) 024:(0.7912) 025:(0.9504) 031:(0.6692) 032:(0.7132) 033:(0.6169) 045:(0.6943) 086:(0.3119) 100:(0.8366) 101:(0.7104) \n",
      "\t\tProto:18 001:(0.5301) 002:(0.9867) 003:(0.2008) 004:(0.0791) 023:(0.3427) 024:(0.5998) 025:(0.2175) 031:(0.1514) 032:(0.231) 033:(0.2877) 045:(0.0973) 086:(0.0408) 100:(0.9503) 101:(0.9951) \n",
      "\t Child: 052+053\n",
      "\t\tProto:3 050:(0.4739) 051:(0.1647) 052:(0.1014) 053:(1.0) \n",
      "\t\tProto:8 050:(0.6809) 051:(0.9941) 052:(0.0156) 053:(0.9999) \n",
      "\t\tProto:9 050:(0.8403) 051:(0.9957) 052:(0.8959) 053:(0.9512) \n",
      "\t\tProto:15 050:(0.96) 051:(0.9885) 052:(0.9992) 053:(0.9871) \n",
      "\t\tProto:19 050:(0.9808) 051:(0.9903) 052:(0.9464) 053:(0.6504) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 120it [00:02, 47.16it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 052+053\n",
      "\t Child: cub_052_Pied_billed_Grebe\n",
      "\t\tProto:0 052:(0.9587) \n",
      "\t\tProto:8 052:(0.9988) \n",
      "\t\tProto:4 052:(1.0) \n",
      "\t\tProto:14 052:(0.9999) \n",
      "\t Child: 053+050\n",
      "\t\tProto:1 050:(0.5616) 051:(0.5862) 053:(0.9558) \n",
      "\t\tProto:2 050:(0.8769) 051:(0.9873) 053:(0.996) \n",
      "\t\tProto:3 050:(0.8863) 051:(0.9899) 053:(0.9941) \n",
      "\t\tProto:6 050:(0.9912) 051:(0.9956) 053:(0.9926) \n",
      "\t\tProto:7 050:(0.9099) 051:(0.9982) 053:(0.9785) \n",
      "\t\tProto:9 050:(0.9261) 051:(0.9979) 053:(0.9905) \n",
      "\t\tProto:10 050:(0.8899) 051:(0.8944) 053:(0.1667) \n",
      "\t\tProto:11 050:(0.9898) 051:(0.9986) 053:(0.9881) \n",
      "\t\tProto:12 050:(0.9816) 051:(0.8963) 053:(0.9559) \n",
      "\t\tProto:13 050:(0.9997) 051:(0.9986) 053:(0.9464) \n",
      "\t\tProto:16 050:(0.9464) 051:(0.9809) 053:(0.9977) \n",
      "\t\tProto:17 050:(0.4604) 051:(0.856) 053:(0.5544) \n",
      "\t\tProto:19 050:(0.985) 051:(0.9821) 053:(0.907) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 420it [00:07, 56.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 004+086\n",
      "\t Child: 004+032\n",
      "\t\tProto:0 004:(0.1669) 031:(0.975) 032:(0.9976) 033:(0.9986) \n",
      "\t\tProto:2 004:(0.9407) 031:(0.8734) 032:(0.9983) 033:(0.9984) \n",
      "\t\tProto:6 004:(0.998) 031:(0.9861) 032:(0.9936) 033:(0.9978) \n",
      "\t\tProto:9 004:(0.0188) 031:(0.9905) 032:(0.9915) 033:(0.9419) \n",
      "\t\tProto:12 004:(0.9676) 031:(0.9251) 032:(0.1311) 033:(0.7084) \n",
      "\t\tProto:13 004:(0.9987) 031:(0.9993) 032:(0.9986) 033:(0.9878) \n",
      "\t\tProto:15 004:(0.9898) 031:(0.9665) 032:(0.9812) 033:(0.9096) \n",
      "\t Child: 086+045\n",
      "\t\tProto:1 001:(0.9977) 002:(0.9994) 003:(0.9994) 023:(1.0) 024:(0.9995) 025:(1.0) 045:(0.9598) 086:(1.0) 100:(0.9997) 101:(0.9996) \n",
      "\t\tProto:3 001:(0.8767) 002:(0.4772) 003:(0.6463) 023:(0.0827) 024:(0.2068) 025:(0.4949) 045:(0.9821) 086:(0.8265) 100:(0.1141) 101:(0.1942) \n",
      "\t\tProto:4 001:(0.4335) 002:(0.6162) 003:(0.8708) 023:(0.0336) 024:(0.4325) 025:(0.8619) 045:(0.7866) 086:(0.4247) 100:(0.083) 101:(0.0924) \n",
      "\t\tProto:5 001:(0.0042) 002:(0.1048) 003:(0.1304) 023:(0.9997) 024:(1.0) 025:(0.9995) 045:(0.0253) 086:(0.5434) 100:(0.0114) 101:(0.1424) \n",
      "\t\tProto:7 001:(0.8847) 002:(0.9234) 003:(0.9951) 023:(0.6564) 024:(0.8055) 025:(0.9022) 045:(0.9421) 086:(0.557) 100:(0.8731) 101:(0.6656) \n",
      "\t\tProto:8 001:(0.9877) 002:(0.2482) 003:(0.1582) 023:(0.0602) 024:(0.0389) 025:(0.1058) 045:(0.2587) 086:(0.9583) 100:(0.8545) 101:(0.9832) \n",
      "\t\tProto:10 001:(0.6975) 002:(0.9187) 003:(0.9373) 023:(0.9279) 024:(0.9982) 025:(0.2585) 045:(0.5395) 086:(0.7315) 100:(0.0238) 101:(0.5075) \n",
      "\t\tProto:11 001:(0.997) 002:(0.9943) 003:(0.8625) 023:(0.9972) 024:(0.9704) 025:(0.9988) 045:(0.2639) 086:(0.9997) 100:(0.1694) 101:(0.4856) \n",
      "\t\tProto:14 001:(0.8867) 002:(0.9378) 003:(0.5063) 023:(0.1171) 024:(0.1648) 025:(0.1573) 045:(0.9032) 086:(0.6872) 100:(0.9327) 101:(0.4261) \n",
      "\t\tProto:17 001:(0.953) 002:(0.9989) 003:(0.9918) 023:(0.1686) 024:(0.161) 025:(0.3577) 045:(0.9679) 086:(0.6481) 100:(0.9996) 101:(0.9996) \n",
      "\t\tProto:19 001:(0.2381) 002:(0.3072) 003:(0.4219) 023:(0.9978) 024:(0.9999) 025:(0.9953) 045:(0.0271) 086:(0.8398) 100:(0.1508) 101:(0.3647) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 90it [00:02, 36.11it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 053+050\n",
      "\t Child: 050+051\n",
      "\t\tProto:2 050:(0.9991) 051:(0.9992) \n",
      "\t\tProto:3 050:(0.9961) 051:(0.9948) \n",
      "\t\tProto:5 050:(0.7436) 051:(0.9582) \n",
      "\t\tProto:8 050:(0.9991) 051:(0.9985) \n",
      "\t\tProto:9 050:(0.9825) 051:(0.9983) \n",
      "\t\tProto:12 050:(0.9828) 051:(0.9756) \n",
      "\t\tProto:13 050:(0.9684) 051:(0.9893) \n",
      "\t\tProto:15 050:(0.9449) 051:(0.9991) \n",
      "\t\tProto:17 050:(0.9658) 051:(0.9918) \n",
      "\t\tProto:19 050:(0.9853) 051:(0.987) \n",
      "\t Child: cub_053_Western_Grebe\n",
      "\t\tProto:6 053:(0.9999) \n",
      "\t\tProto:7 053:(1.0) \n",
      "\t\tProto:10 053:(0.9995) \n",
      "\t\tProto:11 053:(1.0) \n",
      "\t\tProto:14 053:(0.9997) \n",
      "\t\tProto:18 053:(0.9986) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 120it [00:02, 41.54it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 004+032\n",
      "\t Child: 032+033\n",
      "\t\tProto:0 031:(0.9871) 032:(0.9324) 033:(0.9727) \n",
      "\t\tProto:3 031:(0.9939) 032:(0.9712) 033:(0.9854) \n",
      "\t\tProto:4 031:(0.8933) 032:(0.9941) 033:(0.964) \n",
      "\t\tProto:6 031:(0.9911) 032:(0.9889) 033:(0.9346) \n",
      "\t\tProto:7 031:(0.9774) 032:(0.9868) 033:(0.9935) \n",
      "\t\tProto:8 031:(0.9818) 032:(1.0) 033:(0.9999) \n",
      "\t\tProto:9 031:(0.9739) 032:(0.9352) 033:(0.9807) \n",
      "\t\tProto:10 031:(0.9952) 032:(0.9684) 033:(0.9615) \n",
      "\t\tProto:12 031:(0.9998) 032:(0.9139) 033:(0.9824) \n",
      "\t\tProto:13 031:(0.996) 032:(0.961) 033:(0.7019) \n",
      "\t\tProto:14 031:(0.8839) 032:(0.9789) 033:(0.8816) \n",
      "\t\tProto:15 031:(0.9752) 032:(0.9963) 033:(0.9721) \n",
      "\t\tProto:17 031:(0.9634) 032:(1.0) 033:(1.0) \n",
      "\t\tProto:18 031:(0.9968) 032:(0.9823) 033:(0.9929) \n",
      "\t\tProto:19 031:(0.9951) 032:(0.9813) 033:(0.96) \n",
      "\t Child: cub_004_Groove_billed_Ani\n",
      "\t\tProto:16 004:(1.0) \n",
      "\t\tProto:11 004:(0.9999) \n",
      "\t\tProto:5 004:(1.0) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 300it [00:06, 48.88it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 086+045\n",
      "\t Child: 045+101\n",
      "\t\tProto:0 001:(0.9837) 002:(0.6442) 003:(0.9457) 023:(0.2638) 024:(0.0636) 025:(0.401) 045:(0.9223) 100:(0.515) 101:(0.0469) \n",
      "\t\tProto:1 001:(0.7715) 002:(0.2903) 003:(0.2234) 023:(0.9726) 024:(0.9998) 025:(0.9623) 045:(0.1768) 100:(0.1535) 101:(0.2597) \n",
      "\t\tProto:2 001:(1.0) 002:(1.0) 003:(1.0) 023:(1.0) 024:(1.0) 025:(1.0) 045:(1.0) 100:(1.0) 101:(1.0) \n",
      "\t\tProto:3 001:(0.2871) 002:(0.9972) 003:(0.1334) 023:(0.9987) 024:(0.999) 025:(0.9963) 045:(0.7145) 100:(0.1164) 101:(0.9995) \n",
      "\t\tProto:4 001:(0.7512) 002:(0.6962) 003:(0.2306) 023:(0.9998) 024:(1.0) 025:(0.9998) 045:(0.1481) 100:(0.2236) 101:(0.6411) \n",
      "\t\tProto:5 001:(0.8569) 002:(0.9717) 003:(0.976) 023:(0.4372) 024:(0.1693) 025:(0.4808) 045:(0.8237) 100:(0.9998) 101:(0.9997) \n",
      "\t\tProto:7 001:(0.9563) 002:(0.8725) 003:(0.9999) 023:(0.2752) 024:(0.0417) 025:(0.0283) 045:(0.999) 100:(0.0527) 101:(0.0446) \n",
      "\t\tProto:8 001:(0.9547) 002:(0.7359) 003:(0.9394) 023:(0.9367) 024:(0.9897) 025:(0.9955) 045:(0.6822) 100:(0.466) 101:(0.2213) \n",
      "\t\tProto:10 001:(0.3761) 002:(0.9499) 003:(0.0561) 023:(0.0055) 024:(0.7805) 025:(0.3453) 045:(0.4644) 100:(0.8783) 101:(0.9839) \n",
      "\t\tProto:11 001:(0.9394) 002:(0.9865) 003:(0.9954) 023:(0.529) 024:(0.0722) 025:(0.1621) 045:(0.9864) 100:(0.1925) 101:(0.7243) \n",
      "\t\tProto:12 001:(0.9973) 002:(0.9641) 003:(0.9941) 023:(0.728) 024:(0.5753) 025:(0.6276) 045:(0.7876) 100:(0.4164) 101:(0.0662) \n",
      "\t\tProto:13 001:(0.4242) 002:(0.61) 003:(0.3842) 023:(0.9941) 024:(0.7533) 025:(0.9911) 045:(0.2053) 100:(0.9292) 101:(0.4482) \n",
      "\t\tProto:14 001:(0.8269) 002:(0.999) 003:(0.2321) 023:(0.3313) 024:(0.5704) 025:(0.9639) 045:(0.8266) 100:(0.4549) 101:(0.9961) \n",
      "\t\tProto:15 001:(0.9948) 002:(0.9381) 003:(0.7605) 023:(0.1271) 024:(0.0609) 025:(0.1129) 045:(0.8368) 100:(0.9748) 101:(0.8597) \n",
      "\t\tProto:18 001:(0.5827) 002:(0.3889) 003:(0.2911) 023:(0.1359) 024:(0.5301) 025:(0.9649) 045:(0.3194) 100:(0.9962) 101:(0.7517) \n",
      "\t\tProto:19 001:(0.6414) 002:(0.9239) 003:(0.2066) 023:(0.588) 024:(0.2236) 025:(0.0348) 045:(0.0084) 100:(0.9989) 101:(0.9969) \n",
      "\t Child: cub_086_Pacific_Loon\n",
      "\t\tProto:16 086:(0.9991) \n",
      "\t\tProto:17 086:(0.9996) \n",
      "\t\tProto:6 086:(0.9998) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 100% 60/60 [00:01<00:00, 33.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 050+051\n",
      "\t Child: cub_051_Horned_Grebe\n",
      "\t\tProto:2 051:(0.9881) \n",
      "\t\tProto:3 051:(0.9996) \n",
      "\t\tProto:12 051:(1.0) \n",
      "\t\tProto:13 051:(0.9987) \n",
      "\t\tProto:15 051:(0.9573) \n",
      "\t\tProto:16 051:(0.9957) \n",
      "\t Child: cub_050_Eared_Grebe\n",
      "\t\tProto:4 050:(1.0) \n",
      "\t\tProto:5 050:(1.0) \n",
      "\t\tProto:8 050:(0.9965) \n",
      "\t\tProto:9 050:(0.9938) \n",
      "\t\tProto:14 050:(0.9992) \n",
      "\t\tProto:19 050:(0.9994) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 90it [00:02, 37.29it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 032+033\n",
      "\t Child: 033+031\n",
      "\t\tProto:0 031:(0.9999) 033:(0.999) \n",
      "\t\tProto:3 031:(0.9995) 033:(0.9981) \n",
      "\t\tProto:9 031:(0.999) 033:(0.9991) \n",
      "\t\tProto:11 031:(0.9782) 033:(0.9977) \n",
      "\t\tProto:15 031:(0.9999) 033:(0.9919) \n",
      "\t\tProto:19 031:(0.9997) 033:(0.9939) \n",
      "\t Child: cub_032_Mangrove_Cuckoo\n",
      "\t\tProto:1 032:(1.0) \n",
      "\t\tProto:4 032:(0.9997) \n",
      "\t\tProto:7 032:(0.9977) \n",
      "\t\tProto:10 032:(0.9962) \n",
      "\t\tProto:13 032:(0.999) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 270it [00:05, 53.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 045+101\n",
      "\t Child: 101+023\n",
      "\t\tProto:0 023:(0.9958) 024:(0.9998) 025:(0.993) 100:(0.477) 101:(0.4447) \n",
      "\t\tProto:1 023:(0.9977) 024:(0.9992) 025:(0.9984) 100:(0.9949) 101:(0.9965) \n",
      "\t\tProto:2 023:(0.9773) 024:(0.8123) 025:(0.9909) 100:(0.2911) 101:(0.2684) \n",
      "\t\tProto:6 023:(0.9963) 024:(0.9856) 025:(0.9607) 100:(0.9963) 101:(0.9967) \n",
      "\t\tProto:7 023:(0.8308) 024:(0.3724) 025:(0.9005) 100:(0.9921) 101:(0.9997) \n",
      "\t\tProto:8 023:(0.9999) 024:(0.9971) 025:(0.9997) 100:(0.9903) 101:(0.9991) \n",
      "\t\tProto:11 023:(0.8017) 024:(0.9776) 025:(0.9335) 100:(0.2619) 101:(0.3088) \n",
      "\t\tProto:19 023:(0.5875) 024:(0.9998) 025:(0.4644) 100:(0.9831) 101:(0.9972) \n",
      "\t Child: 045+003\n",
      "\t\tProto:3 001:(0.9956) 002:(0.9531) 003:(0.9992) 045:(0.9467) \n",
      "\t\tProto:4 001:(0.9858) 002:(0.9971) 003:(0.8195) 045:(0.9777) \n",
      "\t\tProto:5 001:(0.9769) 002:(0.9968) 003:(0.9962) 045:(0.862) \n",
      "\t\tProto:9 001:(0.8898) 002:(0.8932) 003:(0.8217) 045:(0.8897) \n",
      "\t\tProto:10 001:(0.9912) 002:(0.9879) 003:(0.9518) 045:(0.2664) \n",
      "\t\tProto:12 001:(0.9763) 002:(0.8091) 003:(0.9805) 045:(0.8802) \n",
      "\t\tProto:13 001:(0.9868) 002:(0.978) 003:(0.9396) 045:(0.8981) \n",
      "\t\tProto:15 001:(0.9638) 002:(0.9938) 003:(0.9989) 045:(0.999) \n",
      "\t\tProto:18 001:(0.9665) 002:(0.9622) 003:(0.9851) 045:(0.99) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 100% 60/60 [00:01<00:00, 31.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 033+031\n",
      "\t Child: cub_031_Black_billed_Cuckoo\n",
      "\t\tProto:2 031:(0.9994) \n",
      "\t\tProto:5 031:(0.9999) \n",
      "\t\tProto:8 031:(0.996) \n",
      "\t\tProto:11 031:(1.0) \n",
      "\t\tProto:17 031:(0.991) \n",
      "\t\tProto:18 031:(0.9826) \n",
      "\t Child: cub_033_Yellow_billed_Cuckoo\n",
      "\t\tProto:16 033:(1.0) \n",
      "\t\tProto:12 033:(1.0) \n",
      "\t\tProto:14 033:(0.9996) \n",
      "\t\tProto:15 033:(1.0) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 120it [00:02, 40.60it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 045+003\n",
      "\t Child: 003+002\n",
      "\t\tProto:0 001:(0.6931) 002:(0.9226) 003:(0.9288) \n",
      "\t\tProto:1 001:(0.9957) 002:(0.9566) 003:(0.9306) \n",
      "\t\tProto:2 001:(0.9989) 002:(0.9971) 003:(0.9987) \n",
      "\t\tProto:3 001:(0.9956) 002:(0.9914) 003:(0.9962) \n",
      "\t\tProto:4 001:(0.8615) 002:(0.9973) 003:(0.8051) \n",
      "\t\tProto:9 001:(0.9994) 002:(0.9418) 003:(0.9977) \n",
      "\t\tProto:10 001:(0.9208) 002:(0.8984) 003:(0.953) \n",
      "\t\tProto:11 001:(0.9848) 002:(0.9961) 003:(0.9482) \n",
      "\t\tProto:15 001:(0.9561) 002:(0.8994) 003:(0.9949) \n",
      "\t\tProto:16 001:(0.9867) 002:(0.9968) 003:(0.9275) \n",
      "\t\tProto:17 001:(0.9679) 002:(0.9996) 003:(0.5497) \n",
      "\t\tProto:18 001:(0.9951) 002:(0.9953) 003:(0.9935) \n",
      "\t Child: cub_045_Northern_Fulmar\n",
      "\t\tProto:8 045:(0.9968) \n",
      "\t\tProto:14 045:(0.9997) \n",
      "\t\tProto:6 045:(0.9956) \n",
      "\t\tProto:7 045:(0.9956) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 150it [00:03, 49.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 101+023\n",
      "\t Child: 023+025\n",
      "\t\tProto:0 023:(0.9981) 024:(0.9966) 025:(0.9942) \n",
      "\t\tProto:2 023:(0.9889) 024:(0.9998) 025:(0.9873) \n",
      "\t\tProto:4 023:(0.9947) 024:(0.9999) 025:(0.9994) \n",
      "\t\tProto:6 023:(0.9881) 024:(0.9915) 025:(0.9961) \n",
      "\t\tProto:9 023:(0.9338) 024:(0.9916) 025:(0.9786) \n",
      "\t\tProto:10 023:(0.9993) 024:(0.9931) 025:(0.9978) \n",
      "\t\tProto:11 023:(0.9996) 024:(0.9992) 025:(0.9999) \n",
      "\t\tProto:18 023:(0.9862) 024:(0.9966) 025:(0.9983) \n",
      "\t Child: 101+100\n",
      "\t\tProto:1 100:(0.9975) 101:(0.8373) \n",
      "\t\tProto:3 100:(0.9997) 101:(0.9995) \n",
      "\t\tProto:7 100:(0.9993) 101:(0.997) \n",
      "\t\tProto:8 100:(0.9996) 101:(0.9997) \n",
      "\t\tProto:13 100:(0.9981) 101:(0.9569) \n",
      "\t\tProto:15 100:(0.9939) 101:(0.9951) \n",
      "\t\tProto:16 100:(1.0) 101:(1.0) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 90it [00:02, 38.07it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 003+002\n",
      "\t Child: 002+001\n",
      "\t\tProto:0 001:(0.9971) 002:(0.9996) \n",
      "\t\tProto:3 001:(1.0) 002:(1.0) \n",
      "\t\tProto:9 001:(0.9954) 002:(0.9964) \n",
      "\t\tProto:11 001:(0.9777) 002:(0.9965) \n",
      "\t\tProto:12 001:(0.9846) 002:(0.9996) \n",
      "\t\tProto:13 001:(0.9986) 002:(0.9993) \n",
      "\t\tProto:15 001:(0.8837) 002:(0.9471) \n",
      "\t\tProto:17 001:(0.9994) 002:(0.9926) \n",
      "\t\tProto:18 001:(0.9971) 002:(0.9977) \n",
      "\t Child: cub_003_Sooty_Albatross\n",
      "\t\tProto:8 003:(0.9986) \n",
      "\t\tProto:1 003:(0.9999) \n",
      "\t\tProto:5 003:(1.0) \n",
      "\t\tProto:7 003:(0.9998) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 100% 60/60 [00:01<00:00, 31.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 101+100\n",
      "\t Child: cub_101_White_Pelican\n",
      "\t\tProto:0 101:(0.9999) \n",
      "\t\tProto:2 101:(0.9999) \n",
      "\t\tProto:3 101:(0.999) \n",
      "\t\tProto:8 101:(0.9994) \n",
      "\t\tProto:15 101:(0.9973) \n",
      "\t\tProto:18 101:(0.9981) \n",
      "\t Child: cub_100_Brown_Pelican\n",
      "\t\tProto:1 100:(0.9997) \n",
      "\t\tProto:6 100:(0.9999) \n",
      "\t\tProto:11 100:(1.0) \n",
      "\t\tProto:12 100:(0.9991) \n",
      "\t\tProto:16 100:(1.0) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 90it [00:02, 40.21it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 023+025\n",
      "\t Child: cub_023_Brandt_Cormorant\n",
      "\t\tProto:0 023:(1.0) \n",
      "\t\tProto:4 023:(0.9696) \n",
      "\t\tProto:7 023:(0.9994) \n",
      "\t\tProto:17 023:(0.9725) \n",
      "\t\tProto:18 023:(0.9999) \n",
      "\t Child: 025+024\n",
      "\t\tProto:1 024:(0.9926) 025:(0.9678) \n",
      "\t\tProto:3 024:(0.9999) 025:(0.9961) \n",
      "\t\tProto:5 024:(0.9802) 025:(0.9883) \n",
      "\t\tProto:6 024:(0.9909) 025:(0.9704) \n",
      "\t\tProto:10 024:(0.9337) 025:(0.9344) \n",
      "\t\tProto:12 024:(1.0) 025:(0.9991) \n",
      "\t\tProto:13 024:(0.9999) 025:(0.9955) \n",
      "\t\tProto:14 024:(0.9993) 025:(0.9961) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 100% 60/60 [00:01<00:00, 31.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 002+001\n",
      "\t Child: cub_001_Black_footed_Albatross\n",
      "\t\tProto:0 001:(0.999) \n",
      "\t\tProto:1 001:(0.9993) \n",
      "\t\tProto:2 001:(0.9999) \n",
      "\t\tProto:8 001:(0.9999) \n",
      "\t\tProto:19 001:(0.9993) \n",
      "\t Child: cub_002_Laysan_Albatross\n",
      "\t\tProto:16 002:(1.0) \n",
      "\t\tProto:11 002:(0.9977) \n",
      "\t\tProto:5 002:(0.9997) \n",
      "\t\tProto:14 002:(0.9999) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 100% 60/60 [00:01<00:00, 31.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 025+024\n",
      "\t Child: cub_024_Red_faced_Cormorant\n",
      "\t\tProto:1 024:(1.0) \n",
      "\t\tProto:3 024:(1.0) \n",
      "\t\tProto:8 024:(0.9985) \n",
      "\t\tProto:9 024:(1.0) \n",
      "\t\tProto:14 024:(1.0) \n",
      "\t Child: cub_025_Pelagic_Cormorant\n",
      "\t\tProto:4 025:(0.9999) \n",
      "\t\tProto:7 025:(0.9989) \n",
      "\t\tProto:10 025:(0.9999) \n",
      "\t\tProto:17 025:(0.9995) \n",
      "\t\tProto:19 025:(0.9998) \n",
      "Done !!!\n"
     ]
    }
   ],
   "source": [
    "# Proto activations on leaf descendents - topk images\n",
    "\n",
    "def get_heatmap_uninterpolated(latent_activation, input_image):\n",
    "    image_a = latent_activation.cpu().numpy()\n",
    "    image_a = (image_a - image_a.min()) / (image_a.max() - image_a.min())\n",
    "\n",
    "    input_image = (input_image - input_image.min()) / (input_image.max() - input_image.min())\n",
    "    image_b = input_image.permute(1, 2, 0).cpu().numpy()\n",
    "    \n",
    "    reshaped_image_a = np.array(Image.fromarray((image_a[0] * 255).astype('uint8')).resize((input_image.shape[-1], input_image.shape[-1]), \\\n",
    "                                                                                          resample=Image.NEAREST ))\n",
    "    normalized_heatmap = (reshaped_image_a - np.min(reshaped_image_a)) / (np.max(reshaped_image_a) - np.min(reshaped_image_a))\n",
    "    \n",
    "    heatmap_colormap = plt.get_cmap('jet')\n",
    "    heatmap_colored = heatmap_colormap(normalized_heatmap)\n",
    "    \n",
    "    heatmap_colored_uint8 = (heatmap_colored[:, :, :3] * 255).astype(np.uint8)\n",
    "    image_a_heatmap_pillow = Image.fromarray(heatmap_colored_uint8)\n",
    "    image_b_pillow = Image.fromarray((image_b * 255).astype('uint8'))\n",
    "    \n",
    "    result_image = Image.blend(image_b_pillow, image_a_heatmap_pillow, alpha=0.3)\n",
    "    \n",
    "    return np.array(result_image)\n",
    "\n",
    "from util.data import ModifiedLabelLoader\n",
    "from collections import defaultdict\n",
    "import heapq\n",
    "import pdb\n",
    "from util.vis_pipnet import get_img_coordinates\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import ImageFont, Image, ImageDraw as D\n",
    "import torchvision\n",
    "from datetime import datetime\n",
    "txt_file = open(os.path.join(run_path, \"num_proto_details_\"+datetime.now().strftime(\"%m:%d:%H:%M:%S\")+\".txt\"), \"a\")\n",
    "txt_file.write('\\n')\n",
    "\n",
    "find_non_descendants = False # True, False # param\n",
    "topk = 6\n",
    "save_images = False # True\n",
    "font = ImageFont.truetype(\"arial.ttf\", 50)\n",
    "save_activation_as_npy_path = 'activation_as_npy'\n",
    "\n",
    "from datetime import datetime\n",
    "txt_file = open(os.path.join(run_path, \"num_proto_details_\"+datetime.now().strftime(\"%m:%d:%H:%M:%S\")+\".txt\"), \"a\")\n",
    "txt_file.write('\\n')\n",
    "\n",
    "def write_num_proto_details(proto_mean_activations, node_name, net, threshold, txt_file, args):\n",
    "    \n",
    "    rand_input = torch.randn((1, 3, args.image_size, args.image_size))\n",
    "    with torch.no_grad():\n",
    "        *_, pooled, out = net(rand_input)\n",
    "    num_protos = pooled[node_name].shape[1]\n",
    "    used_protos = len(proto_mean_activations)\n",
    "    non_overspecific = 0\n",
    "    for p in proto_mean_activations:\n",
    "        logstr = '\\t'*2 + f'Proto:{p} '\n",
    "        protos_mean_for_all_leaf_descedants = []\n",
    "        for leaf_descendent in proto_mean_activations[p]:\n",
    "            mean_activation = round(np.mean([activation for activation, *_ in proto_mean_activations[p][leaf_descendent]]), 4)\n",
    "            protos_mean_for_all_leaf_descedants.append(mean_activation)\n",
    "            \n",
    "        if all([(mean_activation>0.2) for mean_activation in protos_mean_for_all_leaf_descedants]):\n",
    "            non_overspecific += 1\n",
    "            \n",
    "    txt_file.write(f\"Node:{node_name},Total:{num_protos},Used:{used_protos},Good:{non_overspecific},threshold={threshold}\\n\")\n",
    "\n",
    "\n",
    "def get_heap():\n",
    "    list_ = []\n",
    "    heapq.heapify(list_)\n",
    "    return list_\n",
    "\n",
    "patchsize, skip = get_patch_size(args)\n",
    "\n",
    "for node in root.nodes_with_children():\n",
    "#     if node.name == 'root':\n",
    "#         continue\n",
    "#     non_leaf_children_names = [child.name for child in node.children if not child.is_leaf()]\n",
    "#     if len(non_leaf_children_names) == 0: # if all the children are leaf nodes then skip this node\n",
    "#         continue\n",
    "\n",
    "    name2label = projectloader.dataset.class_to_idx\n",
    "    label2name = {label:name for name, label in name2label.items()}\n",
    "    modifiedLabelLoader = ModifiedLabelLoader(projectloader, node)\n",
    "    coarse_label2name = modifiedLabelLoader.modifiedlabel2name\n",
    "    node_label_to_children = {label: name for name, label in node.children_to_labels.items()}\n",
    "    \n",
    "    imgs = modifiedLabelLoader.filtered_imgs\n",
    "\n",
    "    img_iter = tqdm(enumerate(modifiedLabelLoader),\n",
    "                    total=len(modifiedLabelLoader),\n",
    "                    mininterval=50.,\n",
    "                    desc='Collecting topk',\n",
    "                    ncols=0)\n",
    "\n",
    "    classification_weights = getattr(net.module, '_'+node.name+'_classification').weight\n",
    "    \n",
    "    # maps proto_number -> grand_child_name (or descendant leaf name) -> list of top-k activations\n",
    "    proto_mean_activations = defaultdict(lambda: defaultdict(get_heap))\n",
    "\n",
    "    # maps class names to the prototypes that belong to that\n",
    "    class_and_prototypes = defaultdict(set)\n",
    "\n",
    "    for i, (xs, orig_y, ys) in img_iter:\n",
    "#         if coarse_label2name[ys.item()] not in non_leaf_children_names:\n",
    "#             continue\n",
    "\n",
    "        xs, ys = xs.to(device), ys.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model_output = net(xs, inference=False)\n",
    "            if len(model_output) == 3:\n",
    "                softmaxes, pooled, _ = model_output\n",
    "            elif len(model_output) == 4:\n",
    "                _, softmaxes, pooled, _ = model_output\n",
    "            pooled = pooled[node.name].squeeze(0) \n",
    "            softmaxes = softmaxes[node.name]#.squeeze(0)\n",
    "\n",
    "            for p in range(pooled.shape[0]): # pooled.shape -> [768] (== num of prototypes)\n",
    "                c_weight = torch.max(classification_weights[:,p]) # classification_weights[:,p].shape -> [200] (== num of classes)\n",
    "                relevant_proto_classes = torch.nonzero(classification_weights[:, p] > 1e-3)\n",
    "                relevant_proto_class_names = [node_label_to_children[class_idx.item()] for class_idx in relevant_proto_classes]\n",
    "                \n",
    "                # Take the max per prototype.                             \n",
    "                max_per_prototype, max_idx_per_prototype = torch.max(softmaxes, dim=0)\n",
    "                max_per_prototype_h, max_idx_per_prototype_h = torch.max(max_per_prototype, dim=1)\n",
    "                max_per_prototype_w, max_idx_per_prototype_w = torch.max(max_per_prototype_h, dim=1) #shape (num_prototypes)\n",
    "                \n",
    "                h_idx = max_idx_per_prototype_h[p, max_idx_per_prototype_w[p]]\n",
    "                w_idx = max_idx_per_prototype_w[p]\n",
    "\n",
    "                if len(relevant_proto_class_names) == 0:\n",
    "                    continue\n",
    "                \n",
    "#                 if (len(relevant_proto_class_names) == 1) and (relevant_proto_class_names[0] not in non_leaf_children_names):\n",
    "#                     continue\n",
    "                \n",
    "                h_coor_min, h_coor_max, w_coor_min, w_coor_max = get_img_coordinates(args.image_size, softmaxes.shape, patchsize, skip, h_idx, w_idx)\n",
    "                latent_activation = softmaxes[:, p, :, :]\n",
    "                \n",
    "                if not find_non_descendants:\n",
    "                    if (coarse_label2name[ys.item()] in relevant_proto_class_names):\n",
    "                        child_node = root.get_node(coarse_label2name[ys.item()])\n",
    "                        leaf_descendent = label2name[orig_y.item()][4:7]\n",
    "                        img_to_open = imgs[i][0] # it is a tuple of (path to image, lable)\n",
    "                        if topk and (len(proto_mean_activations[p][leaf_descendent]) >= topk):\n",
    "                            heapq.heappushpop(proto_mean_activations[p][leaf_descendent],\\\n",
    "                                              (pooled[p].item(), img_to_open,\\\n",
    "                                               (h_coor_min, h_coor_max, w_coor_min, w_coor_max), latent_activation))\n",
    "                        else:\n",
    "                            heapq.heappush(proto_mean_activations[p][leaf_descendent],\\\n",
    "                                           (pooled[p].item(), img_to_open,\\\n",
    "                                            (h_coor_min, h_coor_max, w_coor_min, w_coor_max), latent_activation))\n",
    "                else:\n",
    "                    if (coarse_label2name[ys.item()] not in relevant_proto_class_names):\n",
    "                        child_node = root.get_node(coarse_label2name[ys.item()])\n",
    "                        leaf_descendent = label2name[orig_y.item()][4:7]\n",
    "                        img_to_open = imgs[i][0] # it is a tuple of (path to image, lable)\n",
    "                        if topk and (len(proto_mean_activations[p][leaf_descendent]) >= topk):\n",
    "                            heapq.heappushpop(proto_mean_activations[p][leaf_descendent],\\\n",
    "                                              (pooled[p].item(), img_to_open,\\\n",
    "                                               (h_coor_min, h_coor_max, w_coor_min, w_coor_max), latent_activation))\n",
    "                        else:\n",
    "                            heapq.heappush(proto_mean_activations[p][leaf_descendent],\\\n",
    "                                           (pooled[p].item(), img_to_open,\\\n",
    "                                            (h_coor_min, h_coor_max, w_coor_min, w_coor_max), latent_activation))\n",
    "\n",
    "                class_and_prototypes[', '.join(relevant_proto_class_names)].add(p)\n",
    "\n",
    "    write_num_proto_details(proto_mean_activations, node.name, net, threshold=0.2, txt_file=txt_file, args=args)\n",
    "\n",
    "    print('Node', node.name)\n",
    "    for child_classname in class_and_prototypes:\n",
    "        \n",
    "        print('\\t'*1, 'Child:', child_classname)\n",
    "        for p in class_and_prototypes[child_classname]:\n",
    "            \n",
    "            logstr = '\\t'*2 + f'Proto:{p} '\n",
    "            mean_activation_of_every_leaf = []\n",
    "            for leaf_descendent in proto_mean_activations[p]:\n",
    "                mean_activation = round(np.mean([activation for activation, *_ in proto_mean_activations[p][leaf_descendent]]), 4)\n",
    "                num_images = len(proto_mean_activations[p][leaf_descendent])\n",
    "                logstr += f'{leaf_descendent}:({mean_activation}) '\n",
    "                mean_activation_of_every_leaf.append(mean_activation)\n",
    "            print(logstr)\n",
    "            \n",
    "            # if the mean_activation is less for all leaf descendants skip the node\n",
    "            if all([mean_act < 0.2 for mean_act in mean_activation_of_every_leaf]):\n",
    "                print(f'Skipping proto {p} of {node.name}')\n",
    "                continue\n",
    "            \n",
    "            # have this for NON descendants\n",
    "            if len(proto_mean_activations[p]) == 0:\n",
    "                continue\n",
    "            \n",
    "            if save_images or save_activation_as_npy_path:\n",
    "                patches = []\n",
    "                right_descriptions = []\n",
    "                text_region_width = 3 # 3x the width of a patch\n",
    "                for leaf_descendent, heap in proto_mean_activations[p].items():\n",
    "                    heap = sorted(heap)[::-1]\n",
    "                    mean_activation = round(np.mean([activation for activation, *_ in proto_mean_activations[p][leaf_descendent]]), 4)\n",
    "                    for rank, ele in enumerate(heap):\n",
    "                        activation, img_to_open, (h_coor_min, h_coor_max, w_coor_min, w_coor_max), latent_activation = ele\n",
    "                        image = transforms.Resize(size=(args.image_size, args.image_size))(Image.open(img_to_open))\n",
    "                        img_tensor = transforms.ToTensor()(image)#.unsqueeze_(0) #shape (1, 3, h, w)\n",
    "                        img_tensor_patch = img_tensor[:, h_coor_min:h_coor_max, w_coor_min:w_coor_max]\n",
    "                        overlayed_image_np = get_heatmap(latent_activation, img_tensor)\n",
    "                        overlayed_image = torch.tensor(overlayed_image_np).permute(2, 0, 1).float() / 255.\n",
    "                        patches.append(overlayed_image)\n",
    "                        \n",
    "                        # added for NUMPY SAVING\n",
    "                        \n",
    "                        if save_activation_as_npy_path:\n",
    "#                             upscaled_similarity_interpolated = get_upscaled_activation_interpolated(latent_activation,\n",
    "#                                                                                        image_size=(args.image_size, args.image_size))\n",
    "                            latent_activation_npy = latent_activation.squeeze().cpu().numpy()\n",
    "                            data = {'node_name': node.name,\n",
    "                                    'proto_num': p,\n",
    "                                    'leaf_desc': leaf_descendent,\n",
    "                                     'rank': rank,\n",
    "                                     'img_path': img_to_open,\n",
    "                                     'img_filename': ntpath.basename(img_to_open),\n",
    "                                     'activation': latent_activation_npy,\n",
    "                                     'max_activation': activation,\n",
    "                                     'model_type': 'NAIVE-HPIPNET'}\n",
    "                            filename = str(rank)+ '-' + ntpath.basename(img_to_open) + '.npy'\n",
    "                            save_path = os.path.join(run_path, save_activation_as_npy_path, \\\n",
    "                                                     node.name, str(p), leaf_descendent,\n",
    "                                                     filename)\n",
    "                            os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "                            np.save(save_path, data, allow_pickle=True)\n",
    "\n",
    "                    # description on the right hand side\n",
    "                    text = f'{mean_activation}, {leaf_descendent}'\n",
    "                    txtimage = Image.new(\"RGB\", (patches[0].shape[-2]*text_region_width,patches[0].shape[-1]), (0, 0, 0))\n",
    "                    draw = D.Draw(txtimage)\n",
    "                    draw.text((150, patches[0].shape[1]//2), text, anchor='mm', fill=\"white\", font=font)\n",
    "                    txttensor = transforms.ToTensor()(txtimage)#.unsqueeze_(0)\n",
    "                    right_descriptions.append(txttensor)\n",
    "                    \n",
    "                # weird thing padding should be zero for non descendants else it raises some error # change\n",
    "                if find_non_descendants or (len(patches) == topk): # (len(patches) == topk) means there is only one leaf descendant\n",
    "                    padding = 0\n",
    "                else:\n",
    "                    padding = 1\n",
    "\n",
    "                grid = torchvision.utils.make_grid(patches, nrow=topk, padding=padding)\n",
    "                grid_right_descriptions = torchvision.utils.make_grid(right_descriptions, nrow=1, padding=padding)\n",
    "\n",
    "                # merging right description with the grid of images\n",
    "                grid = torch.cat([grid, grid_right_descriptions], dim=-1)\n",
    "\n",
    "                # description on the top\n",
    "                text = f'Node:{node.name}, p{p}, Child:{child_classname}'\n",
    "                txtimage = Image.new(\"RGB\", (grid.shape[-1], args.wshape), (0, 0, 0))\n",
    "                draw = D.Draw(txtimage)\n",
    "                draw.text((150, patches[0].shape[1]//2), text, anchor='mm', fill=\"white\", font=font)\n",
    "                txttensor = transforms.ToTensor()(txtimage)#.unsqueeze_(0)\n",
    "\n",
    "                # merging top description with the grid of images\n",
    "                grid = torch.cat([grid, txttensor], dim=1)\n",
    "                \n",
    "                if save_images:\n",
    "                    prefix = 'non_' if find_non_descendants else ''\n",
    "                    os.makedirs(os.path.join(run_path, prefix+f'descendent_specific_topk_heatmap_ep={epoch}', node.name), exist_ok=True)\n",
    "                    torchvision.utils.save_image(grid, os.path.join(run_path, prefix+f'descendent_specific_topk_heatmap_ep={epoch}', node.name, f'{child_classname}-p{p}.png'))\n",
    "\n",
    "txt_file.write('\\n')\n",
    "txt_file.close()\n",
    "print('Done !!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
