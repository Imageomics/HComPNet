{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf3473e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "import sys, os\n",
    "import random\n",
    "import numpy as np\n",
    "from shutil import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "import shutil\n",
    "import pickle\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torchvision.datasets.folder import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "from skimage.filters import threshold_local, gaussian\n",
    "\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ecb88ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_path = '/home/harishbabu/projects/PIPNet/runs/010-CUB-27-imgnet_OOD_cnext26_img=224_nprotos=20'\n",
    "# run_path = '/home/harishbabu/projects/PIPNet/runs/031-CUB-18-imgnet_cnext26_img=224_nprotos=20_orth-on-rel'\n",
    "# run_path = '/home/harishbabu/projects/PIPNet/runs/032-CUB-18-imgnet_cnext26_img=224_nprotos=20_orth-on-rel'\n",
    "\n",
    "# run_path = '/home/harishbabu/projects/PIPNet/runs/035-CUB-18-imgnet_OOD_cnext26_img=224_nprotos=20_orth-on-rel'\n",
    "\n",
    "# run_path = '/home/harishbabu/projects/PIPNet/runs/043-035_clone-CUB-18-imgnet_OOD_cnext26_img=224_nprotos=20_orth-on-rel'\n",
    "# run_path = \"/home/harishbabu/projects/PIPNet/runs/036-CUB-18-imgnet_OOD_cnext26_img=224_nprotos=20_orth-on-rel_uniformity\"\n",
    "# run_path = \"/home/harishbabu/projects/PIPNet/runs/041-035_clone-CUB-18-imgnet_OOD_cnext26_img=224_nprotos=20_orth-on-rel\"\n",
    "# run_path = \"/home/harishbabu/projects/PIPNet/runs/042-035_clone-CUB-18-imgnet_OOD_cnext26_img=224_nprotos=20_orth-on-rel\"\n",
    "\n",
    "# run_path = \"/home/harishbabu/projects/PIPNet/runs/044-CUB-18-imgnet_OOD_cnext26_img=224_nprotos=20-or-4per-desc_orth-on-rel\"\n",
    "\n",
    "# run_path = \"/home/harishbabu/projects/PIPNet/runs/046-CUB-18-imgnet_OOD_cnext26_img=224_nprotos=10per-desc_orth-on-rel\"\n",
    "# run_path = \"/home/harishbabu/projects/PIPNet/runs/047-CUB-18-imgnet_OOD_cnext26_img=224_nprotos=5per-desc_tanh-desc\"\n",
    "# run_path = \"/home/harishbabu/projects/PIPNet/runs/048-CUB-18-imgnet_OOD_cnext26_img=224_nprotos=5per-desc_tanh-desc_unit-sphere\"\n",
    "# run_path = \"/home/harishbabu/projects/PIPNet/runs/051-CUB-18-imgnet_cnext26_img=224_nprotos=4per-desc_tanh-desc_unit-sphere_AW=5-TW=2-UW=2-CW=2\"\n",
    "# run_path = \"/home/harishbabu/projects/PIPNet/runs/052-CUB-18-imgnet_OOD_cnext26_img=224_nprotos=4per-desc_tanh-desc_unit-sphere_AW=5-TW=2-UW=2-CW=2\"\n",
    "# run_path = \"/home/harishbabu/projects/PIPNet/runs/055-CUB-18_cnext26_img=224_nprotos=4per-desc_unit-sphere_no-softmax_AW=3-TW=2-UW=3-CW=2\"\n",
    "# run_path = \"/home/harishbabu/projects/PIPNet/runs/056-CUB-18-imgnet_cnext26_img=224_nprotos=4per-desc_unit-sphere_no-softmax_AW=3-TW=2-UW=3-CW=2\"\n",
    "# run_path = \"/home/harishbabu/projects/PIPNet/runs/057-CUB-18-imgnet_cnext26_img=224_nprotos=4per-desc_unit-sphere_no-meanpool_no-softmax_AW=3-TW=2-UW=3-CW=2\"\n",
    "# run_path = \"/home/harishbabu/projects/PIPNet/runs/058-CUB-18-imgnet_with-equalize-aug_cnext26_img=224_nprotos=4per-desc_unit-sphere_no-meanpool_no-softmax_AW=3-TW=2-UW=3-CW=2\"\n",
    "\n",
    "# with unit sphere\n",
    "# run_path = \"/home/harishbabu/projects/PIPNet/runs/059-CUB-18-imgnet_with-equalize-aug_cnext26_img=224_nprotos=4per-desc_unit-sphere_finetune=5_no-meanpool_no-softmax_AW=3-TW=2-UW=3-CW=2_batch=20\"\n",
    "\n",
    "# unit sphere with softmax\n",
    "# run_path = \"/home/harishbabu/projects/PIPNet/runs/065-CUB-18-imgnet_with-equalize-aug_cnext26_img=224_nprotos=4per-desc_unit-sphere_finetune=5_no-meanpool_with-softmax_AW=3-TW=2-UW=3-CW=2_batch=20\"\n",
    "\n",
    "# original hpipnet with 20 protos per node no KO, no OOD, no tanh-desc\n",
    "# run_path = \"/home/harishbabu/projects/PIPNet/runs/062-CUB-18-imgnet_with-equalize-aug_cnext26_img=224_nprotos=20_no-KO_no-OOD\"\n",
    "\n",
    "# original hpipnet with 20 protos per node no KO, no OOD, WITH tanh-desc\n",
    "# run_path = \"/home/harishbabu/projects/PIPNet/runs/063-CUB-18-imgnet_with-equalize-aug_cnext26_img=224_nprotos=20_no-KO_no-OOD_tanh-desc\"\n",
    "\n",
    "# with unit sphere but no AL+UNI\n",
    "# run_path = \"/home/harishbabu/projects/PIPNet/runs/066-CUB-18-imgnet_with-equalize-aug_cnext26_img=224_nprotos=4per-desc_unit-sphere_finetune=5_no-meanpool_no-softmax_no-align_no-uni_AW=3-TW=2-UW=3-CW=2_batch=20\"\n",
    "\n",
    "# with unit sphere, protopool, with softmax, no tanh-desc\n",
    "# run_path = \"/home/harishbabu/projects/PIPNet/runs/067-CUB-18-imgnet_with-equalize-aug_cnext26_img=224_nprotos=4per-desc_unit-sphere-protopool_finetune=5_no-meanpool_with-softmax_AW=3-TW=2-UW=3-CW=2_batch=20\"\n",
    "\n",
    "# with unit sphere, protopool, with softmax, no tanh-desc, INCORRECT\n",
    "# run_path = \"/home/harishbabu/projects/PIPNet/runs/067-incorrect-CUB-18-imgnet_with-equalize-aug_cnext26_img=224_nprotos=4per-desc_unit-sphere-protopool_finetune=5_no-meanpool_with-softmax_AW=3-TW=2-UW=3-CW=2_batch=20\"\n",
    "\n",
    "# with unit sphere, protopool, no softmax, no tanh-desc\n",
    "# run_path = \"/home/harishbabu/projects/PIPNet/runs/068-CUB-18-imgnet_with-equalize-aug_cnext26_img=224_nprotos=4per-desc_unit-sphere-protopool_finetune=5_no-meanpool_no-softmax_AW=3-TW=2-UW=3-CW=2_batch=20\"\n",
    "\n",
    "# 071 with bias\n",
    "# run_path = \"/home/harishbabu/projects/PIPNet/runs/071-CUB-18-imgnet_with-equalize-aug_cnext26_img=224_nprotos=4per-desc_unit-sphere-protopool_finetune=5_no-meanpool_with-softmax_with-addon-bias_AW=3-TW=2-UW=3-CW=2_batch=20\"\n",
    "\n",
    "# 072 gumbel softmax\n",
    "# run_path = \"/home/harishbabu/projects/PIPNet/runs/072-CUB-18-imgnet_with-equalize-aug_cnext26_img=224_nprotos=4per-desc_unit-sphere-protopool_finetune=5_no-meanpool_with-gumbel-softmax_no-addon-bias_AW=3-TW=2-UW=3-CW=2_batch=20\"\n",
    "\n",
    "# 073 gumbel softmax, tau-1.0\n",
    "# run_path = \"/home/harishbabu/projects/PIPNet/runs/073-CUB-18-imgnet_with-equalize-aug_cnext26_img=224_nprotos=4per-desc_unit-sphere-protopool_finetune=5_no-meanpool_with-gumbel-softmax-tau=1_no-addon-bias_AW=3-TW=2-UW=3-CW=2_batch=20\"\n",
    "\n",
    "# 075 068 with focal loss\n",
    "# run_path = \"/home/harishbabu/projects/PIPNet/runs/075-068-with-focal_CUB-18-imgnet_with-equalize-aug_cnext26_img=224_nprotos=4per-desc_unit-sphere-protopool_finetune=5_no-meanpool_no-softmax_no-addon-bias_AW=3-TW=2-UW=3-CW=2_batch=20\"\n",
    "\n",
    "# 076 cs followed by softmax. Uses align_pf along with align+uni\n",
    "# run_path = \"/home/harishbabu/projects/PIPNet/runs/076_CUB-18-imgnet_with-equalize-aug_cnext26_img=224_nprotos=4per-desc_unit-sphere-protopool_finetune=5_align-pf-during-training_no-meanpool_no-softmax_no-addon-bias_AW=3-TW=2-UW=3-CW=2-APW=5_batch=20\"\n",
    "\n",
    "# 074 multiply_cs_softmax\n",
    "# run_path = \"/home/harishbabu/projects/PIPNet/runs/074-CUB-18-imgnet_with-equalize-aug_cnext26_img=224_nprotos=4per-desc_unit-sphere-protopool_finetune=5_no-meanpool_with-softmax_multi-cs-softmax_no-addon-bias_AW=3-TW=2-UW=3-CW=2_batch=20\"\n",
    "\n",
    "# 077 unit sphere protopool with cosin no softmax constant 20 protos per node\n",
    "# run_path = \"/home/harishbabu/projects/PIPNet/runs/077_CUB-18-imgnet_with-equalize-aug_cnext26_img=224_nprotos=20-sphere-protopool_finetune=5_align-pf-during-training_no-meanpool_no-softmax_no-addon-bias_AW=3-TW=2-UW=3-CW=2_batch=20\"\n",
    "\n",
    "# 082 unit sphere cs followed by softmax with minmazimize loss\n",
    "# run_path = \"/home/harishbabu/projects/PIPNet/runs/082-CUB-18-imgnet_with-equalize-aug_cnext26_img=224_nprotos=4per-leaf-desc_unit-sphere_finetune=5_no-meanpool_with-softmax_no-addon-bias_AW=3-TW=2-MMW=2-UW=3-CW=2_mm-loss_batch=48\"\n",
    "\n",
    "# 083 unit sphere cs followed by softmax with minmazimize loss\n",
    "# run_path = \"/home/harishbabu/projects/PIPNet/runs/083-CUB-18-imgnet_with-equalize-aug_cnext26_img=224_nprotos=4per-leaf-desc_unit-sphere_finetune=5_no-meanpool_with-softmax_no-addon-bias_AW=3-TW=2-MMW=2-UW=3-CW=2_no-align_no-uni_no-mm-loss_batch=48\"\n",
    "\n",
    "# 085 unit sphere cs followed by softmax-with-tau with minmazimize loss\n",
    "# run_path = \"/home/harishbabu/projects/PIPNet/runs/085-notebook-CUB-18-imgnet_with-equalize-aug_cnext26_img=224_nprotos=4per-leaf-desc_unit-sphere_finetune=5_no-meanpool_with-softmax-tau=0.2_no-addon-bias_AW=3-TW=2-MMW=2-UW=3-CW=2_mm-loss_batch=12\"\n",
    "\n",
    "# 091 basic gaussian multiplier on stage 4\n",
    "# run_path = \"/home/harishbabu/projects/PIPNet/runs/091-CUB-18-imgnet_with-equalize-aug_cnext26_BGM=4|1.0|50_img=224_latent-dim=256_nprotos=4per-leaf-desc_unit-sphere_finetune=5_no-meanpool_with-softmax-tau=0.2_no-addon-bias_AW=3-TW=2-MMW=2-UW=3-CW=2_mm-loss_batch=48\"\n",
    "\n",
    "# 092 basic gaussian multiplier on stage 3, 4\n",
    "# run_path = \"/home/harishbabu/projects/PIPNet/runs/092-CUB-18-imgnet_with-equalize-aug_cnext26_BGM=3,4|1.0|50_img=224_latent-dim=256_nprotos=4per-leaf-desc_unit-sphere_finetune=5_no-meanpool_with-softmax-tau=0.2_no-addon-bias_AW=3-TW=2-MMW=2-UW=3-CW=2_mm-loss_batch=48\"\n",
    "\n",
    "# 093 128 dim linear\n",
    "# run_path = \"/home/harishbabu/projects/PIPNet/runs/093-CUB-18-imgnet_with-equalize-aug_cnext26_BGM=4|1.0|50_img=224_latent-dim=128_nprotos=20_unit-sphere_finetune=5_no-meanpool_with-softmax-tau=0.2_no-addon-bias_AW=3-TW=2-MMW=2-UW=1-CW=2_mm-loss_batch=48\"\n",
    "\n",
    "# 094 128 dim linear\n",
    "# run_path = \"/home/harishbabu/projects/PIPNet/runs/094-CUB-18-imgnet_with-equalize-aug_cnext26_BGM=4|1.0|50_img=224_latent-dim=128_nprotos=20_unit-sphere_finetune=5_no-meanpool_with-softmax-tau=0.2_no-addon-bias_AW=3-TW=2-MMW=2-UW=1-CW=2_mm-loss_batch=48\"\n",
    "\n",
    "# 095 ablation 091 without AL+UNI\n",
    "# run_path = \"/home/harishbabu/projects/PIPNet/runs/095-091-woALUNI-CUB-18-imgnet_with-equalize-aug_cnext26_BGM=4|1.0|50_img=224_nprotos=4per-leaf-desc_unit-sphere_finetune=5_no-meanpool_with-softmax-tau=0.2_no-addon-bias_AW=3-TW=2-MMW=2-UW=3-CW=2_no-AL_no-UNI_mm-loss_batch=48\"\n",
    "\n",
    "# 096 ablation 091 without AL+UNI\n",
    "# run_path = \"/home/harishbabu/projects/PIPNet/runs/096-091-wfocal-CUB-18-imgnet_with-equalize-aug_cnext26_BGM=4|1.0|50_img=224_nprotos=4per-leaf-desc_unit-sphere_finetune=5_no-meanpool_with-softmax-tau=0.2_no-addon-bias_AW=3-TW=2-MMW=2-UW=3-CW=2_mm-loss_batch=48\"\n",
    "\n",
    "# 097 - 091 with bg\n",
    "# run_path = \"/home/harishbabu/projects/PIPNet/runs/097-091-wbg-CUB-18_with-equalize-aug_cnext26_BGM=4|1.0|50_img=224_nprotos=4per-leaf-desc_unit-sphere_finetune=5_no-meanpool_with-softmax-tau=0.2_no-addon-bias_AW=3-TW=2-MMW=2-UW=3-CW=2_mm-loss_batch=48\"\n",
    "\n",
    "# 0100 cub29 with 20 per node\n",
    "# run_path = \"/home/harishbabu/projects/PIPNet/runs/100_CUB-29-imgnet_with-equalize-aug_cnext26_BGM=4|1.0|50_img=224_nprotos=20_unit-sphere-protopool_no-meanpool_with-softmax-tau=0.2_no-addon-bias_AW=3-TW=2-MMW=2-UW=3-CW=2_mm-loss_batch=48\"\n",
    "\n",
    "# 0101 baseline with 4 per desc per node\n",
    "# run_path = \"/home/harishbabu/projects/PIPNet/runs/101-baseline-CUB-18-imgnet_with-equalize-aug_cnext26_img=224_nprotos=4per-desc_no-KO_no-OOD\"\n",
    "\n",
    "# 0103 091 with 20 per node\n",
    "# run_path = \"/home/harishbabu/projects/PIPNet/runs/103-091-wProtoPool20PerNode_CUB-18-imgnet_with-equalize-aug_cnext26_BGM=4|1.0|50_img=224_nprotos=20_unit-sphere-protopool_no-meanpool_with-softmax-tau=0.2_no-addon-bias_AW=3-TW=2-MMW=2-UW=3-CW=2_batch=48\"\n",
    "\n",
    "# 0108 LOU3\n",
    "run_path = \"/home/harishbabu/projects/PIPNet/runs/108-wProtoPool20PerNode_LOU3_CUB-18-imgnet-bg_with-equalize-aug_cnext26_BGM=4|1.0|50_img=224_nprotos=20_unit-sphere-protopool_no-meanpool_with-softmax-tau=0.2_no-addon-bias_AW=3-TW=2-MMW=2-UW=3-CW=2_batch=48\"\n",
    "\n",
    "try:\n",
    "    sys.path.remove('/home/harishbabu/projects/PIPNet')\n",
    "except:\n",
    "    pass\n",
    "sys.path.insert(0, os.path.join(run_path, 'source_clone'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d53cfe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipnet.pipnet import PIPNet, get_network\n",
    "from util.log import Log\n",
    "from util.args import get_args, save_args, get_optimizer_nn\n",
    "from util.data import get_dataloaders\n",
    "from util.func import init_weights_xavier\n",
    "from pipnet.train import train_pipnet, test_pipnet\n",
    "# from pipnet.test import eval_pipnet, get_thresholds, eval_ood\n",
    "from util.eval_cub_csv import eval_prototypes_cub_parts_csv, get_topk_cub, get_proto_patches_cub\n",
    "from util.vis_pipnet import visualize, visualize_topk\n",
    "from util.visualize_prediction import vis_pred, vis_pred_experiments\n",
    "from util.node import Node\n",
    "from util.phylo_utils import construct_phylo_tree, construct_discretized_phylo_tree\n",
    "from util.func import get_patch_size\n",
    "from util.data import ModifiedLabelLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c11d438b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------- No discretization -------------------------\n"
     ]
    }
   ],
   "source": [
    "args_file = open(os.path.join(run_path, 'metadata', 'args.pickle'), 'rb')\n",
    "args = pickle.load(args_file)\n",
    "\n",
    "if args.phylo_config:\n",
    "    phylo_config = OmegaConf.load(args.phylo_config)\n",
    "\n",
    "if args.phylo_config:\n",
    "    # construct the phylo tree\n",
    "    if phylo_config.phyloDistances_string == 'None':\n",
    "        if '031' in run_path: # this run uses a different phylogeny file that had an extra root node which is a mistake\n",
    "            root = construct_phylo_tree('/home/harishbabu/data/phlyogenyCUB/18Species-with-extra-root-node/1_tree-consensus-Hacket-18Species-modified_cub-names_v1.phy')\n",
    "        else:\n",
    "            root = construct_phylo_tree(phylo_config.phylogeny_path)\n",
    "        print('-'*25 + ' No discretization ' + '-'*25)\n",
    "    else:\n",
    "        root = construct_discretized_phylo_tree(phylo_config.phylogeny_path, phylo_config.phyloDistances_string)\n",
    "        print('-'*25 + ' Discretized ' + '-'*25)\n",
    "else:\n",
    "    # construct the tree (original hierarchy as described in the paper)\n",
    "    root = Node(\"root\")\n",
    "    root.add_children(['animal','vehicle','everyday_object','weapon','scuba_diver'])\n",
    "    root.add_children_to('animal',['non_primate','primate'])\n",
    "    root.add_children_to('non_primate',['African_elephant','giant_panda','lion'])\n",
    "    root.add_children_to('primate',['capuchin','gibbon','orangutan'])\n",
    "    root.add_children_to('vehicle',['ambulance','pickup','sports_car'])\n",
    "    root.add_children_to('everyday_object',['laptop','sandal','wine_bottle'])\n",
    "    root.add_children_to('weapon',['assault_rifle','rifle'])\n",
    "    # flat root\n",
    "    # root.add_children(['scuba_diver','African_elephant','giant_panda','lion','capuchin','gibbon','orangutan','ambulance','pickup','sports_car','laptop','sandal','wine_bottle','assault_rifle','rifle'])\n",
    "root.assign_all_descendents()\n",
    "\n",
    "exp_no = int(os.path.basename(run_path)[:3])\n",
    "\n",
    "if exp_no < 77:\n",
    "    if ('num_protos_per_descendant' in args) and (args.num_protos_per_descendant > 0):\n",
    "        for node in root.nodes_with_children():\n",
    "            node.set_num_protos(args.num_protos_per_descendant)\n",
    "else:\n",
    "    if ('num_protos_per_descendant' in args):\n",
    "        # update num of protos per node based on num_protos_per_descendant\n",
    "        if args.num_features == 0 and args.num_protos_per_descendant == 0:\n",
    "            raise Exception('Either of num_features or num_protos_per_descendant must be greater than zero')\n",
    "        for node in root.nodes_with_children():\n",
    "            node.set_num_protos(num_protos_per_descendant=args.num_protos_per_descendant,\\\n",
    "                                min_protos=args.num_features,\\\n",
    "                                split_protos=('protopool' in args) and (args.protopool == 'n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e3ed910",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harishbabu/.conda/envs/hpnet1/lib/python3.8/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num classes (k) =  18 ['cub_001_Black_footed_Albatross', 'cub_002_Laysan_Albatross', 'cub_003_Sooty_Albatross', 'cub_004_Groove_billed_Ani', 'cub_023_Brandt_Cormorant'] etc.\n",
      "48 48\n",
      "Classes:  {'cub_001_Black_footed_Albatross': 0, 'cub_002_Laysan_Albatross': 1, 'cub_003_Sooty_Albatross': 2, 'cub_004_Groove_billed_Ani': 3, 'cub_023_Brandt_Cormorant': 4, 'cub_024_Red_faced_Cormorant': 5, 'cub_025_Pelagic_Cormorant': 6, 'cub_031_Black_billed_Cuckoo': 7, 'cub_032_Mangrove_Cuckoo': 8, 'cub_033_Yellow_billed_Cuckoo': 9, 'cub_045_Northern_Fulmar': 10, 'cub_050_Eared_Grebe': 11, 'cub_051_Horned_Grebe': 12, 'cub_052_Pied_billed_Grebe': 13, 'cub_053_Western_Grebe': 14, 'cub_086_Pacific_Loon': 15, 'cub_100_Brown_Pelican': 16, 'cub_101_White_Pelican': 17}\n",
      "stage 4\n",
      "Number of prototypes:  20\n",
      "----------Prototypes per descendant: 0----------\n",
      "Assigned 20 protos to node root\n",
      "Assigned 20 protos to node 052+053\n",
      "Assigned 20 protos to node 004+086\n",
      "Assigned 20 protos to node 053+050\n",
      "Assigned 20 protos to node 004+032\n",
      "Assigned 20 protos to node 086+045\n",
      "Assigned 20 protos to node 050+051\n",
      "Assigned 20 protos to node 032+033\n",
      "Assigned 20 protos to node 045+101\n",
      "Assigned 20 protos to node 033+031\n",
      "Assigned 20 protos to node 045+003\n",
      "Assigned 20 protos to node 101+023\n",
      "Assigned 20 protos to node 003+002\n",
      "Assigned 20 protos to node 101+100\n",
      "Assigned 20 protos to node 023+025\n",
      "Assigned 20 protos to node 002+001\n",
      "Assigned 20 protos to node 025+024\n",
      "DataParallel(\n",
      "  (module): PIPNet(\n",
      "    (_net): ConvNeXt(\n",
      "      (features): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))\n",
      "          (1): LayerNorm2d((96,), eps=1e-06, elementwise_affine=True)\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): CNBlock(\n",
      "            (block): Sequential(\n",
      "              (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
      "              (1): Permute()\n",
      "              (2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
      "              (3): Linear(in_features=96, out_features=384, bias=True)\n",
      "              (4): GELU(approximate='none')\n",
      "              (5): Linear(in_features=384, out_features=96, bias=True)\n",
      "              (6): Permute()\n",
      "            )\n",
      "            (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
      "          )\n",
      "          (1): CNBlock(\n",
      "            (block): Sequential(\n",
      "              (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
      "              (1): Permute()\n",
      "              (2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
      "              (3): Linear(in_features=96, out_features=384, bias=True)\n",
      "              (4): GELU(approximate='none')\n",
      "              (5): Linear(in_features=384, out_features=96, bias=True)\n",
      "              (6): Permute()\n",
      "            )\n",
      "            (stochastic_depth): StochasticDepth(p=0.0058823529411764705, mode=row)\n",
      "          )\n",
      "          (2): CNBlock(\n",
      "            (block): Sequential(\n",
      "              (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
      "              (1): Permute()\n",
      "              (2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
      "              (3): Linear(in_features=96, out_features=384, bias=True)\n",
      "              (4): GELU(approximate='none')\n",
      "              (5): Linear(in_features=384, out_features=96, bias=True)\n",
      "              (6): Permute()\n",
      "            )\n",
      "            (stochastic_depth): StochasticDepth(p=0.011764705882352941, mode=row)\n",
      "          )\n",
      "        )\n",
      "        (2): Sequential(\n",
      "          (0): LayerNorm2d((96,), eps=1e-06, elementwise_affine=True)\n",
      "          (1): Conv2d(96, 192, kernel_size=(2, 2), stride=(2, 2))\n",
      "        )\n",
      "        (3): Sequential(\n",
      "          (0): CNBlock(\n",
      "            (block): Sequential(\n",
      "              (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
      "              (1): Permute()\n",
      "              (2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "              (3): Linear(in_features=192, out_features=768, bias=True)\n",
      "              (4): GELU(approximate='none')\n",
      "              (5): Linear(in_features=768, out_features=192, bias=True)\n",
      "              (6): Permute()\n",
      "            )\n",
      "            (stochastic_depth): StochasticDepth(p=0.017647058823529415, mode=row)\n",
      "          )\n",
      "          (1): CNBlock(\n",
      "            (block): Sequential(\n",
      "              (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
      "              (1): Permute()\n",
      "              (2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "              (3): Linear(in_features=192, out_features=768, bias=True)\n",
      "              (4): GELU(approximate='none')\n",
      "              (5): Linear(in_features=768, out_features=192, bias=True)\n",
      "              (6): Permute()\n",
      "            )\n",
      "            (stochastic_depth): StochasticDepth(p=0.023529411764705882, mode=row)\n",
      "          )\n",
      "          (2): CNBlock(\n",
      "            (block): Sequential(\n",
      "              (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
      "              (1): Permute()\n",
      "              (2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "              (3): Linear(in_features=192, out_features=768, bias=True)\n",
      "              (4): GELU(approximate='none')\n",
      "              (5): Linear(in_features=768, out_features=192, bias=True)\n",
      "              (6): Permute()\n",
      "            )\n",
      "            (stochastic_depth): StochasticDepth(p=0.029411764705882353, mode=row)\n",
      "          )\n",
      "        )\n",
      "        (4): Sequential(\n",
      "          (0): LayerNorm2d((192,), eps=1e-06, elementwise_affine=True)\n",
      "          (1): Conv2d(192, 384, kernel_size=(2, 2), stride=(1, 1))\n",
      "        )\n",
      "        (5): Sequential(\n",
      "          (0): CNBlock(\n",
      "            (block): Sequential(\n",
      "              (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "              (1): Permute()\n",
      "              (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "              (3): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (4): GELU(approximate='none')\n",
      "              (5): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (6): Permute()\n",
      "            )\n",
      "            (stochastic_depth): StochasticDepth(p=0.03529411764705883, mode=row)\n",
      "          )\n",
      "          (1): CNBlock(\n",
      "            (block): Sequential(\n",
      "              (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "              (1): Permute()\n",
      "              (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "              (3): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (4): GELU(approximate='none')\n",
      "              (5): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (6): Permute()\n",
      "            )\n",
      "            (stochastic_depth): StochasticDepth(p=0.0411764705882353, mode=row)\n",
      "          )\n",
      "          (2): CNBlock(\n",
      "            (block): Sequential(\n",
      "              (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "              (1): Permute()\n",
      "              (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "              (3): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (4): GELU(approximate='none')\n",
      "              (5): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (6): Permute()\n",
      "            )\n",
      "            (stochastic_depth): StochasticDepth(p=0.047058823529411764, mode=row)\n",
      "          )\n",
      "          (3): CNBlock(\n",
      "            (block): Sequential(\n",
      "              (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "              (1): Permute()\n",
      "              (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "              (3): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (4): GELU(approximate='none')\n",
      "              (5): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (6): Permute()\n",
      "            )\n",
      "            (stochastic_depth): StochasticDepth(p=0.052941176470588235, mode=row)\n",
      "          )\n",
      "          (4): CNBlock(\n",
      "            (block): Sequential(\n",
      "              (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "              (1): Permute()\n",
      "              (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "              (3): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (4): GELU(approximate='none')\n",
      "              (5): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (6): Permute()\n",
      "            )\n",
      "            (stochastic_depth): StochasticDepth(p=0.058823529411764705, mode=row)\n",
      "          )\n",
      "          (5): CNBlock(\n",
      "            (block): Sequential(\n",
      "              (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "              (1): Permute()\n",
      "              (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "              (3): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (4): GELU(approximate='none')\n",
      "              (5): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (6): Permute()\n",
      "            )\n",
      "            (stochastic_depth): StochasticDepth(p=0.06470588235294118, mode=row)\n",
      "          )\n",
      "          (6): CNBlock(\n",
      "            (block): Sequential(\n",
      "              (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "              (1): Permute()\n",
      "              (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "              (3): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (4): GELU(approximate='none')\n",
      "              (5): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (6): Permute()\n",
      "            )\n",
      "            (stochastic_depth): StochasticDepth(p=0.07058823529411766, mode=row)\n",
      "          )\n",
      "          (7): CNBlock(\n",
      "            (block): Sequential(\n",
      "              (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "              (1): Permute()\n",
      "              (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "              (3): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (4): GELU(approximate='none')\n",
      "              (5): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (6): Permute()\n",
      "            )\n",
      "            (stochastic_depth): StochasticDepth(p=0.07647058823529412, mode=row)\n",
      "          )\n",
      "          (8): CNBlock(\n",
      "            (block): Sequential(\n",
      "              (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "              (1): Permute()\n",
      "              (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "              (3): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (4): GELU(approximate='none')\n",
      "              (5): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (6): Permute()\n",
      "            )\n",
      "            (stochastic_depth): StochasticDepth(p=0.0823529411764706, mode=row)\n",
      "          )\n",
      "        )\n",
      "        (6): Sequential(\n",
      "          (0): LayerNorm2d((384,), eps=1e-06, elementwise_affine=True)\n",
      "          (1): Conv2d(384, 768, kernel_size=(2, 2), stride=(1, 1))\n",
      "        )\n",
      "        (7): Sequential(\n",
      "          (0): CNBlock(\n",
      "            (block): Sequential(\n",
      "              (0): BasicGaussianMultiplierConv2D(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
      "              (1): Permute()\n",
      "              (2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "              (3): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (4): GELU(approximate='none')\n",
      "              (5): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (6): Permute()\n",
      "            )\n",
      "            (stochastic_depth): StochasticDepth(p=0.08823529411764706, mode=row)\n",
      "          )\n",
      "          (1): CNBlock(\n",
      "            (block): Sequential(\n",
      "              (0): BasicGaussianMultiplierConv2D(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
      "              (1): Permute()\n",
      "              (2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "              (3): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (4): GELU(approximate='none')\n",
      "              (5): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (6): Permute()\n",
      "            )\n",
      "            (stochastic_depth): StochasticDepth(p=0.09411764705882353, mode=row)\n",
      "          )\n",
      "          (2): CNBlock(\n",
      "            (block): Sequential(\n",
      "              (0): BasicGaussianMultiplierConv2D(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
      "              (1): Permute()\n",
      "              (2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "              (3): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (4): GELU(approximate='none')\n",
      "              (5): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (6): Permute()\n",
      "            )\n",
      "            (stochastic_depth): StochasticDepth(p=0.1, mode=row)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (avgpool): Identity()\n",
      "      (classifier): Identity()\n",
      "    )\n",
      "    (_root_add_on): UnitConv2D(768, 20, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (_052+053_add_on): UnitConv2D(768, 20, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (_004+086_add_on): UnitConv2D(768, 20, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (_053+050_add_on): UnitConv2D(768, 20, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (_004+032_add_on): UnitConv2D(768, 20, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (_086+045_add_on): UnitConv2D(768, 20, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (_050+051_add_on): UnitConv2D(768, 20, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (_032+033_add_on): UnitConv2D(768, 20, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (_045+101_add_on): UnitConv2D(768, 20, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (_033+031_add_on): UnitConv2D(768, 20, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (_045+003_add_on): UnitConv2D(768, 20, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (_101+023_add_on): UnitConv2D(768, 20, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (_003+002_add_on): UnitConv2D(768, 20, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (_101+100_add_on): UnitConv2D(768, 20, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (_023+025_add_on): UnitConv2D(768, 20, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (_002+001_add_on): UnitConv2D(768, 20, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (_025+024_add_on): UnitConv2D(768, 20, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (_pool): Sequential(\n",
      "      (0): AdaptiveMaxPool2d(output_size=(1, 1))\n",
      "      (1): Flatten(start_dim=1, end_dim=-1)\n",
      "    )\n",
      "    (_avg_pool): Sequential(\n",
      "      (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "      (1): Flatten(start_dim=1, end_dim=-1)\n",
      "    )\n",
      "    (_root_classification): NonNegLinear()\n",
      "    (_052+053_classification): NonNegLinear()\n",
      "    (_004+086_classification): NonNegLinear()\n",
      "    (_053+050_classification): NonNegLinear()\n",
      "    (_004+032_classification): NonNegLinear()\n",
      "    (_086+045_classification): NonNegLinear()\n",
      "    (_050+051_classification): NonNegLinear()\n",
      "    (_032+033_classification): NonNegLinear()\n",
      "    (_045+101_classification): NonNegLinear()\n",
      "    (_033+031_classification): NonNegLinear()\n",
      "    (_045+003_classification): NonNegLinear()\n",
      "    (_101+023_classification): NonNegLinear()\n",
      "    (_003+002_classification): NonNegLinear()\n",
      "    (_101+100_classification): NonNegLinear()\n",
      "    (_023+025_classification): NonNegLinear()\n",
      "    (_002+001_classification): NonNegLinear()\n",
      "    (_025+024_classification): NonNegLinear()\n",
      "    (_softmax): Softmax(dim=1)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    device_ids = [torch.cuda.current_device()]\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    device_ids = []\n",
    "\n",
    "# args_file = open(os.path.join(run_path, 'metadata', 'args.pickle'), 'rb')\n",
    "# args = pickle.load(args_file)\n",
    "\n",
    "# ckpt_file_name = 'net_overspecific_pruned_replaced_thresh=0.5_last'\n",
    "ckpt_file_name = 'net_trained_last'\n",
    "# ckpt_file_name = 'net_trained_10'\n",
    "# ckpt_file_name = 'net_pretrained'\n",
    "epoch = ckpt_file_name.split('_')[-1]\n",
    "\n",
    "ckpt_path = os.path.join(run_path, 'checkpoints', ckpt_file_name)\n",
    "checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "\n",
    "if ckpt_file_name != 'net_trained_last':\n",
    "    print('\\n', (10*'-')+'WARNING: Not using the final trained model'+(10*'-'), '\\n')\n",
    "\n",
    "# Obtain the dataset and dataloaders\n",
    "trainloader, trainloader_pretraining, trainloader_normal, trainloader_normal_augment, projectloader, testloader, test_projectloader, classes = get_dataloaders(args, device)\n",
    "\n",
    "print(args.batch_size, trainloader.batch_size)\n",
    "\n",
    "if len(classes)<=20:\n",
    "    if args.validation_size == 0.:\n",
    "        print(\"Classes: \", testloader.dataset.class_to_idx, flush=True)\n",
    "    else:\n",
    "        print(\"Classes: \", str(classes), flush=True)\n",
    "\n",
    "# Create a convolutional network based on arguments and add 1x1 conv layer\n",
    "feature_net, add_on_layers, pool_layer, classification_layers, num_prototypes = get_network(len(classes), args, root=root)\n",
    "   \n",
    "# Create a PIP-Net\n",
    "net = PIPNet(num_classes=len(classes),\n",
    "                    num_prototypes=num_prototypes,\n",
    "                    feature_net = feature_net,\n",
    "                    args = args,\n",
    "                    add_on_layers = add_on_layers,\n",
    "                    pool_layer = pool_layer,\n",
    "                    classification_layers = classification_layers,\n",
    "                    num_parent_nodes = len(root.nodes_with_children()),\n",
    "                    root = root\n",
    "                    )\n",
    "net = net.to(device=device)\n",
    "net = nn.DataParallel(net, device_ids = device_ids)    \n",
    "net.load_state_dict(checkpoint['model_state_dict'],strict=True)\n",
    "print(net.eval())\n",
    "criterion = nn.NLLLoss(reduction='mean').to(device)\n",
    "\n",
    "# Forward one batch through the backbone to get the latent output size\n",
    "# with torch.no_grad():\n",
    "#     xs1, _, _ = next(iter(trainloader))\n",
    "#     xs1 = xs1.to(device)\n",
    "#     proto_features, _, _ = net(xs1)\n",
    "#     wshape = proto_features['root'].shape[-1]\n",
    "#     args.wshape = wshape #needed for calculating image patch size\n",
    "#     print(\"Output shape: \", proto_features['root'].shape, flush=True)\n",
    "    \n",
    "args.wshape = 26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0153d4a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(net.module._net).upper().startswith('CONVNEXT')\n",
    "\n",
    "[i for i in net.module._net.modules() if isinstance(i, nn.Conv2d)][-1].out_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8d07b1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PIPNet(\n",
       "   (_net): ConvNeXt(\n",
       "     (features): Sequential(\n",
       "       (0): Conv2dNormActivation(\n",
       "         (0): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))\n",
       "         (1): LayerNorm2d((96,), eps=1e-06, elementwise_affine=True)\n",
       "       )\n",
       "       (1): Sequential(\n",
       "         (0): CNBlock(\n",
       "           (block): Sequential(\n",
       "             (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
       "             (1): Permute()\n",
       "             (2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
       "             (3): Linear(in_features=96, out_features=384, bias=True)\n",
       "             (4): GELU(approximate='none')\n",
       "             (5): Linear(in_features=384, out_features=96, bias=True)\n",
       "             (6): Permute()\n",
       "           )\n",
       "           (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
       "         )\n",
       "         (1): CNBlock(\n",
       "           (block): Sequential(\n",
       "             (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
       "             (1): Permute()\n",
       "             (2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
       "             (3): Linear(in_features=96, out_features=384, bias=True)\n",
       "             (4): GELU(approximate='none')\n",
       "             (5): Linear(in_features=384, out_features=96, bias=True)\n",
       "             (6): Permute()\n",
       "           )\n",
       "           (stochastic_depth): StochasticDepth(p=0.0058823529411764705, mode=row)\n",
       "         )\n",
       "         (2): CNBlock(\n",
       "           (block): Sequential(\n",
       "             (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
       "             (1): Permute()\n",
       "             (2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
       "             (3): Linear(in_features=96, out_features=384, bias=True)\n",
       "             (4): GELU(approximate='none')\n",
       "             (5): Linear(in_features=384, out_features=96, bias=True)\n",
       "             (6): Permute()\n",
       "           )\n",
       "           (stochastic_depth): StochasticDepth(p=0.011764705882352941, mode=row)\n",
       "         )\n",
       "       )\n",
       "       (2): Sequential(\n",
       "         (0): LayerNorm2d((96,), eps=1e-06, elementwise_affine=True)\n",
       "         (1): Conv2d(96, 192, kernel_size=(2, 2), stride=(2, 2))\n",
       "       )\n",
       "       (3): Sequential(\n",
       "         (0): CNBlock(\n",
       "           (block): Sequential(\n",
       "             (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
       "             (1): Permute()\n",
       "             (2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "             (3): Linear(in_features=192, out_features=768, bias=True)\n",
       "             (4): GELU(approximate='none')\n",
       "             (5): Linear(in_features=768, out_features=192, bias=True)\n",
       "             (6): Permute()\n",
       "           )\n",
       "           (stochastic_depth): StochasticDepth(p=0.017647058823529415, mode=row)\n",
       "         )\n",
       "         (1): CNBlock(\n",
       "           (block): Sequential(\n",
       "             (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
       "             (1): Permute()\n",
       "             (2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "             (3): Linear(in_features=192, out_features=768, bias=True)\n",
       "             (4): GELU(approximate='none')\n",
       "             (5): Linear(in_features=768, out_features=192, bias=True)\n",
       "             (6): Permute()\n",
       "           )\n",
       "           (stochastic_depth): StochasticDepth(p=0.023529411764705882, mode=row)\n",
       "         )\n",
       "         (2): CNBlock(\n",
       "           (block): Sequential(\n",
       "             (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
       "             (1): Permute()\n",
       "             (2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "             (3): Linear(in_features=192, out_features=768, bias=True)\n",
       "             (4): GELU(approximate='none')\n",
       "             (5): Linear(in_features=768, out_features=192, bias=True)\n",
       "             (6): Permute()\n",
       "           )\n",
       "           (stochastic_depth): StochasticDepth(p=0.029411764705882353, mode=row)\n",
       "         )\n",
       "       )\n",
       "       (4): Sequential(\n",
       "         (0): LayerNorm2d((192,), eps=1e-06, elementwise_affine=True)\n",
       "         (1): Conv2d(192, 384, kernel_size=(2, 2), stride=(1, 1))\n",
       "       )\n",
       "       (5): Sequential(\n",
       "         (0): CNBlock(\n",
       "           (block): Sequential(\n",
       "             (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "             (1): Permute()\n",
       "             (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "             (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "             (4): GELU(approximate='none')\n",
       "             (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "             (6): Permute()\n",
       "           )\n",
       "           (stochastic_depth): StochasticDepth(p=0.03529411764705883, mode=row)\n",
       "         )\n",
       "         (1): CNBlock(\n",
       "           (block): Sequential(\n",
       "             (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "             (1): Permute()\n",
       "             (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "             (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "             (4): GELU(approximate='none')\n",
       "             (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "             (6): Permute()\n",
       "           )\n",
       "           (stochastic_depth): StochasticDepth(p=0.0411764705882353, mode=row)\n",
       "         )\n",
       "         (2): CNBlock(\n",
       "           (block): Sequential(\n",
       "             (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "             (1): Permute()\n",
       "             (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "             (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "             (4): GELU(approximate='none')\n",
       "             (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "             (6): Permute()\n",
       "           )\n",
       "           (stochastic_depth): StochasticDepth(p=0.047058823529411764, mode=row)\n",
       "         )\n",
       "         (3): CNBlock(\n",
       "           (block): Sequential(\n",
       "             (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "             (1): Permute()\n",
       "             (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "             (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "             (4): GELU(approximate='none')\n",
       "             (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "             (6): Permute()\n",
       "           )\n",
       "           (stochastic_depth): StochasticDepth(p=0.052941176470588235, mode=row)\n",
       "         )\n",
       "         (4): CNBlock(\n",
       "           (block): Sequential(\n",
       "             (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "             (1): Permute()\n",
       "             (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "             (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "             (4): GELU(approximate='none')\n",
       "             (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "             (6): Permute()\n",
       "           )\n",
       "           (stochastic_depth): StochasticDepth(p=0.058823529411764705, mode=row)\n",
       "         )\n",
       "         (5): CNBlock(\n",
       "           (block): Sequential(\n",
       "             (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "             (1): Permute()\n",
       "             (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "             (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "             (4): GELU(approximate='none')\n",
       "             (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "             (6): Permute()\n",
       "           )\n",
       "           (stochastic_depth): StochasticDepth(p=0.06470588235294118, mode=row)\n",
       "         )\n",
       "         (6): CNBlock(\n",
       "           (block): Sequential(\n",
       "             (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "             (1): Permute()\n",
       "             (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "             (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "             (4): GELU(approximate='none')\n",
       "             (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "             (6): Permute()\n",
       "           )\n",
       "           (stochastic_depth): StochasticDepth(p=0.07058823529411766, mode=row)\n",
       "         )\n",
       "         (7): CNBlock(\n",
       "           (block): Sequential(\n",
       "             (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "             (1): Permute()\n",
       "             (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "             (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "             (4): GELU(approximate='none')\n",
       "             (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "             (6): Permute()\n",
       "           )\n",
       "           (stochastic_depth): StochasticDepth(p=0.07647058823529412, mode=row)\n",
       "         )\n",
       "         (8): CNBlock(\n",
       "           (block): Sequential(\n",
       "             (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "             (1): Permute()\n",
       "             (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "             (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "             (4): GELU(approximate='none')\n",
       "             (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "             (6): Permute()\n",
       "           )\n",
       "           (stochastic_depth): StochasticDepth(p=0.0823529411764706, mode=row)\n",
       "         )\n",
       "       )\n",
       "       (6): Sequential(\n",
       "         (0): LayerNorm2d((384,), eps=1e-06, elementwise_affine=True)\n",
       "         (1): Conv2d(384, 768, kernel_size=(2, 2), stride=(1, 1))\n",
       "       )\n",
       "       (7): Sequential(\n",
       "         (0): CNBlock(\n",
       "           (block): Sequential(\n",
       "             (0): BasicGaussianMultiplierConv2D(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "             (1): Permute()\n",
       "             (2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "             (3): Linear(in_features=768, out_features=3072, bias=True)\n",
       "             (4): GELU(approximate='none')\n",
       "             (5): Linear(in_features=3072, out_features=768, bias=True)\n",
       "             (6): Permute()\n",
       "           )\n",
       "           (stochastic_depth): StochasticDepth(p=0.08823529411764706, mode=row)\n",
       "         )\n",
       "         (1): CNBlock(\n",
       "           (block): Sequential(\n",
       "             (0): BasicGaussianMultiplierConv2D(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "             (1): Permute()\n",
       "             (2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "             (3): Linear(in_features=768, out_features=3072, bias=True)\n",
       "             (4): GELU(approximate='none')\n",
       "             (5): Linear(in_features=3072, out_features=768, bias=True)\n",
       "             (6): Permute()\n",
       "           )\n",
       "           (stochastic_depth): StochasticDepth(p=0.09411764705882353, mode=row)\n",
       "         )\n",
       "         (2): CNBlock(\n",
       "           (block): Sequential(\n",
       "             (0): BasicGaussianMultiplierConv2D(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "             (1): Permute()\n",
       "             (2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "             (3): Linear(in_features=768, out_features=3072, bias=True)\n",
       "             (4): GELU(approximate='none')\n",
       "             (5): Linear(in_features=3072, out_features=768, bias=True)\n",
       "             (6): Permute()\n",
       "           )\n",
       "           (stochastic_depth): StochasticDepth(p=0.1, mode=row)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (avgpool): Identity()\n",
       "     (classifier): Identity()\n",
       "   )\n",
       "   (_root_add_on): UnitConv2D(768, 20, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "   (_052+053_add_on): UnitConv2D(768, 20, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "   (_004+086_add_on): UnitConv2D(768, 20, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "   (_053+050_add_on): UnitConv2D(768, 20, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "   (_004+032_add_on): UnitConv2D(768, 20, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "   (_086+045_add_on): UnitConv2D(768, 20, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "   (_050+051_add_on): UnitConv2D(768, 20, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "   (_032+033_add_on): UnitConv2D(768, 20, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "   (_045+101_add_on): UnitConv2D(768, 20, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "   (_033+031_add_on): UnitConv2D(768, 20, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "   (_045+003_add_on): UnitConv2D(768, 20, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "   (_101+023_add_on): UnitConv2D(768, 20, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "   (_003+002_add_on): UnitConv2D(768, 20, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "   (_101+100_add_on): UnitConv2D(768, 20, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "   (_023+025_add_on): UnitConv2D(768, 20, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "   (_002+001_add_on): UnitConv2D(768, 20, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "   (_025+024_add_on): UnitConv2D(768, 20, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "   (_pool): Sequential(\n",
       "     (0): AdaptiveMaxPool2d(output_size=(1, 1))\n",
       "     (1): Flatten(start_dim=1, end_dim=-1)\n",
       "   )\n",
       "   (_avg_pool): Sequential(\n",
       "     (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "     (1): Flatten(start_dim=1, end_dim=-1)\n",
       "   )\n",
       "   (_root_classification): NonNegLinear()\n",
       "   (_052+053_classification): NonNegLinear()\n",
       "   (_004+086_classification): NonNegLinear()\n",
       "   (_053+050_classification): NonNegLinear()\n",
       "   (_004+032_classification): NonNegLinear()\n",
       "   (_086+045_classification): NonNegLinear()\n",
       "   (_050+051_classification): NonNegLinear()\n",
       "   (_032+033_classification): NonNegLinear()\n",
       "   (_045+101_classification): NonNegLinear()\n",
       "   (_033+031_classification): NonNegLinear()\n",
       "   (_045+003_classification): NonNegLinear()\n",
       "   (_101+023_classification): NonNegLinear()\n",
       "   (_003+002_classification): NonNegLinear()\n",
       "   (_101+100_classification): NonNegLinear()\n",
       "   (_023+025_classification): NonNegLinear()\n",
       "   (_002+001_classification): NonNegLinear()\n",
       "   (_025+024_classification): NonNegLinear()\n",
       "   (_softmax): Softmax(dim=1)\n",
       " ),\n",
       " ConvNeXt(\n",
       "   (features): Sequential(\n",
       "     (0): Conv2dNormActivation(\n",
       "       (0): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))\n",
       "       (1): LayerNorm2d((96,), eps=1e-06, elementwise_affine=True)\n",
       "     )\n",
       "     (1): Sequential(\n",
       "       (0): CNBlock(\n",
       "         (block): Sequential(\n",
       "           (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
       "           (1): Permute()\n",
       "           (2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
       "           (3): Linear(in_features=96, out_features=384, bias=True)\n",
       "           (4): GELU(approximate='none')\n",
       "           (5): Linear(in_features=384, out_features=96, bias=True)\n",
       "           (6): Permute()\n",
       "         )\n",
       "         (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
       "       )\n",
       "       (1): CNBlock(\n",
       "         (block): Sequential(\n",
       "           (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
       "           (1): Permute()\n",
       "           (2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
       "           (3): Linear(in_features=96, out_features=384, bias=True)\n",
       "           (4): GELU(approximate='none')\n",
       "           (5): Linear(in_features=384, out_features=96, bias=True)\n",
       "           (6): Permute()\n",
       "         )\n",
       "         (stochastic_depth): StochasticDepth(p=0.0058823529411764705, mode=row)\n",
       "       )\n",
       "       (2): CNBlock(\n",
       "         (block): Sequential(\n",
       "           (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
       "           (1): Permute()\n",
       "           (2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
       "           (3): Linear(in_features=96, out_features=384, bias=True)\n",
       "           (4): GELU(approximate='none')\n",
       "           (5): Linear(in_features=384, out_features=96, bias=True)\n",
       "           (6): Permute()\n",
       "         )\n",
       "         (stochastic_depth): StochasticDepth(p=0.011764705882352941, mode=row)\n",
       "       )\n",
       "     )\n",
       "     (2): Sequential(\n",
       "       (0): LayerNorm2d((96,), eps=1e-06, elementwise_affine=True)\n",
       "       (1): Conv2d(96, 192, kernel_size=(2, 2), stride=(2, 2))\n",
       "     )\n",
       "     (3): Sequential(\n",
       "       (0): CNBlock(\n",
       "         (block): Sequential(\n",
       "           (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
       "           (1): Permute()\n",
       "           (2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "           (3): Linear(in_features=192, out_features=768, bias=True)\n",
       "           (4): GELU(approximate='none')\n",
       "           (5): Linear(in_features=768, out_features=192, bias=True)\n",
       "           (6): Permute()\n",
       "         )\n",
       "         (stochastic_depth): StochasticDepth(p=0.017647058823529415, mode=row)\n",
       "       )\n",
       "       (1): CNBlock(\n",
       "         (block): Sequential(\n",
       "           (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
       "           (1): Permute()\n",
       "           (2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "           (3): Linear(in_features=192, out_features=768, bias=True)\n",
       "           (4): GELU(approximate='none')\n",
       "           (5): Linear(in_features=768, out_features=192, bias=True)\n",
       "           (6): Permute()\n",
       "         )\n",
       "         (stochastic_depth): StochasticDepth(p=0.023529411764705882, mode=row)\n",
       "       )\n",
       "       (2): CNBlock(\n",
       "         (block): Sequential(\n",
       "           (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
       "           (1): Permute()\n",
       "           (2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "           (3): Linear(in_features=192, out_features=768, bias=True)\n",
       "           (4): GELU(approximate='none')\n",
       "           (5): Linear(in_features=768, out_features=192, bias=True)\n",
       "           (6): Permute()\n",
       "         )\n",
       "         (stochastic_depth): StochasticDepth(p=0.029411764705882353, mode=row)\n",
       "       )\n",
       "     )\n",
       "     (4): Sequential(\n",
       "       (0): LayerNorm2d((192,), eps=1e-06, elementwise_affine=True)\n",
       "       (1): Conv2d(192, 384, kernel_size=(2, 2), stride=(1, 1))\n",
       "     )\n",
       "     (5): Sequential(\n",
       "       (0): CNBlock(\n",
       "         (block): Sequential(\n",
       "           (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "           (1): Permute()\n",
       "           (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "           (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "           (4): GELU(approximate='none')\n",
       "           (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "           (6): Permute()\n",
       "         )\n",
       "         (stochastic_depth): StochasticDepth(p=0.03529411764705883, mode=row)\n",
       "       )\n",
       "       (1): CNBlock(\n",
       "         (block): Sequential(\n",
       "           (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "           (1): Permute()\n",
       "           (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "           (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "           (4): GELU(approximate='none')\n",
       "           (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "           (6): Permute()\n",
       "         )\n",
       "         (stochastic_depth): StochasticDepth(p=0.0411764705882353, mode=row)\n",
       "       )\n",
       "       (2): CNBlock(\n",
       "         (block): Sequential(\n",
       "           (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "           (1): Permute()\n",
       "           (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "           (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "           (4): GELU(approximate='none')\n",
       "           (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "           (6): Permute()\n",
       "         )\n",
       "         (stochastic_depth): StochasticDepth(p=0.047058823529411764, mode=row)\n",
       "       )\n",
       "       (3): CNBlock(\n",
       "         (block): Sequential(\n",
       "           (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "           (1): Permute()\n",
       "           (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "           (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "           (4): GELU(approximate='none')\n",
       "           (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "           (6): Permute()\n",
       "         )\n",
       "         (stochastic_depth): StochasticDepth(p=0.052941176470588235, mode=row)\n",
       "       )\n",
       "       (4): CNBlock(\n",
       "         (block): Sequential(\n",
       "           (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "           (1): Permute()\n",
       "           (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "           (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "           (4): GELU(approximate='none')\n",
       "           (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "           (6): Permute()\n",
       "         )\n",
       "         (stochastic_depth): StochasticDepth(p=0.058823529411764705, mode=row)\n",
       "       )\n",
       "       (5): CNBlock(\n",
       "         (block): Sequential(\n",
       "           (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "           (1): Permute()\n",
       "           (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "           (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "           (4): GELU(approximate='none')\n",
       "           (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "           (6): Permute()\n",
       "         )\n",
       "         (stochastic_depth): StochasticDepth(p=0.06470588235294118, mode=row)\n",
       "       )\n",
       "       (6): CNBlock(\n",
       "         (block): Sequential(\n",
       "           (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "           (1): Permute()\n",
       "           (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "           (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "           (4): GELU(approximate='none')\n",
       "           (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "           (6): Permute()\n",
       "         )\n",
       "         (stochastic_depth): StochasticDepth(p=0.07058823529411766, mode=row)\n",
       "       )\n",
       "       (7): CNBlock(\n",
       "         (block): Sequential(\n",
       "           (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "           (1): Permute()\n",
       "           (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "           (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "           (4): GELU(approximate='none')\n",
       "           (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "           (6): Permute()\n",
       "         )\n",
       "         (stochastic_depth): StochasticDepth(p=0.07647058823529412, mode=row)\n",
       "       )\n",
       "       (8): CNBlock(\n",
       "         (block): Sequential(\n",
       "           (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "           (1): Permute()\n",
       "           (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "           (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "           (4): GELU(approximate='none')\n",
       "           (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "           (6): Permute()\n",
       "         )\n",
       "         (stochastic_depth): StochasticDepth(p=0.0823529411764706, mode=row)\n",
       "       )\n",
       "     )\n",
       "     (6): Sequential(\n",
       "       (0): LayerNorm2d((384,), eps=1e-06, elementwise_affine=True)\n",
       "       (1): Conv2d(384, 768, kernel_size=(2, 2), stride=(1, 1))\n",
       "     )\n",
       "     (7): Sequential(\n",
       "       (0): CNBlock(\n",
       "         (block): Sequential(\n",
       "           (0): BasicGaussianMultiplierConv2D(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "           (1): Permute()\n",
       "           (2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "           (3): Linear(in_features=768, out_features=3072, bias=True)\n",
       "           (4): GELU(approximate='none')\n",
       "           (5): Linear(in_features=3072, out_features=768, bias=True)\n",
       "           (6): Permute()\n",
       "         )\n",
       "         (stochastic_depth): StochasticDepth(p=0.08823529411764706, mode=row)\n",
       "       )\n",
       "       (1): CNBlock(\n",
       "         (block): Sequential(\n",
       "           (0): BasicGaussianMultiplierConv2D(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "           (1): Permute()\n",
       "           (2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "           (3): Linear(in_features=768, out_features=3072, bias=True)\n",
       "           (4): GELU(approximate='none')\n",
       "           (5): Linear(in_features=3072, out_features=768, bias=True)\n",
       "           (6): Permute()\n",
       "         )\n",
       "         (stochastic_depth): StochasticDepth(p=0.09411764705882353, mode=row)\n",
       "       )\n",
       "       (2): CNBlock(\n",
       "         (block): Sequential(\n",
       "           (0): BasicGaussianMultiplierConv2D(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "           (1): Permute()\n",
       "           (2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "           (3): Linear(in_features=768, out_features=3072, bias=True)\n",
       "           (4): GELU(approximate='none')\n",
       "           (5): Linear(in_features=3072, out_features=768, bias=True)\n",
       "           (6): Permute()\n",
       "         )\n",
       "         (stochastic_depth): StochasticDepth(p=0.1, mode=row)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (avgpool): Identity()\n",
       "   (classifier): Identity()\n",
       " ),\n",
       " Sequential(\n",
       "   (0): Conv2dNormActivation(\n",
       "     (0): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))\n",
       "     (1): LayerNorm2d((96,), eps=1e-06, elementwise_affine=True)\n",
       "   )\n",
       "   (1): Sequential(\n",
       "     (0): CNBlock(\n",
       "       (block): Sequential(\n",
       "         (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
       "         (1): Permute()\n",
       "         (2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
       "         (3): Linear(in_features=96, out_features=384, bias=True)\n",
       "         (4): GELU(approximate='none')\n",
       "         (5): Linear(in_features=384, out_features=96, bias=True)\n",
       "         (6): Permute()\n",
       "       )\n",
       "       (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
       "     )\n",
       "     (1): CNBlock(\n",
       "       (block): Sequential(\n",
       "         (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
       "         (1): Permute()\n",
       "         (2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
       "         (3): Linear(in_features=96, out_features=384, bias=True)\n",
       "         (4): GELU(approximate='none')\n",
       "         (5): Linear(in_features=384, out_features=96, bias=True)\n",
       "         (6): Permute()\n",
       "       )\n",
       "       (stochastic_depth): StochasticDepth(p=0.0058823529411764705, mode=row)\n",
       "     )\n",
       "     (2): CNBlock(\n",
       "       (block): Sequential(\n",
       "         (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
       "         (1): Permute()\n",
       "         (2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
       "         (3): Linear(in_features=96, out_features=384, bias=True)\n",
       "         (4): GELU(approximate='none')\n",
       "         (5): Linear(in_features=384, out_features=96, bias=True)\n",
       "         (6): Permute()\n",
       "       )\n",
       "       (stochastic_depth): StochasticDepth(p=0.011764705882352941, mode=row)\n",
       "     )\n",
       "   )\n",
       "   (2): Sequential(\n",
       "     (0): LayerNorm2d((96,), eps=1e-06, elementwise_affine=True)\n",
       "     (1): Conv2d(96, 192, kernel_size=(2, 2), stride=(2, 2))\n",
       "   )\n",
       "   (3): Sequential(\n",
       "     (0): CNBlock(\n",
       "       (block): Sequential(\n",
       "         (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
       "         (1): Permute()\n",
       "         (2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "         (3): Linear(in_features=192, out_features=768, bias=True)\n",
       "         (4): GELU(approximate='none')\n",
       "         (5): Linear(in_features=768, out_features=192, bias=True)\n",
       "         (6): Permute()\n",
       "       )\n",
       "       (stochastic_depth): StochasticDepth(p=0.017647058823529415, mode=row)\n",
       "     )\n",
       "     (1): CNBlock(\n",
       "       (block): Sequential(\n",
       "         (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
       "         (1): Permute()\n",
       "         (2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "         (3): Linear(in_features=192, out_features=768, bias=True)\n",
       "         (4): GELU(approximate='none')\n",
       "         (5): Linear(in_features=768, out_features=192, bias=True)\n",
       "         (6): Permute()\n",
       "       )\n",
       "       (stochastic_depth): StochasticDepth(p=0.023529411764705882, mode=row)\n",
       "     )\n",
       "     (2): CNBlock(\n",
       "       (block): Sequential(\n",
       "         (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
       "         (1): Permute()\n",
       "         (2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "         (3): Linear(in_features=192, out_features=768, bias=True)\n",
       "         (4): GELU(approximate='none')\n",
       "         (5): Linear(in_features=768, out_features=192, bias=True)\n",
       "         (6): Permute()\n",
       "       )\n",
       "       (stochastic_depth): StochasticDepth(p=0.029411764705882353, mode=row)\n",
       "     )\n",
       "   )\n",
       "   (4): Sequential(\n",
       "     (0): LayerNorm2d((192,), eps=1e-06, elementwise_affine=True)\n",
       "     (1): Conv2d(192, 384, kernel_size=(2, 2), stride=(1, 1))\n",
       "   )\n",
       "   (5): Sequential(\n",
       "     (0): CNBlock(\n",
       "       (block): Sequential(\n",
       "         (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "         (1): Permute()\n",
       "         (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "         (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "         (4): GELU(approximate='none')\n",
       "         (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "         (6): Permute()\n",
       "       )\n",
       "       (stochastic_depth): StochasticDepth(p=0.03529411764705883, mode=row)\n",
       "     )\n",
       "     (1): CNBlock(\n",
       "       (block): Sequential(\n",
       "         (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "         (1): Permute()\n",
       "         (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "         (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "         (4): GELU(approximate='none')\n",
       "         (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "         (6): Permute()\n",
       "       )\n",
       "       (stochastic_depth): StochasticDepth(p=0.0411764705882353, mode=row)\n",
       "     )\n",
       "     (2): CNBlock(\n",
       "       (block): Sequential(\n",
       "         (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "         (1): Permute()\n",
       "         (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "         (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "         (4): GELU(approximate='none')\n",
       "         (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "         (6): Permute()\n",
       "       )\n",
       "       (stochastic_depth): StochasticDepth(p=0.047058823529411764, mode=row)\n",
       "     )\n",
       "     (3): CNBlock(\n",
       "       (block): Sequential(\n",
       "         (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "         (1): Permute()\n",
       "         (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "         (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "         (4): GELU(approximate='none')\n",
       "         (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "         (6): Permute()\n",
       "       )\n",
       "       (stochastic_depth): StochasticDepth(p=0.052941176470588235, mode=row)\n",
       "     )\n",
       "     (4): CNBlock(\n",
       "       (block): Sequential(\n",
       "         (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "         (1): Permute()\n",
       "         (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "         (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "         (4): GELU(approximate='none')\n",
       "         (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "         (6): Permute()\n",
       "       )\n",
       "       (stochastic_depth): StochasticDepth(p=0.058823529411764705, mode=row)\n",
       "     )\n",
       "     (5): CNBlock(\n",
       "       (block): Sequential(\n",
       "         (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "         (1): Permute()\n",
       "         (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "         (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "         (4): GELU(approximate='none')\n",
       "         (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "         (6): Permute()\n",
       "       )\n",
       "       (stochastic_depth): StochasticDepth(p=0.06470588235294118, mode=row)\n",
       "     )\n",
       "     (6): CNBlock(\n",
       "       (block): Sequential(\n",
       "         (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "         (1): Permute()\n",
       "         (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "         (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "         (4): GELU(approximate='none')\n",
       "         (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "         (6): Permute()\n",
       "       )\n",
       "       (stochastic_depth): StochasticDepth(p=0.07058823529411766, mode=row)\n",
       "     )\n",
       "     (7): CNBlock(\n",
       "       (block): Sequential(\n",
       "         (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "         (1): Permute()\n",
       "         (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "         (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "         (4): GELU(approximate='none')\n",
       "         (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "         (6): Permute()\n",
       "       )\n",
       "       (stochastic_depth): StochasticDepth(p=0.07647058823529412, mode=row)\n",
       "     )\n",
       "     (8): CNBlock(\n",
       "       (block): Sequential(\n",
       "         (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "         (1): Permute()\n",
       "         (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "         (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "         (4): GELU(approximate='none')\n",
       "         (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "         (6): Permute()\n",
       "       )\n",
       "       (stochastic_depth): StochasticDepth(p=0.0823529411764706, mode=row)\n",
       "     )\n",
       "   )\n",
       "   (6): Sequential(\n",
       "     (0): LayerNorm2d((384,), eps=1e-06, elementwise_affine=True)\n",
       "     (1): Conv2d(384, 768, kernel_size=(2, 2), stride=(1, 1))\n",
       "   )\n",
       "   (7): Sequential(\n",
       "     (0): CNBlock(\n",
       "       (block): Sequential(\n",
       "         (0): BasicGaussianMultiplierConv2D(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "         (1): Permute()\n",
       "         (2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "         (3): Linear(in_features=768, out_features=3072, bias=True)\n",
       "         (4): GELU(approximate='none')\n",
       "         (5): Linear(in_features=3072, out_features=768, bias=True)\n",
       "         (6): Permute()\n",
       "       )\n",
       "       (stochastic_depth): StochasticDepth(p=0.08823529411764706, mode=row)\n",
       "     )\n",
       "     (1): CNBlock(\n",
       "       (block): Sequential(\n",
       "         (0): BasicGaussianMultiplierConv2D(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "         (1): Permute()\n",
       "         (2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "         (3): Linear(in_features=768, out_features=3072, bias=True)\n",
       "         (4): GELU(approximate='none')\n",
       "         (5): Linear(in_features=3072, out_features=768, bias=True)\n",
       "         (6): Permute()\n",
       "       )\n",
       "       (stochastic_depth): StochasticDepth(p=0.09411764705882353, mode=row)\n",
       "     )\n",
       "     (2): CNBlock(\n",
       "       (block): Sequential(\n",
       "         (0): BasicGaussianMultiplierConv2D(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "         (1): Permute()\n",
       "         (2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "         (3): Linear(in_features=768, out_features=3072, bias=True)\n",
       "         (4): GELU(approximate='none')\n",
       "         (5): Linear(in_features=3072, out_features=768, bias=True)\n",
       "         (6): Permute()\n",
       "       )\n",
       "       (stochastic_depth): StochasticDepth(p=0.1, mode=row)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " Conv2dNormActivation(\n",
       "   (0): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))\n",
       "   (1): LayerNorm2d((96,), eps=1e-06, elementwise_affine=True)\n",
       " ),\n",
       " Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4)),\n",
       " LayerNorm2d((96,), eps=1e-06, elementwise_affine=True),\n",
       " Sequential(\n",
       "   (0): CNBlock(\n",
       "     (block): Sequential(\n",
       "       (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
       "       (1): Permute()\n",
       "       (2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
       "       (3): Linear(in_features=96, out_features=384, bias=True)\n",
       "       (4): GELU(approximate='none')\n",
       "       (5): Linear(in_features=384, out_features=96, bias=True)\n",
       "       (6): Permute()\n",
       "     )\n",
       "     (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
       "   )\n",
       "   (1): CNBlock(\n",
       "     (block): Sequential(\n",
       "       (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
       "       (1): Permute()\n",
       "       (2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
       "       (3): Linear(in_features=96, out_features=384, bias=True)\n",
       "       (4): GELU(approximate='none')\n",
       "       (5): Linear(in_features=384, out_features=96, bias=True)\n",
       "       (6): Permute()\n",
       "     )\n",
       "     (stochastic_depth): StochasticDepth(p=0.0058823529411764705, mode=row)\n",
       "   )\n",
       "   (2): CNBlock(\n",
       "     (block): Sequential(\n",
       "       (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
       "       (1): Permute()\n",
       "       (2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
       "       (3): Linear(in_features=96, out_features=384, bias=True)\n",
       "       (4): GELU(approximate='none')\n",
       "       (5): Linear(in_features=384, out_features=96, bias=True)\n",
       "       (6): Permute()\n",
       "     )\n",
       "     (stochastic_depth): StochasticDepth(p=0.011764705882352941, mode=row)\n",
       "   )\n",
       " ),\n",
       " CNBlock(\n",
       "   (block): Sequential(\n",
       "     (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
       "     (1): Permute()\n",
       "     (2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
       "     (3): Linear(in_features=96, out_features=384, bias=True)\n",
       "     (4): GELU(approximate='none')\n",
       "     (5): Linear(in_features=384, out_features=96, bias=True)\n",
       "     (6): Permute()\n",
       "   )\n",
       "   (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
       " ),\n",
       " Sequential(\n",
       "   (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
       "   (1): Permute()\n",
       "   (2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
       "   (3): Linear(in_features=96, out_features=384, bias=True)\n",
       "   (4): GELU(approximate='none')\n",
       "   (5): Linear(in_features=384, out_features=96, bias=True)\n",
       "   (6): Permute()\n",
       " ),\n",
       " Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96),\n",
       " Permute(),\n",
       " LayerNorm((96,), eps=1e-06, elementwise_affine=True),\n",
       " Linear(in_features=96, out_features=384, bias=True),\n",
       " GELU(approximate='none'),\n",
       " Linear(in_features=384, out_features=96, bias=True),\n",
       " Permute(),\n",
       " StochasticDepth(p=0.0, mode=row),\n",
       " CNBlock(\n",
       "   (block): Sequential(\n",
       "     (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
       "     (1): Permute()\n",
       "     (2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
       "     (3): Linear(in_features=96, out_features=384, bias=True)\n",
       "     (4): GELU(approximate='none')\n",
       "     (5): Linear(in_features=384, out_features=96, bias=True)\n",
       "     (6): Permute()\n",
       "   )\n",
       "   (stochastic_depth): StochasticDepth(p=0.0058823529411764705, mode=row)\n",
       " ),\n",
       " Sequential(\n",
       "   (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
       "   (1): Permute()\n",
       "   (2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
       "   (3): Linear(in_features=96, out_features=384, bias=True)\n",
       "   (4): GELU(approximate='none')\n",
       "   (5): Linear(in_features=384, out_features=96, bias=True)\n",
       "   (6): Permute()\n",
       " ),\n",
       " Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96),\n",
       " Permute(),\n",
       " LayerNorm((96,), eps=1e-06, elementwise_affine=True),\n",
       " Linear(in_features=96, out_features=384, bias=True),\n",
       " GELU(approximate='none'),\n",
       " Linear(in_features=384, out_features=96, bias=True),\n",
       " Permute(),\n",
       " StochasticDepth(p=0.0058823529411764705, mode=row),\n",
       " CNBlock(\n",
       "   (block): Sequential(\n",
       "     (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
       "     (1): Permute()\n",
       "     (2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
       "     (3): Linear(in_features=96, out_features=384, bias=True)\n",
       "     (4): GELU(approximate='none')\n",
       "     (5): Linear(in_features=384, out_features=96, bias=True)\n",
       "     (6): Permute()\n",
       "   )\n",
       "   (stochastic_depth): StochasticDepth(p=0.011764705882352941, mode=row)\n",
       " ),\n",
       " Sequential(\n",
       "   (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
       "   (1): Permute()\n",
       "   (2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
       "   (3): Linear(in_features=96, out_features=384, bias=True)\n",
       "   (4): GELU(approximate='none')\n",
       "   (5): Linear(in_features=384, out_features=96, bias=True)\n",
       "   (6): Permute()\n",
       " ),\n",
       " Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96),\n",
       " Permute(),\n",
       " LayerNorm((96,), eps=1e-06, elementwise_affine=True),\n",
       " Linear(in_features=96, out_features=384, bias=True),\n",
       " GELU(approximate='none'),\n",
       " Linear(in_features=384, out_features=96, bias=True),\n",
       " Permute(),\n",
       " StochasticDepth(p=0.011764705882352941, mode=row),\n",
       " Sequential(\n",
       "   (0): LayerNorm2d((96,), eps=1e-06, elementwise_affine=True)\n",
       "   (1): Conv2d(96, 192, kernel_size=(2, 2), stride=(2, 2))\n",
       " ),\n",
       " LayerNorm2d((96,), eps=1e-06, elementwise_affine=True),\n",
       " Conv2d(96, 192, kernel_size=(2, 2), stride=(2, 2)),\n",
       " Sequential(\n",
       "   (0): CNBlock(\n",
       "     (block): Sequential(\n",
       "       (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
       "       (1): Permute()\n",
       "       (2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "       (3): Linear(in_features=192, out_features=768, bias=True)\n",
       "       (4): GELU(approximate='none')\n",
       "       (5): Linear(in_features=768, out_features=192, bias=True)\n",
       "       (6): Permute()\n",
       "     )\n",
       "     (stochastic_depth): StochasticDepth(p=0.017647058823529415, mode=row)\n",
       "   )\n",
       "   (1): CNBlock(\n",
       "     (block): Sequential(\n",
       "       (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
       "       (1): Permute()\n",
       "       (2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "       (3): Linear(in_features=192, out_features=768, bias=True)\n",
       "       (4): GELU(approximate='none')\n",
       "       (5): Linear(in_features=768, out_features=192, bias=True)\n",
       "       (6): Permute()\n",
       "     )\n",
       "     (stochastic_depth): StochasticDepth(p=0.023529411764705882, mode=row)\n",
       "   )\n",
       "   (2): CNBlock(\n",
       "     (block): Sequential(\n",
       "       (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
       "       (1): Permute()\n",
       "       (2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "       (3): Linear(in_features=192, out_features=768, bias=True)\n",
       "       (4): GELU(approximate='none')\n",
       "       (5): Linear(in_features=768, out_features=192, bias=True)\n",
       "       (6): Permute()\n",
       "     )\n",
       "     (stochastic_depth): StochasticDepth(p=0.029411764705882353, mode=row)\n",
       "   )\n",
       " ),\n",
       " CNBlock(\n",
       "   (block): Sequential(\n",
       "     (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
       "     (1): Permute()\n",
       "     (2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "     (3): Linear(in_features=192, out_features=768, bias=True)\n",
       "     (4): GELU(approximate='none')\n",
       "     (5): Linear(in_features=768, out_features=192, bias=True)\n",
       "     (6): Permute()\n",
       "   )\n",
       "   (stochastic_depth): StochasticDepth(p=0.017647058823529415, mode=row)\n",
       " ),\n",
       " Sequential(\n",
       "   (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
       "   (1): Permute()\n",
       "   (2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "   (3): Linear(in_features=192, out_features=768, bias=True)\n",
       "   (4): GELU(approximate='none')\n",
       "   (5): Linear(in_features=768, out_features=192, bias=True)\n",
       "   (6): Permute()\n",
       " ),\n",
       " Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192),\n",
       " Permute(),\n",
       " LayerNorm((192,), eps=1e-06, elementwise_affine=True),\n",
       " Linear(in_features=192, out_features=768, bias=True),\n",
       " GELU(approximate='none'),\n",
       " Linear(in_features=768, out_features=192, bias=True),\n",
       " Permute(),\n",
       " StochasticDepth(p=0.017647058823529415, mode=row),\n",
       " CNBlock(\n",
       "   (block): Sequential(\n",
       "     (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
       "     (1): Permute()\n",
       "     (2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "     (3): Linear(in_features=192, out_features=768, bias=True)\n",
       "     (4): GELU(approximate='none')\n",
       "     (5): Linear(in_features=768, out_features=192, bias=True)\n",
       "     (6): Permute()\n",
       "   )\n",
       "   (stochastic_depth): StochasticDepth(p=0.023529411764705882, mode=row)\n",
       " ),\n",
       " Sequential(\n",
       "   (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
       "   (1): Permute()\n",
       "   (2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "   (3): Linear(in_features=192, out_features=768, bias=True)\n",
       "   (4): GELU(approximate='none')\n",
       "   (5): Linear(in_features=768, out_features=192, bias=True)\n",
       "   (6): Permute()\n",
       " ),\n",
       " Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192),\n",
       " Permute(),\n",
       " LayerNorm((192,), eps=1e-06, elementwise_affine=True),\n",
       " Linear(in_features=192, out_features=768, bias=True),\n",
       " GELU(approximate='none'),\n",
       " Linear(in_features=768, out_features=192, bias=True),\n",
       " Permute(),\n",
       " StochasticDepth(p=0.023529411764705882, mode=row),\n",
       " CNBlock(\n",
       "   (block): Sequential(\n",
       "     (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
       "     (1): Permute()\n",
       "     (2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "     (3): Linear(in_features=192, out_features=768, bias=True)\n",
       "     (4): GELU(approximate='none')\n",
       "     (5): Linear(in_features=768, out_features=192, bias=True)\n",
       "     (6): Permute()\n",
       "   )\n",
       "   (stochastic_depth): StochasticDepth(p=0.029411764705882353, mode=row)\n",
       " ),\n",
       " Sequential(\n",
       "   (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
       "   (1): Permute()\n",
       "   (2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "   (3): Linear(in_features=192, out_features=768, bias=True)\n",
       "   (4): GELU(approximate='none')\n",
       "   (5): Linear(in_features=768, out_features=192, bias=True)\n",
       "   (6): Permute()\n",
       " ),\n",
       " Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192),\n",
       " Permute(),\n",
       " LayerNorm((192,), eps=1e-06, elementwise_affine=True),\n",
       " Linear(in_features=192, out_features=768, bias=True),\n",
       " GELU(approximate='none'),\n",
       " Linear(in_features=768, out_features=192, bias=True),\n",
       " Permute(),\n",
       " StochasticDepth(p=0.029411764705882353, mode=row),\n",
       " Sequential(\n",
       "   (0): LayerNorm2d((192,), eps=1e-06, elementwise_affine=True)\n",
       "   (1): Conv2d(192, 384, kernel_size=(2, 2), stride=(1, 1))\n",
       " ),\n",
       " LayerNorm2d((192,), eps=1e-06, elementwise_affine=True),\n",
       " Conv2d(192, 384, kernel_size=(2, 2), stride=(1, 1)),\n",
       " Sequential(\n",
       "   (0): CNBlock(\n",
       "     (block): Sequential(\n",
       "       (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "       (1): Permute()\n",
       "       (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "       (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "       (4): GELU(approximate='none')\n",
       "       (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "       (6): Permute()\n",
       "     )\n",
       "     (stochastic_depth): StochasticDepth(p=0.03529411764705883, mode=row)\n",
       "   )\n",
       "   (1): CNBlock(\n",
       "     (block): Sequential(\n",
       "       (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "       (1): Permute()\n",
       "       (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "       (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "       (4): GELU(approximate='none')\n",
       "       (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "       (6): Permute()\n",
       "     )\n",
       "     (stochastic_depth): StochasticDepth(p=0.0411764705882353, mode=row)\n",
       "   )\n",
       "   (2): CNBlock(\n",
       "     (block): Sequential(\n",
       "       (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "       (1): Permute()\n",
       "       (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "       (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "       (4): GELU(approximate='none')\n",
       "       (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "       (6): Permute()\n",
       "     )\n",
       "     (stochastic_depth): StochasticDepth(p=0.047058823529411764, mode=row)\n",
       "   )\n",
       "   (3): CNBlock(\n",
       "     (block): Sequential(\n",
       "       (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "       (1): Permute()\n",
       "       (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "       (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "       (4): GELU(approximate='none')\n",
       "       (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "       (6): Permute()\n",
       "     )\n",
       "     (stochastic_depth): StochasticDepth(p=0.052941176470588235, mode=row)\n",
       "   )\n",
       "   (4): CNBlock(\n",
       "     (block): Sequential(\n",
       "       (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "       (1): Permute()\n",
       "       (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "       (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "       (4): GELU(approximate='none')\n",
       "       (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "       (6): Permute()\n",
       "     )\n",
       "     (stochastic_depth): StochasticDepth(p=0.058823529411764705, mode=row)\n",
       "   )\n",
       "   (5): CNBlock(\n",
       "     (block): Sequential(\n",
       "       (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "       (1): Permute()\n",
       "       (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "       (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "       (4): GELU(approximate='none')\n",
       "       (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "       (6): Permute()\n",
       "     )\n",
       "     (stochastic_depth): StochasticDepth(p=0.06470588235294118, mode=row)\n",
       "   )\n",
       "   (6): CNBlock(\n",
       "     (block): Sequential(\n",
       "       (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "       (1): Permute()\n",
       "       (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "       (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "       (4): GELU(approximate='none')\n",
       "       (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "       (6): Permute()\n",
       "     )\n",
       "     (stochastic_depth): StochasticDepth(p=0.07058823529411766, mode=row)\n",
       "   )\n",
       "   (7): CNBlock(\n",
       "     (block): Sequential(\n",
       "       (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "       (1): Permute()\n",
       "       (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "       (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "       (4): GELU(approximate='none')\n",
       "       (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "       (6): Permute()\n",
       "     )\n",
       "     (stochastic_depth): StochasticDepth(p=0.07647058823529412, mode=row)\n",
       "   )\n",
       "   (8): CNBlock(\n",
       "     (block): Sequential(\n",
       "       (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "       (1): Permute()\n",
       "       (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "       (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "       (4): GELU(approximate='none')\n",
       "       (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "       (6): Permute()\n",
       "     )\n",
       "     (stochastic_depth): StochasticDepth(p=0.0823529411764706, mode=row)\n",
       "   )\n",
       " ),\n",
       " CNBlock(\n",
       "   (block): Sequential(\n",
       "     (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "     (1): Permute()\n",
       "     (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "     (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "     (4): GELU(approximate='none')\n",
       "     (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "     (6): Permute()\n",
       "   )\n",
       "   (stochastic_depth): StochasticDepth(p=0.03529411764705883, mode=row)\n",
       " ),\n",
       " Sequential(\n",
       "   (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "   (1): Permute()\n",
       "   (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "   (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "   (4): GELU(approximate='none')\n",
       "   (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "   (6): Permute()\n",
       " ),\n",
       " Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384),\n",
       " Permute(),\n",
       " LayerNorm((384,), eps=1e-06, elementwise_affine=True),\n",
       " Linear(in_features=384, out_features=1536, bias=True),\n",
       " GELU(approximate='none'),\n",
       " Linear(in_features=1536, out_features=384, bias=True),\n",
       " Permute(),\n",
       " StochasticDepth(p=0.03529411764705883, mode=row),\n",
       " CNBlock(\n",
       "   (block): Sequential(\n",
       "     (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "     (1): Permute()\n",
       "     (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "     (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "     (4): GELU(approximate='none')\n",
       "     (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "     (6): Permute()\n",
       "   )\n",
       "   (stochastic_depth): StochasticDepth(p=0.0411764705882353, mode=row)\n",
       " ),\n",
       " Sequential(\n",
       "   (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "   (1): Permute()\n",
       "   (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "   (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "   (4): GELU(approximate='none')\n",
       "   (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "   (6): Permute()\n",
       " ),\n",
       " Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384),\n",
       " Permute(),\n",
       " LayerNorm((384,), eps=1e-06, elementwise_affine=True),\n",
       " Linear(in_features=384, out_features=1536, bias=True),\n",
       " GELU(approximate='none'),\n",
       " Linear(in_features=1536, out_features=384, bias=True),\n",
       " Permute(),\n",
       " StochasticDepth(p=0.0411764705882353, mode=row),\n",
       " CNBlock(\n",
       "   (block): Sequential(\n",
       "     (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "     (1): Permute()\n",
       "     (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "     (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "     (4): GELU(approximate='none')\n",
       "     (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "     (6): Permute()\n",
       "   )\n",
       "   (stochastic_depth): StochasticDepth(p=0.047058823529411764, mode=row)\n",
       " ),\n",
       " Sequential(\n",
       "   (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "   (1): Permute()\n",
       "   (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "   (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "   (4): GELU(approximate='none')\n",
       "   (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "   (6): Permute()\n",
       " ),\n",
       " Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384),\n",
       " Permute(),\n",
       " LayerNorm((384,), eps=1e-06, elementwise_affine=True),\n",
       " Linear(in_features=384, out_features=1536, bias=True),\n",
       " GELU(approximate='none'),\n",
       " Linear(in_features=1536, out_features=384, bias=True),\n",
       " Permute(),\n",
       " StochasticDepth(p=0.047058823529411764, mode=row),\n",
       " CNBlock(\n",
       "   (block): Sequential(\n",
       "     (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "     (1): Permute()\n",
       "     (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "     (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "     (4): GELU(approximate='none')\n",
       "     (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "     (6): Permute()\n",
       "   )\n",
       "   (stochastic_depth): StochasticDepth(p=0.052941176470588235, mode=row)\n",
       " ),\n",
       " Sequential(\n",
       "   (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "   (1): Permute()\n",
       "   (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "   (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "   (4): GELU(approximate='none')\n",
       "   (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "   (6): Permute()\n",
       " ),\n",
       " Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384),\n",
       " Permute(),\n",
       " LayerNorm((384,), eps=1e-06, elementwise_affine=True),\n",
       " Linear(in_features=384, out_features=1536, bias=True),\n",
       " GELU(approximate='none'),\n",
       " Linear(in_features=1536, out_features=384, bias=True),\n",
       " Permute(),\n",
       " StochasticDepth(p=0.052941176470588235, mode=row),\n",
       " CNBlock(\n",
       "   (block): Sequential(\n",
       "     (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "     (1): Permute()\n",
       "     (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "     (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "     (4): GELU(approximate='none')\n",
       "     (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "     (6): Permute()\n",
       "   )\n",
       "   (stochastic_depth): StochasticDepth(p=0.058823529411764705, mode=row)\n",
       " ),\n",
       " Sequential(\n",
       "   (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "   (1): Permute()\n",
       "   (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "   (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "   (4): GELU(approximate='none')\n",
       "   (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "   (6): Permute()\n",
       " ),\n",
       " Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384),\n",
       " Permute(),\n",
       " LayerNorm((384,), eps=1e-06, elementwise_affine=True),\n",
       " Linear(in_features=384, out_features=1536, bias=True),\n",
       " GELU(approximate='none'),\n",
       " Linear(in_features=1536, out_features=384, bias=True),\n",
       " Permute(),\n",
       " StochasticDepth(p=0.058823529411764705, mode=row),\n",
       " CNBlock(\n",
       "   (block): Sequential(\n",
       "     (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "     (1): Permute()\n",
       "     (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "     (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "     (4): GELU(approximate='none')\n",
       "     (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "     (6): Permute()\n",
       "   )\n",
       "   (stochastic_depth): StochasticDepth(p=0.06470588235294118, mode=row)\n",
       " ),\n",
       " Sequential(\n",
       "   (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "   (1): Permute()\n",
       "   (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "   (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "   (4): GELU(approximate='none')\n",
       "   (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "   (6): Permute()\n",
       " ),\n",
       " Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384),\n",
       " Permute(),\n",
       " LayerNorm((384,), eps=1e-06, elementwise_affine=True),\n",
       " Linear(in_features=384, out_features=1536, bias=True),\n",
       " GELU(approximate='none'),\n",
       " Linear(in_features=1536, out_features=384, bias=True),\n",
       " Permute(),\n",
       " StochasticDepth(p=0.06470588235294118, mode=row),\n",
       " CNBlock(\n",
       "   (block): Sequential(\n",
       "     (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "     (1): Permute()\n",
       "     (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "     (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "     (4): GELU(approximate='none')\n",
       "     (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "     (6): Permute()\n",
       "   )\n",
       "   (stochastic_depth): StochasticDepth(p=0.07058823529411766, mode=row)\n",
       " ),\n",
       " Sequential(\n",
       "   (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "   (1): Permute()\n",
       "   (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "   (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "   (4): GELU(approximate='none')\n",
       "   (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "   (6): Permute()\n",
       " ),\n",
       " Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384),\n",
       " Permute(),\n",
       " LayerNorm((384,), eps=1e-06, elementwise_affine=True),\n",
       " Linear(in_features=384, out_features=1536, bias=True),\n",
       " GELU(approximate='none'),\n",
       " Linear(in_features=1536, out_features=384, bias=True),\n",
       " Permute(),\n",
       " StochasticDepth(p=0.07058823529411766, mode=row),\n",
       " CNBlock(\n",
       "   (block): Sequential(\n",
       "     (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "     (1): Permute()\n",
       "     (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "     (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "     (4): GELU(approximate='none')\n",
       "     (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "     (6): Permute()\n",
       "   )\n",
       "   (stochastic_depth): StochasticDepth(p=0.07647058823529412, mode=row)\n",
       " ),\n",
       " Sequential(\n",
       "   (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "   (1): Permute()\n",
       "   (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "   (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "   (4): GELU(approximate='none')\n",
       "   (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "   (6): Permute()\n",
       " ),\n",
       " Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384),\n",
       " Permute(),\n",
       " LayerNorm((384,), eps=1e-06, elementwise_affine=True),\n",
       " Linear(in_features=384, out_features=1536, bias=True),\n",
       " GELU(approximate='none'),\n",
       " Linear(in_features=1536, out_features=384, bias=True),\n",
       " Permute(),\n",
       " StochasticDepth(p=0.07647058823529412, mode=row),\n",
       " CNBlock(\n",
       "   (block): Sequential(\n",
       "     (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "     (1): Permute()\n",
       "     (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "     (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "     (4): GELU(approximate='none')\n",
       "     (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "     (6): Permute()\n",
       "   )\n",
       "   (stochastic_depth): StochasticDepth(p=0.0823529411764706, mode=row)\n",
       " ),\n",
       " Sequential(\n",
       "   (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "   (1): Permute()\n",
       "   (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "   (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "   (4): GELU(approximate='none')\n",
       "   (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "   (6): Permute()\n",
       " ),\n",
       " Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384),\n",
       " Permute(),\n",
       " LayerNorm((384,), eps=1e-06, elementwise_affine=True),\n",
       " Linear(in_features=384, out_features=1536, bias=True),\n",
       " GELU(approximate='none'),\n",
       " Linear(in_features=1536, out_features=384, bias=True),\n",
       " Permute(),\n",
       " StochasticDepth(p=0.0823529411764706, mode=row),\n",
       " Sequential(\n",
       "   (0): LayerNorm2d((384,), eps=1e-06, elementwise_affine=True)\n",
       "   (1): Conv2d(384, 768, kernel_size=(2, 2), stride=(1, 1))\n",
       " ),\n",
       " LayerNorm2d((384,), eps=1e-06, elementwise_affine=True),\n",
       " Conv2d(384, 768, kernel_size=(2, 2), stride=(1, 1)),\n",
       " Sequential(\n",
       "   (0): CNBlock(\n",
       "     (block): Sequential(\n",
       "       (0): BasicGaussianMultiplierConv2D(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "       (1): Permute()\n",
       "       (2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "       (3): Linear(in_features=768, out_features=3072, bias=True)\n",
       "       (4): GELU(approximate='none')\n",
       "       (5): Linear(in_features=3072, out_features=768, bias=True)\n",
       "       (6): Permute()\n",
       "     )\n",
       "     (stochastic_depth): StochasticDepth(p=0.08823529411764706, mode=row)\n",
       "   )\n",
       "   (1): CNBlock(\n",
       "     (block): Sequential(\n",
       "       (0): BasicGaussianMultiplierConv2D(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "       (1): Permute()\n",
       "       (2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "       (3): Linear(in_features=768, out_features=3072, bias=True)\n",
       "       (4): GELU(approximate='none')\n",
       "       (5): Linear(in_features=3072, out_features=768, bias=True)\n",
       "       (6): Permute()\n",
       "     )\n",
       "     (stochastic_depth): StochasticDepth(p=0.09411764705882353, mode=row)\n",
       "   )\n",
       "   (2): CNBlock(\n",
       "     (block): Sequential(\n",
       "       (0): BasicGaussianMultiplierConv2D(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "       (1): Permute()\n",
       "       (2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "       (3): Linear(in_features=768, out_features=3072, bias=True)\n",
       "       (4): GELU(approximate='none')\n",
       "       (5): Linear(in_features=3072, out_features=768, bias=True)\n",
       "       (6): Permute()\n",
       "     )\n",
       "     (stochastic_depth): StochasticDepth(p=0.1, mode=row)\n",
       "   )\n",
       " ),\n",
       " CNBlock(\n",
       "   (block): Sequential(\n",
       "     (0): BasicGaussianMultiplierConv2D(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "     (1): Permute()\n",
       "     (2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "     (3): Linear(in_features=768, out_features=3072, bias=True)\n",
       "     (4): GELU(approximate='none')\n",
       "     (5): Linear(in_features=3072, out_features=768, bias=True)\n",
       "     (6): Permute()\n",
       "   )\n",
       "   (stochastic_depth): StochasticDepth(p=0.08823529411764706, mode=row)\n",
       " ),\n",
       " Sequential(\n",
       "   (0): BasicGaussianMultiplierConv2D(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "   (1): Permute()\n",
       "   (2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "   (3): Linear(in_features=768, out_features=3072, bias=True)\n",
       "   (4): GELU(approximate='none')\n",
       "   (5): Linear(in_features=3072, out_features=768, bias=True)\n",
       "   (6): Permute()\n",
       " ),\n",
       " BasicGaussianMultiplierConv2D(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768),\n",
       " Permute(),\n",
       " LayerNorm((768,), eps=1e-06, elementwise_affine=True),\n",
       " Linear(in_features=768, out_features=3072, bias=True),\n",
       " GELU(approximate='none'),\n",
       " Linear(in_features=3072, out_features=768, bias=True),\n",
       " Permute(),\n",
       " StochasticDepth(p=0.08823529411764706, mode=row),\n",
       " CNBlock(\n",
       "   (block): Sequential(\n",
       "     (0): BasicGaussianMultiplierConv2D(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "     (1): Permute()\n",
       "     (2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "     (3): Linear(in_features=768, out_features=3072, bias=True)\n",
       "     (4): GELU(approximate='none')\n",
       "     (5): Linear(in_features=3072, out_features=768, bias=True)\n",
       "     (6): Permute()\n",
       "   )\n",
       "   (stochastic_depth): StochasticDepth(p=0.09411764705882353, mode=row)\n",
       " ),\n",
       " Sequential(\n",
       "   (0): BasicGaussianMultiplierConv2D(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "   (1): Permute()\n",
       "   (2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "   (3): Linear(in_features=768, out_features=3072, bias=True)\n",
       "   (4): GELU(approximate='none')\n",
       "   (5): Linear(in_features=3072, out_features=768, bias=True)\n",
       "   (6): Permute()\n",
       " ),\n",
       " BasicGaussianMultiplierConv2D(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768),\n",
       " Permute(),\n",
       " LayerNorm((768,), eps=1e-06, elementwise_affine=True),\n",
       " Linear(in_features=768, out_features=3072, bias=True),\n",
       " GELU(approximate='none'),\n",
       " Linear(in_features=3072, out_features=768, bias=True),\n",
       " Permute(),\n",
       " StochasticDepth(p=0.09411764705882353, mode=row),\n",
       " CNBlock(\n",
       "   (block): Sequential(\n",
       "     (0): BasicGaussianMultiplierConv2D(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "     (1): Permute()\n",
       "     (2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "     (3): Linear(in_features=768, out_features=3072, bias=True)\n",
       "     (4): GELU(approximate='none')\n",
       "     (5): Linear(in_features=3072, out_features=768, bias=True)\n",
       "     (6): Permute()\n",
       "   )\n",
       "   (stochastic_depth): StochasticDepth(p=0.1, mode=row)\n",
       " ),\n",
       " Sequential(\n",
       "   (0): BasicGaussianMultiplierConv2D(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "   (1): Permute()\n",
       "   (2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "   (3): Linear(in_features=768, out_features=3072, bias=True)\n",
       "   (4): GELU(approximate='none')\n",
       "   (5): Linear(in_features=3072, out_features=768, bias=True)\n",
       "   (6): Permute()\n",
       " ),\n",
       " BasicGaussianMultiplierConv2D(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768),\n",
       " Permute(),\n",
       " LayerNorm((768,), eps=1e-06, elementwise_affine=True),\n",
       " Linear(in_features=768, out_features=3072, bias=True),\n",
       " GELU(approximate='none'),\n",
       " Linear(in_features=3072, out_features=768, bias=True),\n",
       " Permute(),\n",
       " StochasticDepth(p=0.1, mode=row),\n",
       " Identity(),\n",
       " Identity(),\n",
       " UnitConv2D(768, 20, kernel_size=(1, 1), stride=(1, 1), bias=False),\n",
       " UnitConv2D(768, 20, kernel_size=(1, 1), stride=(1, 1), bias=False),\n",
       " UnitConv2D(768, 20, kernel_size=(1, 1), stride=(1, 1), bias=False),\n",
       " UnitConv2D(768, 20, kernel_size=(1, 1), stride=(1, 1), bias=False),\n",
       " UnitConv2D(768, 20, kernel_size=(1, 1), stride=(1, 1), bias=False),\n",
       " UnitConv2D(768, 20, kernel_size=(1, 1), stride=(1, 1), bias=False),\n",
       " UnitConv2D(768, 20, kernel_size=(1, 1), stride=(1, 1), bias=False),\n",
       " UnitConv2D(768, 20, kernel_size=(1, 1), stride=(1, 1), bias=False),\n",
       " UnitConv2D(768, 20, kernel_size=(1, 1), stride=(1, 1), bias=False),\n",
       " UnitConv2D(768, 20, kernel_size=(1, 1), stride=(1, 1), bias=False),\n",
       " UnitConv2D(768, 20, kernel_size=(1, 1), stride=(1, 1), bias=False),\n",
       " UnitConv2D(768, 20, kernel_size=(1, 1), stride=(1, 1), bias=False),\n",
       " UnitConv2D(768, 20, kernel_size=(1, 1), stride=(1, 1), bias=False),\n",
       " UnitConv2D(768, 20, kernel_size=(1, 1), stride=(1, 1), bias=False),\n",
       " UnitConv2D(768, 20, kernel_size=(1, 1), stride=(1, 1), bias=False),\n",
       " UnitConv2D(768, 20, kernel_size=(1, 1), stride=(1, 1), bias=False),\n",
       " UnitConv2D(768, 20, kernel_size=(1, 1), stride=(1, 1), bias=False),\n",
       " Sequential(\n",
       "   (0): AdaptiveMaxPool2d(output_size=(1, 1))\n",
       "   (1): Flatten(start_dim=1, end_dim=-1)\n",
       " ),\n",
       " AdaptiveMaxPool2d(output_size=(1, 1)),\n",
       " Flatten(start_dim=1, end_dim=-1),\n",
       " Sequential(\n",
       "   (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "   (1): Flatten(start_dim=1, end_dim=-1)\n",
       " ),\n",
       " AdaptiveAvgPool2d(output_size=(1, 1)),\n",
       " Flatten(start_dim=1, end_dim=-1),\n",
       " NonNegLinear(),\n",
       " NonNegLinear(),\n",
       " NonNegLinear(),\n",
       " NonNegLinear(),\n",
       " NonNegLinear(),\n",
       " NonNegLinear(),\n",
       " NonNegLinear(),\n",
       " NonNegLinear(),\n",
       " NonNegLinear(),\n",
       " NonNegLinear(),\n",
       " NonNegLinear(),\n",
       " NonNegLinear(),\n",
       " NonNegLinear(),\n",
       " NonNegLinear(),\n",
       " NonNegLinear(),\n",
       " NonNegLinear(),\n",
       " NonNegLinear(),\n",
       " Softmax(dim=1)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i in net.module.modules()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6339634b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B syncing is set to <code>`offline`<code> in this directory.  <br/>Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "from util.log import Log\n",
    "os.environ['WANDB_DISABLED'] = 'true'\n",
    "wandb_run = wandb.init(project=\"pipnet\", name=os.path.basename(args.log_dir), config=vars(args), reinit=False)\n",
    "# log = Log(args.log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47e8df17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chosen network is convnext\n",
      "-------------------------Not using OOD data-------------------------\n"
     ]
    }
   ],
   "source": [
    "optimizer_net, optimizer_classifier, params_to_freeze, params_to_train, params_backbone = get_optimizer_nn(net, args)            \n",
    "scheduler_net = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_net, T_max=len(trainloader)*args.epochs, eta_min=args.lr_net/100.)\n",
    "if args.epochs<=30:\n",
    "    scheduler_classifier = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer_classifier, T_0=5, eta_min=0.001, T_mult=1, verbose=False)\n",
    "else:\n",
    "    scheduler_classifier = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer_classifier, T_0=10, eta_min=0.001, T_mult=1, verbose=False)\n",
    "\n",
    "    \n",
    "if args.OOD_dataset:\n",
    "    trainloader_OOD, trainloader_pretraining_OOD, trainloader_normal_OOD, trainloader_normal_augment_OOD, projectloader_OOD, testloader_OOD, test_projectloader_OOD, _ = get_dataloaders(args, device, OOD=True)\n",
    "    print('-'*25 + 'Using OOD data' + '-'*25)\n",
    "else:\n",
    "    trainloader_OOD = trainloader_pretraining_OOD = trainloader_normal_OOD = trainloader_normal_augment_OOD = projectloader_OOD = testloader_OOD = test_projectloader_OOD = None\n",
    "    print('-'*25 + 'Not using OOD data' + '-'*25)\n",
    "    \n",
    "if ('focal_loss' in args) and (args.focal_loss == 'y'):\n",
    "    from util.custom_losses import WeightedCrossEntropyLoss, WeightedNLLLoss, FocalLossWrapper\n",
    "    criterion = FocalLossWrapper(device=device, alpha=1, gamma=args.focal_loss_gamma, reduction='mean').to(device)\n",
    "else:\n",
    "    from util.custom_losses import WeightedCrossEntropyLoss, WeightedNLLLoss, FocalLossWrapper\n",
    "    criterion = WeightedNLLLoss(device=device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7166862",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'log' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m test_info, log_dict \u001b[38;5;241m=\u001b[39m test_pipnet(net, testloader, optimizer_net, optimizer_classifier, \\\n\u001b[1;32m      2\u001b[0m                                     scheduler_net, scheduler_classifier, criterion, \u001b[38;5;241m0\u001b[39m, \\\n\u001b[1;32m      3\u001b[0m                                         args\u001b[38;5;241m.\u001b[39mepochs, device, pretrain\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, finetune\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \\\n\u001b[1;32m      4\u001b[0m                                         test_loader_OOD\u001b[38;5;241m=\u001b[39mtestloader_OOD, kernel_orth\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mkernel_orth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m, \\\n\u001b[1;32m      5\u001b[0m                                             tanh_desc\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mtanh_desc \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m, align\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39malign \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m, uni\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39muni \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m, align_pf\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39malign_pf \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m,\\\n\u001b[0;32m----> 6\u001b[0m                                             minmaximize\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mminmaximize \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m, wandb_run\u001b[38;5;241m=\u001b[39mwandb_run, pretrain_epochs\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mepochs_pretrain, log\u001b[38;5;241m=\u001b[39m\u001b[43mlog\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'log' is not defined"
     ]
    }
   ],
   "source": [
    "test_info, log_dict = test_pipnet(net, testloader, optimizer_net, optimizer_classifier, \\\n",
    "                                    scheduler_net, scheduler_classifier, criterion, 0, \\\n",
    "                                        args.epochs, device, pretrain=False, finetune=False, \\\n",
    "                                        test_loader_OOD=testloader_OOD, kernel_orth=args.kernel_orth == 'y', \\\n",
    "                                            tanh_desc=args.tanh_desc == 'y', align=args.align == 'y', uni=args.uni == 'y', align_pf=args.align_pf == 'y',\\\n",
    "                                            minmaximize=args.minmaximize == 'y', wandb_run=wandb_run, pretrain_epochs=args.epochs_pretrain, log=log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b84a48d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_heatmap(latent_activation, input_image):\n",
    "    image_a = latent_activation.cpu().numpy()\n",
    "    image_a = (image_a - image_a.min()) / (image_a.max() - image_a.min())\n",
    "\n",
    "    input_image = (input_image - input_image.min()) / (input_image.max() - input_image.min())\n",
    "    image_b = input_image.permute(1, 2, 0).cpu().numpy()\n",
    "    \n",
    "    reshaped_image_a = np.array(Image.fromarray((image_a[0] * 255).astype('uint8')).resize((input_image.shape[-1], input_image.shape[-1])))\n",
    "    normalized_heatmap = (reshaped_image_a - np.min(reshaped_image_a)) / (np.max(reshaped_image_a) - np.min(reshaped_image_a))\n",
    "    \n",
    "    heatmap_colormap = plt.get_cmap('jet')\n",
    "    heatmap_colored = heatmap_colormap(normalized_heatmap)\n",
    "    \n",
    "    heatmap_colored_uint8 = (heatmap_colored[:, :, :3] * 255).astype(np.uint8)\n",
    "    image_a_heatmap_pillow = Image.fromarray(heatmap_colored_uint8)\n",
    "    image_b_pillow = Image.fromarray((image_b * 255).astype('uint8'))\n",
    "    \n",
    "    result_image = Image.blend(image_b_pillow, image_a_heatmap_pillow, alpha=0.3)\n",
    "    \n",
    "    return np.array(result_image)\n",
    "\n",
    "\n",
    "def get_heatmap_uninterpolated(latent_activation, input_image):\n",
    "    image_a = latent_activation.cpu().numpy()\n",
    "    image_a = (image_a - image_a.min()) / (image_a.max() - image_a.min())\n",
    "\n",
    "    input_image = (input_image - input_image.min()) / (input_image.max() - input_image.min())\n",
    "    image_b = input_image.permute(1, 2, 0).cpu().numpy()\n",
    "    \n",
    "    reshaped_image_a = np.array(Image.fromarray((image_a[0] * 255).astype('uint8')).resize((input_image.shape[-1], input_image.shape[-1]), \\\n",
    "                                                                                          resample=Image.NEAREST ))\n",
    "    normalized_heatmap = (reshaped_image_a - np.min(reshaped_image_a)) / (np.max(reshaped_image_a) - np.min(reshaped_image_a))\n",
    "    \n",
    "    heatmap_colormap = plt.get_cmap('jet')\n",
    "    heatmap_colored = heatmap_colormap(normalized_heatmap)\n",
    "    \n",
    "    heatmap_colored_uint8 = (heatmap_colored[:, :, :3] * 255).astype(np.uint8)\n",
    "    image_a_heatmap_pillow = Image.fromarray(heatmap_colored_uint8)\n",
    "    image_b_pillow = Image.fromarray((image_b * 255).astype('uint8'))\n",
    "    \n",
    "    result_image = Image.blend(image_b_pillow, image_a_heatmap_pillow, alpha=0.3)\n",
    "    \n",
    "    return np.array(result_image)\n",
    "\n",
    "def get_bb_gaussian_threshold(latent_activation, sigma=1.0, percentile=97, extend_h=0, extend_w=0):\n",
    "    # latent_activation -> []\n",
    "    upscaled_similarity = get_upscaled_activation_uninterpolated(latent_activation, \\\n",
    "                                                                 image_size=(args.image_size, args.image_size))\n",
    "    upscaled_similarity = minmaxscale(upscaled_similarity)\n",
    "    upscaled_similarity = gaussian(upscaled_similarity, sigma=sigma)\n",
    "    upscaled_similarity = threshold_local(upscaled_similarity, block_size=15, method='mean')\n",
    "    h_min, h_max, w_min, w_max = find_top_percentile_bbox(upscaled_similarity ,percentile=97)\n",
    "    h_min = max(0, h_min-extend_h)\n",
    "    h_max = min(upscaled_similarity.shape[0], h_max+extend_h)\n",
    "    w_min = max(0, w_min-extend_w)\n",
    "    w_max = min(upscaled_similarity.shape[1], w_max+extend_w)\n",
    "    return h_min, h_max, w_min, w_max\n",
    "\n",
    "\n",
    "def minmaxscale(tensor):\n",
    "    return (tensor - tensor.min()) / (tensor.max() - tensor.min())\n",
    "\n",
    "from torch.utils.data import DataLoader, SequentialSampler\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def unshuffle_dataloader(dataloader):\n",
    "    if type(dataloader.dataset) == ImageFolder:\n",
    "        dataset = dataloader.dataset\n",
    "    else:\n",
    "        dataset = dataloader.dataset.dataset.dataset\n",
    "    new_dataloader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=dataloader.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=dataloader.num_workers,\n",
    "        pin_memory=dataloader.pin_memory,\n",
    "        drop_last=dataloader.drop_last,\n",
    "        timeout=dataloader.timeout,\n",
    "        worker_init_fn=dataloader.worker_init_fn,\n",
    "        multiprocessing_context=dataloader.multiprocessing_context,\n",
    "        generator=dataloader.generator,\n",
    "        prefetch_factor=dataloader.prefetch_factor,\n",
    "        persistent_workers=dataloader.persistent_workers\n",
    "    )\n",
    "    return new_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "796e2f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 540it [00:16, 32.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node: root Child: 052+053 Num protos pre pruning: 5 Post pruning: 1\n",
      "Node: root Child: 004+086 Num protos pre pruning: 3 Post pruning: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 120it [00:04, 27.83it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node: 052+053 Child: cub_052_Pied_billed_Grebe Num protos pre pruning: 4 Post pruning: 3\n",
      "Node: 052+053 Child: 053+050 Num protos pre pruning: 2 Post pruning: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 420it [00:13, 32.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node: 004+086 Child: 004+032 Num protos pre pruning: 1 Post pruning: 1\n",
      "Node: 004+086 Child: 086+045 Num protos pre pruning: 3 Post pruning: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 90it [00:03, 26.12it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node: 053+050 Child: cub_053_Western_Grebe Num protos pre pruning: 6 Post pruning: 4\n",
      "Node: 053+050 Child: 050+051 Num protos pre pruning: 3 Post pruning: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 120it [00:04, 27.97it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node: 004+032 Child: cub_004_Groove_billed_Ani Num protos pre pruning: 2 Post pruning: 2\n",
      "Node: 004+032 Child: 032+033 Num protos pre pruning: 2 Post pruning: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 300it [00:09, 31.74it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node: 086+045 Child: 045+101 Num protos pre pruning: 1 Post pruning: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 90it [00:03, 26.00it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node: 032+033 Child: cub_032_Mangrove_Cuckoo Num protos pre pruning: 6 Post pruning: 1\n",
      "Node: 032+033 Child: 033+031 Num protos pre pruning: 2 Post pruning: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 270it [00:08, 31.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node: 045+101 Child: 045+003 Num protos pre pruning: 1 Post pruning: 1\n",
      "Node: 045+101 Child: 101+023 Num protos pre pruning: 1 Post pruning: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 120it [00:04, 26.85it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node: 045+003 Child: 003+002 Num protos pre pruning: 2 Post pruning: 2\n",
      "Node: 045+003 Child: cub_045_Northern_Fulmar Num protos pre pruning: 16 Post pruning: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 150it [00:05, 29.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node: 101+023 Child: 101+100 Num protos pre pruning: 1 Post pruning: 1\n",
      "Node: 101+023 Child: 023+025 Num protos pre pruning: 2 Post pruning: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 90it [00:03, 25.49it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node: 003+002 Child: cub_003_Sooty_Albatross Num protos pre pruning: 15 Post pruning: 1\n",
      "Node: 003+002 Child: 002+001 Num protos pre pruning: 2 Post pruning: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 90it [00:03, 25.27it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node: 023+025 Child: cub_023_Brandt_Cormorant Num protos pre pruning: 16 Post pruning: 2\n",
      "Node: 023+025 Child: 025+024 Num protos pre pruning: 3 Post pruning: 3\n",
      "Done !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Proto activations on leaf descendents - topk images\n",
    "\n",
    "from util.data import ModifiedLabelLoader\n",
    "from collections import defaultdict\n",
    "import heapq\n",
    "import pdb\n",
    "from util.vis_pipnet import get_img_coordinates\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import ImageFont, Image, ImageDraw as D\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "# def find_top_percentile_bbox(image, percentile=95):\n",
    "#     threshold = np.percentile(image.flatten(), percentile)\n",
    "#     mask = image >= threshold\n",
    "#     coords = np.argwhere(mask)\n",
    "#     if coords.size == 0:\n",
    "#         return None, None, None, None\n",
    "#     h_min, w_min = coords.min(axis=0)\n",
    "#     h_max, w_max = coords.max(axis=0)\n",
    "#     h_min, h_max, w_min, w_max = map(int, [h_min, h_max, w_min, w_max])\n",
    "#     return h_min, h_max, w_min, w_max\n",
    "\n",
    "# def find_high_activation_crop(activation_map, percentile=95):\n",
    "#     threshold = np.percentile(activation_map, percentile)\n",
    "#     mask = np.ones(activation_map.shape)\n",
    "#     mask[activation_map < threshold] = 0\n",
    "#     lower_y, upper_y, lower_x, upper_x = 0, 0, 0, 0\n",
    "#     for i in range(mask.shape[0]):\n",
    "#         if np.amax(mask[i]) > 0.5:\n",
    "#             lower_y = i\n",
    "#             break\n",
    "#     for i in reversed(range(mask.shape[0])):\n",
    "#         if np.amax(mask[i]) > 0.5:\n",
    "#             upper_y = i\n",
    "#             break\n",
    "#     for j in range(mask.shape[1]):\n",
    "#         if np.amax(mask[:,j]) > 0.5:\n",
    "#             lower_x = j\n",
    "#             break\n",
    "#     for j in reversed(range(mask.shape[1])):\n",
    "#         if np.amax(mask[:,j]) > 0.5:\n",
    "#             upper_x = j\n",
    "#             break\n",
    "#     return lower_y, upper_y+1, lower_x, upper_x+1\n",
    "\n",
    "# def get_upscaled_activation_uninterpolated(latent_activation, image_size):\n",
    "#     image_a = latent_activation.cpu().numpy()\n",
    "#     min_image_a = image_a.min()\n",
    "#     max_image_a = image_a.max()\n",
    "#     image_a = (image_a - min_image_a) / (max_image_a - min_image_a)\n",
    "#     reshaped_image_a = np.array(Image.fromarray((image_a[0] * 255).astype('uint8')).resize((image_size[-1], \\\n",
    "#                                                                                             image_size[-2]), \\\n",
    "#                                                                                           resample=Image.NEAREST ))\n",
    "    \n",
    "#     reshaped_image_a = (reshaped_image_a / 255).astype('float16')\n",
    "#     reshaped_image_a = (reshaped_image_a * (max_image_a - min_image_a)) + min_image_a\n",
    "#     return reshaped_image_a\n",
    "\n",
    "def functional_UnitConv2D(in_features, weight, bias, stride = 1, padding=0):\n",
    "    normalized_weight = F.normalize(weight.data, p=2, dim=(1, 2, 3)) # Normalize the kernels to unit vectors\n",
    "    normalized_input = F.normalize(in_features, p=2, dim=1) # Normalize the input to unit vectors\n",
    "    if bias is not None:\n",
    "        normalized_bias = F.normalize(bias.data, p=2, dim=0) # Normalize the kernels to unit vectors\n",
    "    else:\n",
    "        normalized_bias = None\n",
    "    return F.conv2d(normalized_input, normalized_weight, normalized_bias, stride=stride, padding=padding)\n",
    "\n",
    "def findCorrespondingToMax(base, target):\n",
    "    output, indices = F.max_pool2d(base, kernel_size=(26, 26), return_indices=True)# these are logits\n",
    "    tensor_flattened = target.view(target.shape[0], target.shape[1], -1)\n",
    "    indices_flattened = indices.view(target.shape[0], target.shape[1], -1)\n",
    "    corresponding_values_in_target = torch.gather(tensor_flattened, 2, indices_flattened)\n",
    "    corresponding_values_in_target = corresponding_values_in_target.view(target.shape[0],\\\n",
    "                                     target.shape[1], 1, 1)\n",
    "    pooled_target = corresponding_values_in_target\n",
    "    return pooled_target\n",
    "\n",
    "def customForwardWithCSandSoftmax(net, xs,  inference=False):\n",
    "    features = net.module._net(xs) \n",
    "    proto_features = {}\n",
    "    proto_features_cs = {}\n",
    "    proto_features_softmaxed = {}\n",
    "    pooled = {}\n",
    "    pooled_cs = {}\n",
    "    pooled_softmaxed = {}\n",
    "    out = {}\n",
    "    for node in net.module.root.nodes_with_children():\n",
    "        # this may or may not be cosine similarity based on UniConv2D or Conv2d\n",
    "        proto_features[node.name] = getattr(net.module, '_'+node.name+'_add_on')(features)\n",
    "        \n",
    "        proto_features[node.name] = torch.abs(proto_features[node.name])\n",
    "        \n",
    "        #calculating cosine similarity\n",
    "        prototypes = getattr(net.module, '_'+node.name+'_add_on')\n",
    "        proto_features_cs[node.name] = functional_UnitConv2D(features, prototypes.weight, prototypes.bias)\n",
    "\n",
    "        if net.module.args.softmax == 'y':\n",
    "            softmax_tau = 0.2\n",
    "            proto_features[node.name] = proto_features[node.name] / softmax_tau\n",
    "            proto_features_softmaxed[node.name] = net.module._softmax(proto_features[node.name])\n",
    "            proto_features[node.name] = proto_features_softmaxed[node.name] # will be overwritten if args.multiply_cs_softmax == 'y'\n",
    "        elif net.module.args.gumbel_softmax == 'y':\n",
    "            proto_features_softmaxed[node.name] = net.module._gumbel_softmax(proto_features[node.name])\n",
    "            proto_features[node.name] = proto_features_softmaxed[node.name] # will be overwritten if args.multiply_cs_softmax == 'y'\n",
    "\n",
    "        if net.module.args.multiply_cs_softmax == 'y':\n",
    "            proto_features[node.name] = proto_features_cs[node.name] * proto_features_softmaxed[node.name]\n",
    "        pooled[node.name] = net.module._pool(proto_features[node.name])\n",
    "        \n",
    "        # this could be softmax or cosine similarity\n",
    "        pooled_cs[node.name] = findCorrespondingToMax(base=proto_features[node.name], \\\n",
    "                                                     target=proto_features_cs[node.name])\n",
    "        \n",
    "        pooled_softmaxed[node.name] = findCorrespondingToMax(base=proto_features[node.name], \\\n",
    "                                                     target=proto_features_softmaxed[node.name])\n",
    "\n",
    "        if inference:\n",
    "            pooled[node.name] = torch.where(pooled[node.name] < 0.1, 0., pooled[node.name])  #during inference, ignore all prototypes that have 0.1 similarity or lower\n",
    "        out[node.name] = getattr(net.module, '_'+node.name+'_classification')(pooled[node.name]) #shape (bs*2, num_classes) # these are logits\n",
    "\n",
    "    return features, proto_features, pooled, pooled_cs, pooled_softmaxed, out\n",
    "\n",
    "find_non_descendants = False # True, False # param\n",
    "vizloader_name = 'projectloader'\n",
    "bbox_percentile = 97\n",
    "topk = 10 # param, args param\n",
    "save_images = True #True\n",
    "pruning_threshold = 0.4\n",
    "\n",
    "font = ImageFont.truetype(\"arial.ttf\", 50)\n",
    "font2 = ImageFont.truetype(\"arial.ttf\", 20)\n",
    "\n",
    "from datetime import datetime\n",
    "# txt_file = open(os.path.join(run_path, \"num_proto_details_\"+datetime.now().strftime(\"%m:%d:%H:%M:%S\")+\".txt\"), \"a\")\n",
    "# txt_file.write('\\n')\n",
    "\n",
    "def write_num_proto_details(proto_mean_activations, node_name, net, threshold, txt_file, args):\n",
    "    \n",
    "    rand_input = torch.randn((1, 3, args.image_size, args.image_size))\n",
    "    with torch.no_grad():\n",
    "        *_, pooled, out = net(rand_input)\n",
    "    num_protos = pooled[node_name].shape[1]\n",
    "    used_protos = len(proto_mean_activations)\n",
    "    non_overspecific = 0\n",
    "    for p in proto_mean_activations:\n",
    "        logstr = '\\t'*2 + f'Proto:{p} '\n",
    "        protos_mean_for_all_leaf_descedants = []\n",
    "        for leaf_descendent in proto_mean_activations[p]:\n",
    "            mean_activation = round(np.mean([activation for activation, *_ in proto_mean_activations[p][leaf_descendent]]), 4)\n",
    "            protos_mean_for_all_leaf_descedants.append(mean_activation)\n",
    "            \n",
    "        if all([(mean_activation>0.2) for mean_activation in protos_mean_for_all_leaf_descedants]):\n",
    "            non_overspecific += 1\n",
    "            \n",
    "    txt_file.write(f\"Node:{node_name},Total:{num_protos},Used:{used_protos},Good:{non_overspecific},threshold={threshold}\\n\")\n",
    "\n",
    "\n",
    "def get_heap():\n",
    "    list_ = []\n",
    "    heapq.heapify(list_)\n",
    "    return list_\n",
    "\n",
    "patchsize, skip = get_patch_size(args)\n",
    "\n",
    "vizloader_dict = {'trainloader': trainloader,\n",
    "                 'projectloader': projectloader,\n",
    "                 'testloader': testloader,\n",
    "                 'test_projectloader': test_projectloader}\n",
    "# vizloader_dict[vizloader_name] = unshuffle_dataloader(vizloader_dict[vizloader_name])\n",
    "\n",
    "\n",
    "if type(vizloader_dict[vizloader_name].dataset) == ImageFolder:\n",
    "    name2label = vizloader_dict[vizloader_name].dataset.class_to_idx\n",
    "    label2name = {label:name for name, label in name2label.items()}\n",
    "else:\n",
    "    name2label = vizloader_dict[vizloader_name].dataset.dataset.dataset.class_to_idx\n",
    "    label2name = {label:name for name, label in name2label.items()}\n",
    "\n",
    "for node in root.nodes_with_children():\n",
    "#     if node.name == 'root':\n",
    "#         continue\n",
    "    non_leaf_children_names = [child.name for child in node.children if not child.is_leaf()]\n",
    "    if len(non_leaf_children_names) == 0: # if all the children are leaf nodes then skip this node\n",
    "        continue\n",
    "\n",
    "#     name2label = projectloader.dataset.class_to_idx # param\n",
    "#     label2name = {label:name for name, label in name2label.items()}\n",
    "    modifiedLabelLoader = ModifiedLabelLoader(vizloader_dict[vizloader_name], node)\n",
    "    coarse_label2name = modifiedLabelLoader.modifiedlabel2name\n",
    "    node_label_to_children = {label: name for name, label in node.children_to_labels.items()}\n",
    "    \n",
    "    imgs = modifiedLabelLoader.filtered_imgs\n",
    "\n",
    "    img_iter = tqdm(enumerate(modifiedLabelLoader),\n",
    "                    total=len(modifiedLabelLoader),\n",
    "                    mininterval=50.,\n",
    "                    desc='Collecting topk',\n",
    "                    ncols=0)\n",
    "\n",
    "    classification_weights = getattr(net.module, '_'+node.name+'_classification').weight\n",
    "    \n",
    "    # maps proto_number -> grand_child_name (or descendant leaf name) -> list of top-k activations\n",
    "    proto_mean_activations = defaultdict(lambda: defaultdict(get_heap))\n",
    "\n",
    "    # maps class names to the prototypes that belong to that\n",
    "    class_and_prototypes = defaultdict(set)\n",
    "\n",
    "    for i, (xs, orig_y, ys) in img_iter:\n",
    "        # change\n",
    "#         if not find_non_descendants: \n",
    "#             # do only when finding descendants\n",
    "#             if coarse_label2name[ys.item()] not in non_leaf_children_names:\n",
    "#                 continue\n",
    "\n",
    "        xs, ys = xs.to(device), ys.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            model_output = net(xs, inference=False)\n",
    "            if len(model_output) == 3:\n",
    "                softmaxes, pooled, _ = model_output\n",
    "            elif len(model_output) == 4:\n",
    "                _, softmaxes, pooled, _ = model_output\n",
    "            \n",
    "            _, _, _, pooled_ip, _, _ = customForwardWithCSandSoftmax(net, xs, inference=False)\n",
    "            \n",
    "            pooled = pooled[node.name].squeeze(0)\n",
    "            pooled_ip = pooled_ip[node.name].squeeze(0) \n",
    "            softmaxes = softmaxes[node.name]#.squeeze(0)\n",
    "\n",
    "            for p in range(pooled.shape[0]): # pooled.shape -> [768] (== num of prototypes)\n",
    "                c_weight = torch.max(classification_weights[:,p]) # classification_weights[:,p].shape -> [200] (== num of classes)\n",
    "                relevant_proto_classes = torch.nonzero(classification_weights[:, p] > 1e-3)\n",
    "                relevant_proto_class_names = [node_label_to_children[class_idx.item()] for class_idx in relevant_proto_classes]\n",
    "                \n",
    "                # Take the max per prototype.                             \n",
    "                max_per_prototype, max_idx_per_prototype = torch.max(softmaxes, dim=0)\n",
    "                max_per_prototype_h, max_idx_per_prototype_h = torch.max(max_per_prototype, dim=1)\n",
    "                max_per_prototype_w, max_idx_per_prototype_w = torch.max(max_per_prototype_h, dim=1) #shape (num_prototypes)\n",
    "                \n",
    "                h_idx = max_idx_per_prototype_h[p, max_idx_per_prototype_w[p]]\n",
    "                w_idx = max_idx_per_prototype_w[p]\n",
    "\n",
    "                if len(relevant_proto_class_names) == 0:\n",
    "                    continue\n",
    "                \n",
    "                # change\n",
    "#                 if (len(relevant_proto_class_names) == 1):# and (relevant_proto_class_names[0] not in non_leaf_children_names):\n",
    "#                     continue\n",
    "                \n",
    "                h_coor_min, h_coor_max, w_coor_min, w_coor_max = get_img_coordinates(args.image_size, softmaxes.shape, patchsize, skip, h_idx, w_idx)\n",
    "                latent_activation = softmaxes[:, p, :, :]\n",
    "                if not find_non_descendants:\n",
    "                    if (coarse_label2name[ys.item()] in relevant_proto_class_names):\n",
    "                        child_node = root.get_node(coarse_label2name[ys.item()])\n",
    "                        leaf_descendent = label2name[orig_y.item()][4:7]\n",
    "                        if topk and (len(proto_mean_activations[p][leaf_descendent]) >= topk):\n",
    "                            heapq.heappushpop(proto_mean_activations[p][leaf_descendent],\\\n",
    "                                              (pooled[p].item(), pooled_ip[p].item(), xs,\\\n",
    "                                               (h_coor_min, h_coor_max, w_coor_min, w_coor_max), \\\n",
    "                                               latent_activation))\n",
    "                        else:\n",
    "                            heapq.heappush(proto_mean_activations[p][leaf_descendent],\\\n",
    "                                           (pooled[p].item(), pooled_ip[p].item(), xs,\\\n",
    "                                            (h_coor_min, h_coor_max, w_coor_min, w_coor_max), \\\n",
    "                                            latent_activation))\n",
    "                else:\n",
    "                    if (coarse_label2name[ys.item()] not in relevant_proto_class_names):\n",
    "                        child_node = root.get_node(coarse_label2name[ys.item()])\n",
    "                        leaf_descendent = label2name[orig_y.item()][4:7]\n",
    "                        img_to_open = imgs[i][0] # it is a tuple of (path to image, lable)\n",
    "                        if topk and (len(proto_mean_activations[p][leaf_descendent]) >= topk):\n",
    "                            heapq.heappushpop(proto_mean_activations[p][leaf_descendent],\\\n",
    "                                              (pooled[p].item(), pooled_ip[p].item(), xs,\\\n",
    "                                               (h_coor_min, h_coor_max, w_coor_min, w_coor_max), \\\n",
    "                                               latent_activation))\n",
    "                        else:\n",
    "                            heapq.heappush(proto_mean_activations[p][leaf_descendent],\\\n",
    "                                           (pooled[p].item(), pooled_ip[p].item(), xs,\\\n",
    "                                            (h_coor_min, h_coor_max, w_coor_min, w_coor_max), \\\n",
    "                                            latent_activation))\n",
    "                class_and_prototypes[', '.join(relevant_proto_class_names)].add(p)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "#         class_and_prototypes_post_pruning = copy.deepcopy(class_and_prototypes)\n",
    "        classification_weights_pre_pruning = classification_weights.clone()\n",
    "        for p in proto_mean_activations:\n",
    "            mean_acts_for_all_leafs = []\n",
    "            for leaf_descendant in proto_mean_activations[p]:\n",
    "                mean_activation = np.mean([activation for activation, *_ in proto_mean_activations[p][leaf_descendant]])\n",
    "                mean_acts_for_all_leafs.append(mean_activation)\n",
    "            if (np.array(mean_acts_for_all_leafs) < pruning_threshold).any():\n",
    "                # setting all weights to zero\n",
    "                classification_weights[:, p] = 0.\n",
    "#             if node.name == '004+086' and p == 8:\n",
    "#                 pdb.set_trace()\n",
    "        for child_classname in class_and_prototypes:\n",
    "            child_label = node.children_to_labels[child_classname]\n",
    "            print('Node:', node.name, 'Child:', child_classname, \\\n",
    "                  'Num protos pre pruning:', torch.nonzero(classification_weights_pre_pruning[child_label, :] > 1e-3).shape[0], \\\n",
    "                    'Post pruning:', torch.nonzero(classification_weights[child_label, :] > 1e-3).shape[0])\n",
    "    \n",
    "print('Done !!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "461a750b",
   "metadata": {},
   "outputs": [],
   "source": [
    "log = Log(os.path.join(args.log_dir, f'post_pruning_thresh={pruning_threshold}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a32bcc46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Epoch0: 100% 11/11 [00:36<00:00,  3.29s/it, L:-5.601,LC:0.171, LA:0.00, L_UNI:-3.806, LT:-5.000, L_MM:-5.000, L_OOD:-5.000, L_ORTH:-5.000, losses_used:AL+UNI+CL]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tFine accuracy: 0.79\n",
      "\tNode name: root, acc: 96.48, f1:96.37, samples: 1022, 052+053=204/240=0.85, 004+086=782/782=1.0\n",
      "\tNode name: 052+053, acc: 100.0, f1:100.0, samples: 240, cub_052_Pied_billed_Grebe=60/60=1.0, 053+050=180/180=1.0\n",
      "\tNode name: 004+086, acc: 96.68, f1:96.63, samples: 782, 004+032=202/224=0.9, 086+045=554/558=0.99\n",
      "\tNode name: 053+050, acc: 97.78, f1:97.76, samples: 180, cub_053_Western_Grebe=56/60=0.93, 050+051=120/120=1.0\n",
      "\tNode name: 004+032, acc: 97.32, f1:97.36, samples: 224, cub_004_Groove_billed_Ani=60/60=1.0, 032+033=158/164=0.96\n",
      "\tNode name: 086+045, acc: 89.25, f1:84.18, samples: 558, cub_086_Pacific_Loon=0/60=0.0, 045+101=498/498=1.0\n",
      "\tNode name: 050+051, acc: 80.0, f1:79.98, samples: 120, cub_050_Eared_Grebe=50/60=0.83, cub_051_Horned_Grebe=46/60=0.77\n",
      "\tNode name: 032+033, acc: 95.12, f1:94.97, samples: 164, cub_032_Mangrove_Cuckoo=38/46=0.83, 033+031=118/118=1.0\n",
      "\tNode name: 045+101, acc: 97.59, f1:97.59, samples: 498, 045+003=230/236=0.97, 101+023=256/262=0.98\n",
      "\tNode name: 033+031, acc: 93.22, f1:93.22, samples: 118, cub_033_Yellow_billed_Cuckoo=56/58=0.97, cub_031_Black_billed_Cuckoo=54/60=0.9\n",
      "\tNode name: 045+003, acc: 94.07, f1:94.03, samples: 236, cub_045_Northern_Fulmar=52/60=0.87, 003+002=170/176=0.97\n",
      "\tNode name: 101+023, acc: 98.47, f1:98.47, samples: 262, 101+100=96/100=0.96, 023+025=162/162=1.0\n",
      "\tNode name: 003+002, acc: 90.91, f1:90.99, samples: 176, cub_003_Sooty_Albatross=50/56=0.89, 002+001=110/120=0.92\n",
      "\tNode name: 101+100, acc: 98.0, f1:98.01, samples: 100, cub_101_White_Pelican=40/40=1.0, cub_100_Brown_Pelican=58/60=0.97\n",
      "\tNode name: 023+025, acc: 77.78, f1:76.44, samples: 162, cub_023_Brandt_Cormorant=30/58=0.52, 025+024=96/104=0.92\n",
      "\tNode name: 002+001, acc: 91.67, f1:91.65, samples: 120, cub_002_Laysan_Albatross=52/60=0.87, cub_001_Black_footed_Albatross=58/60=0.97\n",
      "\tNode name: 025+024, acc: 80.77, f1:80.86, samples: 104, cub_025_Pelagic_Cormorant=48/60=0.8, cub_024_Red_faced_Cormorant=36/44=0.82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_info, log_dict = test_pipnet(net, testloader, optimizer_net, optimizer_classifier, \\\n",
    "                                    scheduler_net, scheduler_classifier, criterion, 0, \\\n",
    "                                        args.epochs, device, pretrain=False, finetune=False, \\\n",
    "                                        test_loader_OOD=testloader_OOD, kernel_orth=args.kernel_orth == 'y', \\\n",
    "                                            tanh_desc=args.tanh_desc == 'y', align=args.align == 'y', uni=args.uni == 'y', align_pf=args.align_pf == 'y',\\\n",
    "                                            minmaximize=args.minmaximize == 'y', wandb_run=wandb_run, pretrain_epochs=args.epochs_pretrain, log=log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ba543085",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({'model_state_dict': net.state_dict(), \\\n",
    "            'optimizer_net_state_dict': optimizer_net.state_dict(), \\\n",
    "            'optimizer_classifier_state_dict': optimizer_classifier.state_dict()}, \\\n",
    "           os.path.join(os.path.join(log._log_dir, 'checkpoints'), 'net_trained_last'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a8e33c6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testloader Unique Labels: {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17}\n",
      "testloader total_samples: 1022\n",
      "leave_out_classes cub_032_Mangrove_Cuckoo\n",
      "leave_out_loader Unique Labels: {8}\n",
      "leave_out_loader total_samples: 46\n",
      "Leave out classes cub_003_Sooty_Albatross,cub_051_Horned_Grebe,cub_032_Mangrove_Cuckoo\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Sampler, SubsetRandomSampler\n",
    "\n",
    "def create_filtered_dataloader(dataloader, new_sampler):\n",
    "    if type(dataloader.dataset) == ImageFolder:\n",
    "        dataset = dataloader.dataset\n",
    "    else:\n",
    "        dataset = dataloader.dataset.dataset.dataset\n",
    "    new_dataloader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=dataloader.batch_size,\n",
    "        shuffle=False,\n",
    "        sampler=new_sampler,\n",
    "        num_workers=dataloader.num_workers,\n",
    "        pin_memory=dataloader.pin_memory,\n",
    "        drop_last=dataloader.drop_last,\n",
    "        timeout=dataloader.timeout,\n",
    "        worker_init_fn=dataloader.worker_init_fn,\n",
    "        multiprocessing_context=dataloader.multiprocessing_context,\n",
    "        generator=dataloader.generator,\n",
    "        prefetch_factor=dataloader.prefetch_factor,\n",
    "        persistent_workers=dataloader.persistent_workers\n",
    "    )\n",
    "    return new_dataloader\n",
    "\n",
    "leave_out_loader = testloader\n",
    "# leave_out_loader = test_projectloader\n",
    "unique_labels = set()\n",
    "total_samples = 0\n",
    "for xs, ys in testloader:\n",
    "    unique_labels.update(ys.tolist())\n",
    "    total_samples += xs.shape[0]\n",
    "print(\"testloader Unique Labels:\", unique_labels)\n",
    "print(\"testloader total_samples:\", total_samples)\n",
    "\n",
    "if ('leave_out_classes' in args) and (args.leave_out_classes != ''):\n",
    "    leave_out_classes = args.leave_out_classes.split(',')[2]\n",
    "    print('leave_out_classes', leave_out_classes)\n",
    "    idx_of_classes_to_keep = set()\n",
    "    name2label = leave_out_loader.dataset.class_to_idx # param\n",
    "    label2name = {label:name for name, label in name2label.items()}\n",
    "    for label in label2name:\n",
    "        # NOTE: Keeping the left out classes here\n",
    "        if label2name[label] in leave_out_classes:\n",
    "            idx_of_classes_to_keep.add(label)\n",
    "\n",
    "    target_indices = []\n",
    "    for i in range(len(leave_out_loader.dataset)):\n",
    "        *_, label = leave_out_loader.dataset[i]\n",
    "        if label in idx_of_classes_to_keep:\n",
    "            target_indices.append(i)\n",
    "    sampler = SubsetRandomSampler(target_indices)\n",
    "    to_shuffle = False\n",
    "else:\n",
    "    print('Model not trained with LOU')\n",
    "    \n",
    "leave_out_loader = create_filtered_dataloader(leave_out_loader, sampler)\n",
    "unique_labels = set()\n",
    "total_samples = 0\n",
    "for xs, ys in leave_out_loader:\n",
    "    unique_labels.update(ys.tolist())\n",
    "    total_samples += xs.shape[0]\n",
    "print(\"leave_out_loader Unique Labels:\", unique_labels)\n",
    "print(\"leave_out_loader total_samples:\", total_samples)\n",
    "\n",
    "name2label = leave_out_loader.dataset.class_to_idx\n",
    "label2name = {label:name for name, label in name2label.items()}\n",
    "\n",
    "print('Leave out classes', args.leave_out_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f2b4aae4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cub_001_Black_footed_Albatross': 0,\n",
       " 'cub_002_Laysan_Albatross': 1,\n",
       " 'cub_003_Sooty_Albatross': 2,\n",
       " 'cub_004_Groove_billed_Ani': 3,\n",
       " 'cub_023_Brandt_Cormorant': 4,\n",
       " 'cub_024_Red_faced_Cormorant': 5,\n",
       " 'cub_025_Pelagic_Cormorant': 6,\n",
       " 'cub_031_Black_billed_Cuckoo': 7,\n",
       " 'cub_032_Mangrove_Cuckoo': 8,\n",
       " 'cub_033_Yellow_billed_Cuckoo': 9,\n",
       " 'cub_045_Northern_Fulmar': 10,\n",
       " 'cub_050_Eared_Grebe': 11,\n",
       " 'cub_051_Horned_Grebe': 12,\n",
       " 'cub_052_Pied_billed_Grebe': 13,\n",
       " 'cub_053_Western_Grebe': 14,\n",
       " 'cub_086_Pacific_Loon': 15,\n",
       " 'cub_100_Brown_Pelican': 16,\n",
       " 'cub_101_White_Pelican': 17}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leave_out_loader.dataset.class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7e724d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Epoch0: 100% 1/1 [00:03<00:00,  3.77s/it, L:1.701,LC:1.603, LA:0.00, L_UNI:-3.708, LT:-5.000, L_MM:-5.000, L_OOD:-5.000, L_ORTH:-5.000, losses_used:AL+UNI+CL]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tFine accuracy: 0.0\n",
      "\tNode name: root, acc: 100.0, f1:100.0, samples: 92, 052+053=0/0=nan, 004+086=92/92=1.0\n",
      "\tNode name: 052+053, acc: inf, f1:inf, samples: 0, cub_052_Pied_billed_Grebe=0/0=nan, 053+050=0/0=nan\n",
      "\tNode name: 004+086, acc: 86.96, f1:93.02, samples: 92, 004+032=80/92=0.87, 086+045=0/0=nan\n",
      "\tNode name: 053+050, acc: inf, f1:inf, samples: 0, cub_053_Western_Grebe=0/0=nan, 050+051=0/0=nan\n",
      "\tNode name: 004+032, acc: 91.3, f1:95.45, samples: 92, cub_004_Groove_billed_Ani=0/0=nan, 032+033=84/92=0.91\n",
      "\tNode name: 086+045, acc: inf, f1:inf, samples: 0, cub_086_Pacific_Loon=0/0=nan, 045+101=0/0=nan\n",
      "\tNode name: 050+051, acc: inf, f1:inf, samples: 0, cub_050_Eared_Grebe=0/0=nan, cub_051_Horned_Grebe=0/0=nan\n",
      "\tNode name: 032+033, acc: 0.0, f1:0.0, samples: 92, cub_032_Mangrove_Cuckoo=0/92=0.0, 033+031=0/0=nan\n",
      "\tNode name: 045+101, acc: inf, f1:inf, samples: 0, 045+003=0/0=nan, 101+023=0/0=nan\n",
      "\tNode name: 033+031, acc: inf, f1:inf, samples: 0, cub_033_Yellow_billed_Cuckoo=0/0=nan, cub_031_Black_billed_Cuckoo=0/0=nan\n",
      "\tNode name: 045+003, acc: inf, f1:inf, samples: 0, cub_045_Northern_Fulmar=0/0=nan, 003+002=0/0=nan\n",
      "\tNode name: 101+023, acc: inf, f1:inf, samples: 0, 101+100=0/0=nan, 023+025=0/0=nan\n",
      "\tNode name: 003+002, acc: inf, f1:inf, samples: 0, cub_003_Sooty_Albatross=0/0=nan, 002+001=0/0=nan\n",
      "\tNode name: 101+100, acc: inf, f1:inf, samples: 0, cub_101_White_Pelican=0/0=nan, cub_100_Brown_Pelican=0/0=nan\n",
      "\tNode name: 023+025, acc: inf, f1:inf, samples: 0, cub_023_Brandt_Cormorant=0/0=nan, 025+024=0/0=nan\n",
      "\tNode name: 002+001, acc: inf, f1:inf, samples: 0, cub_002_Laysan_Albatross=0/0=nan, cub_001_Black_footed_Albatross=0/0=nan\n",
      "\tNode name: 025+024, acc: inf, f1:inf, samples: 0, cub_025_Pelagic_Cormorant=0/0=nan, cub_024_Red_faced_Cormorant=0/0=nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# testloader\n",
    "\n",
    "log = Log(os.path.join(args.log_dir, f'LOU_accuracy'))\n",
    "test_info, log_dict = test_pipnet(net, leave_out_loader, optimizer_net, optimizer_classifier, \\\n",
    "                                    scheduler_net, scheduler_classifier, criterion, 0, \\\n",
    "                                        args.epochs, device, pretrain=False, finetune=False, \\\n",
    "                                        test_loader_OOD=testloader_OOD, kernel_orth=args.kernel_orth == 'y', \\\n",
    "                                            tanh_desc=args.tanh_desc == 'y', align=args.align == 'y', uni=args.uni == 'y', align_pf=args.align_pf == 'y',\\\n",
    "                                            minmaximize=args.minmaximize == 'y', wandb_run=wandb_run, pretrain_epochs=args.epochs_pretrain, log=log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "03de2ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Epoch0: 100% 162/162 [00:05<00:00, 31.72it/s, L:2.770,LC:1.627, LA:0.00, L_UNI:-3.416, LT:-5.000, L_MM:-5.000, L_OOD:-5.000, L_ORTH:-5.000, losses_used:AL+UNI+CL]\n",
      "/home/harishbabu/.conda/envs/hpnet1/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3464: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/harishbabu/.conda/envs/hpnet1/lib/python3.8/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tFine accuracy: 0.0\n",
      "\tNode name: root, acc: 85.8, f1:84.96, samples: 324, 052+053=76/120=0.63, 004+086=202/204=0.99\n",
      "\tNode name: 052+053, acc: 95.0, f1:97.44, samples: 120, cub_052_Pied_billed_Grebe=0/0=nan, 053+050=114/120=0.95\n",
      "\tNode name: 004+086, acc: 88.24, f1:88.17, samples: 204, 004+032=76/92=0.83, 086+045=104/112=0.93\n",
      "\tNode name: 053+050, acc: 91.67, f1:95.65, samples: 120, cub_053_Western_Grebe=0/0=nan, 050+051=110/120=0.92\n",
      "\tNode name: 004+032, acc: 86.96, f1:93.02, samples: 92, cub_004_Groove_billed_Ani=0/0=nan, 032+033=80/92=0.87\n",
      "\tNode name: 086+045, acc: 100.0, f1:100.0, samples: 112, cub_086_Pacific_Loon=0/0=nan, 045+101=112/112=1.0\n",
      "\tNode name: 050+051, acc: 0.0, f1:0.0, samples: 120, cub_050_Eared_Grebe=0/0=nan, cub_051_Horned_Grebe=0/120=0.0\n",
      "\tNode name: 032+033, acc: 0.0, f1:0.0, samples: 92, cub_032_Mangrove_Cuckoo=0/92=0.0, 033+031=0/0=nan\n",
      "\tNode name: 045+101, acc: 89.29, f1:94.34, samples: 112, 045+003=100/112=0.89, 101+023=0/0=nan\n",
      "\tNode name: 033+031, acc: inf, f1:inf, samples: 0, cub_033_Yellow_billed_Cuckoo=0/0=nan, cub_031_Black_billed_Cuckoo=0/0=nan\n",
      "\tNode name: 045+003, acc: 78.57, f1:88.0, samples: 112, cub_045_Northern_Fulmar=0/0=nan, 003+002=88/112=0.79\n",
      "\tNode name: 101+023, acc: inf, f1:inf, samples: 0, 101+100=0/0=nan, 023+025=0/0=nan\n",
      "\tNode name: 003+002, acc: 0.0, f1:0.0, samples: 112, cub_003_Sooty_Albatross=0/112=0.0, 002+001=0/0=nan\n",
      "\tNode name: 101+100, acc: inf, f1:inf, samples: 0, cub_101_White_Pelican=0/0=nan, cub_100_Brown_Pelican=0/0=nan\n",
      "\tNode name: 023+025, acc: inf, f1:inf, samples: 0, cub_023_Brandt_Cormorant=0/0=nan, 025+024=0/0=nan\n",
      "\tNode name: 002+001, acc: inf, f1:inf, samples: 0, cub_002_Laysan_Albatross=0/0=nan, cub_001_Black_footed_Albatross=0/0=nan\n",
      "\tNode name: 025+024, acc: inf, f1:inf, samples: 0, cub_025_Pelagic_Cormorant=0/0=nan, cub_024_Red_faced_Cormorant=0/0=nan\n"
     ]
    }
   ],
   "source": [
    "# test_projectloader\n",
    "\n",
    "log = Log(os.path.join(args.log_dir, f'LOU_accuracy'))\n",
    "test_info, log_dict = test_pipnet(net, leave_out_loader, optimizer_net, optimizer_classifier, \\\n",
    "                                    scheduler_net, scheduler_classifier, criterion, 0, \\\n",
    "                                        args.epochs, device, pretrain=False, finetune=False, \\\n",
    "                                        test_loader_OOD=testloader_OOD, kernel_orth=args.kernel_orth == 'y', \\\n",
    "                                            tanh_desc=args.tanh_desc == 'y', align=args.align == 'y', uni=args.uni == 'y', align_pf=args.align_pf == 'y',\\\n",
    "                                            minmaximize=args.minmaximize == 'y', wandb_run=wandb_run, pretrain_epochs=args.epochs_pretrain, log=log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a601f84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "float"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "type(torch.divide(1, 0).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "21080d7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.exists('/fastscratch/harishbabu/data/CUB_18_hpnet/dataset_imgnet_hpnet_bb_crop/train_augmented/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77dfced7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
