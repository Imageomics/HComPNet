{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import sys, os\n",
    "import random\n",
    "import numpy as np\n",
    "from shutil import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "from omegaconf import OmegaConf\n",
    "import shutil\n",
    "import pickle\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from torchvision.datasets.folder import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "# from skimage.filters import threshold_local, gaussian\n",
    "import ntpath\n",
    "from util.data import ModifiedLabelLoader\n",
    "from collections import defaultdict\n",
    "import heapq\n",
    "from util.vis_pipnet import get_img_coordinates\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import ImageFont, Image, ImageDraw as D\n",
    "import torchvision\n",
    "from datetime import datetime\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import math\n",
    "\n",
    "from hcompnet.model import HComPNet, get_network\n",
    "from util.log import Log\n",
    "from util.args import get_args, save_args, get_optimizer_nn\n",
    "from util.data import get_dataloaders\n",
    "from util.func import init_weights_xavier\n",
    "# from pipnet.test import eval_pipnet, get_thresholds, eval_ood\n",
    "from util.node import Node\n",
    "from util.phylo_utils import construct_phylo_tree, construct_discretized_phylo_tree\n",
    "from util.func import get_patch_size\n",
    "from util.data import ModifiedLabelLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path to the experiment\n",
    "run_path = 'runs/experiment'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def minmaxscale(tensor):\n",
    "    return (tensor - tensor.min()) / (tensor.max() - tensor.min())\n",
    "\n",
    "from torch.utils.data import DataLoader, SequentialSampler\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def unshuffle_dataloader(dataloader):\n",
    "    if type(dataloader.dataset) == ImageFolder:\n",
    "        dataset = dataloader.dataset\n",
    "    else:\n",
    "        dataset = dataloader.dataset.dataset.dataset\n",
    "    new_dataloader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=dataloader.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=dataloader.num_workers,\n",
    "        pin_memory=dataloader.pin_memory,\n",
    "        drop_last=dataloader.drop_last,\n",
    "        timeout=dataloader.timeout,\n",
    "        worker_init_fn=dataloader.worker_init_fn,\n",
    "        multiprocessing_context=dataloader.multiprocessing_context,\n",
    "        generator=dataloader.generator,\n",
    "        prefetch_factor=dataloader.prefetch_factor,\n",
    "        persistent_workers=dataloader.persistent_workers\n",
    "    )\n",
    "    return new_dataloader\n",
    "\n",
    "\n",
    "def get_heatmap(latent_activation, input_image, constant_color_scale=False):\n",
    "    image_a = latent_activation.cpu().numpy()\n",
    "    image_b = input_image.permute(1, 2, 0).cpu().numpy()\n",
    "    reshaped_image_a = np.array(Image.fromarray((image_a[0] * 255).astype('uint8')).resize((input_image.shape[-1], input_image.shape[-1])))\n",
    "    if constant_color_scale:\n",
    "        reshaped_image_a = np.concatenate((reshaped_image_a, np.zeros((reshaped_image_a.shape[1], 1)), np.ones((reshaped_image_a.shape[1], 1))*255), axis=1)\n",
    "    normalized_heatmap = (reshaped_image_a - np.min(reshaped_image_a)) / (np.max(reshaped_image_a) - np.min(reshaped_image_a))\n",
    "    heatmap_colormap = plt.get_cmap('jet')\n",
    "    heatmap_colored = heatmap_colormap(normalized_heatmap)\n",
    "    if constant_color_scale:\n",
    "        heatmap_colored = heatmap_colored[:, :-2]\n",
    "    heatmap_colored_uint8 = (heatmap_colored[:, :, :3] * 255).astype(np.uint8)\n",
    "    image_a_heatmap_pillow = Image.fromarray(heatmap_colored_uint8)\n",
    "    image_b_pillow = Image.fromarray((image_b * 255).astype('uint8'))\n",
    "    result_image = Image.blend(image_b_pillow, image_a_heatmap_pillow, alpha=0.3)\n",
    "    return np.array(result_image)\n",
    "\n",
    "\n",
    "def get_heap():\n",
    "    list_ = []\n",
    "    heapq.heapify(list_)\n",
    "    return list_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    device_ids = [torch.cuda.current_device()]\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    device_ids = []\n",
    "args_file = open(os.path.join(run_path, 'metadata', 'args.pickle'), 'rb')\n",
    "args = pickle.load(args_file)\n",
    "\n",
    "# ------------ Load the phylogeny tree ------------\n",
    "phylo_config = OmegaConf.load(args.phylo_config)\n",
    "if phylo_config.phyloDistances_string == 'None':\n",
    "    root = construct_phylo_tree(phylo_config.phylogeny_path)\n",
    "    print('-'*25 + ' No discretization ' + '-'*25)\n",
    "else:\n",
    "    root = construct_discretized_phylo_tree(phylo_config.phylogeny_path, phylo_config.phyloDistances_string)\n",
    "    print('-'*25 + ' Discretized ' + '-'*25)\n",
    "root.assign_all_descendents()\n",
    "for node in root.nodes_with_children():\n",
    "    node.set_num_protos(num_protos_per_descendant=args.num_protos_per_descendant,\\\n",
    "                        num_protos_per_child=args.num_protos_per_child,\\\n",
    "                        min_protos_per_child=args.min_protos_per_child)\n",
    "    \n",
    "# ------------ Load the train and test datasets ------------\n",
    "args.batch_size = 1\n",
    "trainloader, trainloader_pretraining, trainloader_normal, trainloader_normal_augment, projectloader, testloader, test_projectloader, classes = get_dataloaders(args, device)\n",
    "\n",
    "# ------------ Load the model checkpoint ------------\n",
    "ckpt_file_name = 'net_trained_last'\n",
    "epoch = ckpt_file_name.split('_')[-1]\n",
    "ckpt_path = os.path.join(run_path, 'checkpoints', ckpt_file_name)\n",
    "checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "feature_net, add_on_layers, pool_layer, classification_layers = get_network(args, root=root)\n",
    "net = HComPNet(feature_net = feature_net,\n",
    "                args = args,\n",
    "                add_on_layers = add_on_layers,\n",
    "                pool_layer = pool_layer,\n",
    "                classification_layers = classification_layers,\n",
    "                num_parent_nodes = len(root.nodes_with_children()),\n",
    "                root = root)\n",
    "net = net.to(device=device)\n",
    "net = nn.DataParallel(net, device_ids = device_ids)    \n",
    "net.load_state_dict(checkpoint['model_state_dict'],strict=True)\n",
    "# Forward one batch through the backbone to get the latent output size\n",
    "with torch.no_grad():\n",
    "    xs1, _, _ = next(iter(trainloader))\n",
    "    xs1 = xs1.to(device)\n",
    "    _, proto_features, _, _ = net(xs1)\n",
    "    wshape = proto_features['root'].shape[-1]\n",
    "    args.wshape = wshape #needed for calculating image patch size\n",
    "    print(\"Output shape: \", proto_features['root'].shape, flush=True)\n",
    "print(args.wshape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find subtree root - only for finding does not affect the run, use the value found here in the visualization block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if only interested in a subtree, add some leaf nodes of the sub tree to the list\n",
    "# here we find the root of the smallest subtree that contains all the leaves\n",
    "\n",
    "leaf_descendents = set(['cub_052_Pied_billed_Grebe', 'cub_004_Groove_billed_Ani'])\n",
    "subtree_root = root\n",
    "for node in root.nodes_with_children():\n",
    "    if leaf_descendents.issubset(node.leaf_descendents) and (len(node.leaf_descendents) < len(subtree_root.leaf_descendents)):\n",
    "        subtree_root = node\n",
    "print(subtree_root.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save TopK Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Proto activations on leaf descendents - topk images\n",
    "vizloader_name = 'testloader' # projectloader\n",
    "find_non_descendants = False # if True gets topk images from contrasting set (images that are not descendants of a given node)\n",
    "topk = 3\n",
    "save_images = True # True, False\n",
    "font = ImageFont.truetype(\"arial.ttf\", 50)\n",
    "subtree_root = root.get_node('024+051') # Plotting for a subtree since the whole tree is too large\n",
    "    \n",
    "patchsize, skip = get_patch_size(args)\n",
    "vizloader_dict = {'trainloader': trainloader,\n",
    "                 'projectloader': projectloader,\n",
    "                 'testloader': testloader,\n",
    "                 'test_projectloader': test_projectloader}\n",
    "vizloader_dict[vizloader_name] = unshuffle_dataloader(vizloader_dict[vizloader_name])\n",
    "\n",
    "suffix = 'contrasting_set' if find_non_descendants else ''\n",
    "viz_save_dir = os.path.join(run_path, f'topk_viz_{suffix}_root={subtree_root.name}')\n",
    "\n",
    "if type(vizloader_dict[vizloader_name].dataset) == ImageFolder:\n",
    "    name2label = vizloader_dict[vizloader_name].dataset.class_to_idx\n",
    "    label2name = {label:name for name, label in name2label.items()}\n",
    "else:\n",
    "    name2label = vizloader_dict[vizloader_name].dataset.dataset.dataset.class_to_idx\n",
    "    label2name = {label:name for name, label in name2label.items()}\n",
    "    \n",
    "for node in root.nodes_with_children():\n",
    "    if (node.name not in subtree_root.descendents) and (node.name != subtree_root.name):\n",
    "        print('Skipping node', node.name)\n",
    "        continue\n",
    "\n",
    "    name2label = vizloader_dict[vizloader_name].dataset.class_to_idx\n",
    "    label2name = {label:name for name, label in name2label.items()}\n",
    "    modifiedLabelLoader = ModifiedLabelLoader(vizloader_dict[vizloader_name], node)\n",
    "    coarse_label2name = modifiedLabelLoader.modifiedlabel2name\n",
    "    node_label_to_children = {label: name for name, label in node.children_to_labels.items()}\n",
    "    imgs = modifiedLabelLoader.filtered_imgs\n",
    "    img_iter = tqdm(enumerate(modifiedLabelLoader),\n",
    "                    total=len(modifiedLabelLoader),\n",
    "                    mininterval=50.,\n",
    "                    desc='Collecting topk',\n",
    "                    ncols=0)\n",
    "\n",
    "    classification_weights = getattr(net.module, '_'+node.name+'_classification').weight\n",
    "    \n",
    "    # maps proto_number -> grand_child_name (or descendant leaf name) -> list of top-k activations\n",
    "    proto_mean_activations = defaultdict(lambda: defaultdict(get_heap))\n",
    "\n",
    "    # maps class names to the prototypes that belong to that\n",
    "    class_and_prototypes = defaultdict(set)\n",
    "\n",
    "    for i, (xs, orig_y, ys) in img_iter:\n",
    "        xs, ys = xs.to(device), ys.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            _, softmaxes, pooled, _ = net(xs, inference=False)\n",
    "            pooled = pooled[node.name].squeeze(0) \n",
    "            softmaxes = softmaxes[node.name\n",
    "\n",
    "            for p in range(pooled.shape[0]): # pooled.shape -> [768] (== num of prototypes)\n",
    "                c_weight = torch.max(classification_weights[:,p]) # classification_weights[:,p].shape -> [200] (== num of classes)\n",
    "                relevant_proto_classes = torch.nonzero(classification_weights[:, p] > 1e-3)\n",
    "                relevant_proto_class_names = [node_label_to_children[class_idx.item()] for class_idx in relevant_proto_classes]\n",
    "                \n",
    "                # Take the max per prototype.                             \n",
    "                max_per_prototype, max_idx_per_prototype = torch.max(softmaxes, dim=0)\n",
    "                max_per_prototype_h, max_idx_per_prototype_h = torch.max(max_per_prototype, dim=1)\n",
    "                max_per_prototype_w, max_idx_per_prototype_w = torch.max(max_per_prototype_h, dim=1) #shape (num_prototypes)\n",
    "                \n",
    "                h_idx = max_idx_per_prototype_h[p, max_idx_per_prototype_w[p]]\n",
    "                w_idx = max_idx_per_prototype_w[p]\n",
    "\n",
    "                if len(relevant_proto_class_names) == 0:\n",
    "                    continue\n",
    "                \n",
    "                h_coor_min, h_coor_max, w_coor_min, w_coor_max = get_img_coordinates(args.image_size, softmaxes.shape, patchsize, skip, h_idx, w_idx)\n",
    "                latent_activation = softmaxes[:, p, :, :]\n",
    "\n",
    "                if (not find_non_descendants and ((coarse_label2name[ys.item()] in relevant_proto_class_names))) or \\\n",
    "                    (find_non_descendants and ((coarse_label2name[ys.item()] not in relevant_proto_class_names)))\n",
    "                        child_node = root.get_node(coarse_label2name[ys.item()])\n",
    "                        leaf_descendent = label2name[orig_y.item()]#[4:7]\n",
    "                        img_to_open = imgs[i][0] # it is a tuple of (path to image, lable)\n",
    "                        if topk and (len(proto_mean_activations[p][leaf_descendent]) >= topk):\n",
    "                            heapq.heappushpop(proto_mean_activations[p][leaf_descendent],\\\n",
    "                                              (pooled[p].item(), img_to_open,\\\n",
    "                                               (h_coor_min, h_coor_max, w_coor_min, w_coor_max), latent_activation))\n",
    "                        else:\n",
    "                            heapq.heappush(proto_mean_activations[p][leaf_descendent],\\\n",
    "                                           (pooled[p].item(), img_to_open,\\\n",
    "                                            (h_coor_min, h_coor_max, w_coor_min, w_coor_max), latent_activation))\n",
    "\n",
    "                class_and_prototypes[', '.join(relevant_proto_class_names)].add(p)\n",
    "\n",
    "    print('Node', node.name)\n",
    "    for child_classname in class_and_prototypes:\n",
    "        print('\\t'*1, 'Child:', child_classname)\n",
    "        for p in class_and_prototypes[child_classname]:\n",
    "            logstr = '\\t'*2 + f'Proto:{p} '\n",
    "            mean_activation_of_every_leaf = []\n",
    "            for leaf_descendent in proto_mean_activations[p]:\n",
    "                mean_activation = round(np.mean([activation for activation, *_ in proto_mean_activations[p][leaf_descendent]]), 4)\n",
    "                num_images = len(proto_mean_activations[p][leaf_descendent])\n",
    "                logstr += f'{leaf_descendent}:({mean_activation}) '\n",
    "                mean_activation_of_every_leaf.append(mean_activation)\n",
    "            print(logstr)\n",
    "            \n",
    "            if len(proto_mean_activations[p]) == 0:\n",
    "                continue\n",
    "            \n",
    "            if save_images:\n",
    "                patches = []\n",
    "                right_descriptions = []\n",
    "                text_region_width = 3 # 3x the width of a patch\n",
    "\n",
    "                font_size = 40\n",
    "                fnt = ImageFont.truetype(\"arial.ttf\", font_size)\n",
    "                max_width = ImageDraw.Draw(Image.new(\"RGB\", (100, 100), (255, 0, 0))).textlength('-', font=fnt)\n",
    "                for leaf_descendent in proto_mean_activations[p]:\n",
    "                    for word in leaf_descendent.split('_')[2:]:\n",
    "                        width_of_word = ImageDraw.Draw(Image.new(\"RGB\", (100, 100), (255, 0, 0))).textlength(word, font=fnt)\n",
    "                        max_width = max(max_width, width_of_word)\n",
    "\n",
    "                for leaf_descendent, heap in proto_mean_activations[p].items():\n",
    "                    if 'BUT' in args.dataset:\n",
    "                        species_name = ' '.join(leaf_descendent.split('_')[2:4])\n",
    "                    else:\n",
    "                        species_name = ' '.join(leaf_descendent.split('_')[2:])\n",
    "                    heap = sorted(heap)[::-1]\n",
    "                    mean_activation = round(np.mean([activation for activation, *_ in proto_mean_activations[p][leaf_descendent]]), 4)\n",
    "                    for rank, ele in enumerate(heap):\n",
    "                        activation, img_to_open, (h_coor_min, h_coor_max, w_coor_min, w_coor_max), latent_activation = ele\n",
    "                        image = transforms.Resize(size=(args.image_size, args.image_size))(Image.open(img_to_open))\n",
    "                        img_tensor = transforms.ToTensor()(image)#.unsqueeze_(0) #shape (1, 3, h, w)\n",
    "                        overlayed_image_np = get_heatmap(latent_activation, img_tensor, constant_color_scale=True)\n",
    "                        overlayed_image = torch.tensor(overlayed_image_np).permute(2, 0, 1).float() / 255.\n",
    "                        \n",
    "                        reshaped_latent_activation = np.array(Image.fromarray((latent_activation.cpu().numpy()[0] * 255).astype('uint8')).resize((img_tensor.shape[-1], img_tensor.shape[-1])))\n",
    "                        center = np.unravel_index(np.argmax(reshaped_latent_activation), reshaped_latent_activation.shape)\n",
    "                        patch_size = 64\n",
    "                        h_coor_min = int(max(0, center[0] - (patch_size/2.)))\n",
    "                        h_coor_max = int(min(img_tensor.shape[1], center[0] + (patch_size/2.)))\n",
    "                        w_coor_min = int(max(0, center[1] - (patch_size/2.)))\n",
    "                        w_coor_max = int(min(img_tensor.shape[2], center[1] + (patch_size/2.)))\n",
    "                        img_tensor_patch = img_tensor[:, h_coor_min:h_coor_max, w_coor_min:w_coor_max]\n",
    "\n",
    "                        scale_factor = 1.7  # 70% increase\n",
    "                        heatmap_patch = overlayed_image[:, h_coor_min:h_coor_max, w_coor_min:w_coor_max]\n",
    "                        resized_heatmap_patch = F.interpolate(heatmap_patch.unsqueeze(0), scale_factor=scale_factor, \\\n",
    "                                                      mode='bilinear', align_corners=False).squeeze(0)\n",
    "                        resized_heatmap_patch = torchvision.utils.draw_bounding_boxes((resized_heatmap_patch * 255).to(torch.uint8), \\\n",
    "                                                                                torch.tensor([[0, 0, resized_heatmap_patch.shape[2], resized_heatmap_patch.shape[1]]]), \\\n",
    "                                                                                width=4, colors=(255, 0, 0))\n",
    "                        resized_heatmap_patch = resized_heatmap_patch.float() / 255.\n",
    "                        \n",
    "                        resized_img_patch = F.interpolate(img_tensor_patch.unsqueeze(0), scale_factor=scale_factor, \\\n",
    "                                                      mode='bilinear', align_corners=False).squeeze(0)\n",
    "                        resized_img_patch = torchvision.utils.draw_bounding_boxes((resized_img_patch * 255).to(torch.uint8), \\\n",
    "                                                                                torch.tensor([[0, 0, resized_img_patch.shape[2], resized_img_patch.shape[1]]]), \\\n",
    "                                                                                width=4, colors=(255, 255, 0))\n",
    "                        resized_img_patch = resized_img_patch.float() / 255.\n",
    "                        \n",
    "                        resized_patch = torchvision.utils.make_grid([resized_img_patch, resized_heatmap_patch], nrow=1, padding=1, pad_value=1., border=1)\n",
    "                        white_image = torch.ones(3, img_tensor.shape[1], img_tensor.shape[2])\n",
    "                        patch_height = resized_patch.shape[1]\n",
    "                        y_start = (white_image.shape[1] - patch_height) // 2                        \n",
    "                        x_start = 10  # 10 pixels from the left\n",
    "                        white_image[:, y_start:y_start+patch_height, x_start:x_start+resized_patch.shape[2]] = resized_patch\n",
    "\n",
    "                        # Bounding box on original image\n",
    "                        img_tensor = torchvision.utils.draw_bounding_boxes((img_tensor * 255).to(torch.uint8), \\\n",
    "                                                                                torch.tensor([[w_coor_min, h_coor_min, w_coor_max, h_coor_max]]), \\\n",
    "                                                                                width=2, colors=(255, 255, 0))\n",
    "                        img_tensor = img_tensor.float() / 255.\n",
    "\n",
    "                        # Bounding box on overlayed image\n",
    "                        overlayed_image = torchvision.utils.draw_bounding_boxes((overlayed_image * 255).to(torch.uint8), \\\n",
    "                                                                                torch.tensor([[w_coor_min, h_coor_min, w_coor_max, h_coor_max]]), \\\n",
    "                                                                                width=2, colors=(255, 0, 0))\n",
    "                        overlayed_image = overlayed_image.float() / 255.\n",
    "\n",
    "                        grid_cell = torchvision.utils.make_grid([overlayed_image, img_tensor, white_image], nrow=3, padding=5, pad_value=1., border=1)\n",
    "\n",
    "                        patches.append(grid_cell)\n",
    "\n",
    "                    text = '\\n'.join(species_name.split(' '))\n",
    "                    image_size = (math.ceil(max_width) + 10, patches[0].shape[1])\n",
    "                    txtimage = Image.new(\"RGB\", image_size, (255, 255, 255))\n",
    "                    d = ImageDraw.Draw(txtimage)\n",
    "                    d.multiline_text((image_size[0]/2, image_size[1]/2), text, font=fnt, fill=(0, 0, 0), align =\"center\", anchor=\"mm\")\n",
    "                    txttensor = transforms.ToTensor()(txtimage)#.unsqueeze_(0)\n",
    "                    right_descriptions.append(txttensor)\n",
    "                    \n",
    "\n",
    "                padding = 0\n",
    "                grid_rows = []\n",
    "                for k in range(len(proto_mean_activations[p])):\n",
    "                    grid_row = torchvision.utils.make_grid(patches[k*topk:(k+1)*topk], nrow=topk, padding=padding, border=0)\n",
    "                    grid_right_description = torchvision.utils.make_grid(right_descriptions[k], nrow=1, padding=padding, border=0)\n",
    "                    grid_row = torch.cat([grid_right_description, grid_row], dim=-1)\n",
    "                    grid_rows.append(grid_row)\n",
    "                grid = torchvision.utils.make_grid(grid_rows, nrow=1, padding=5, pad_value=1.)\n",
    "                    \n",
    "                if save_images:\n",
    "                    os.makedirs(os.path.join(viz_save_dir, node.name), exist_ok=True)\n",
    "                    torchvision.utils.save_image(grid, os.path.join(viz_save_dir, node.name, f'{child_classname}-p{p}.png'), border=0)\n",
    "\n",
    "print('Done !!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Proto activations on leaf descendents - topk images\n",
    "\n",
    "def get_heatmap(latent_activation, input_image, constant_color_scale=False):\n",
    "    image_a = latent_activation.cpu().numpy()\n",
    "    # image_a = (image_a - image_a.min()) / (image_a.max() - image_a.min())\n",
    "\n",
    "    # input_image = (input_image - input_image.min()) / (input_image.max() - input_image.min())\n",
    "    image_b = input_image.permute(1, 2, 0).cpu().numpy()\n",
    "    \n",
    "    reshaped_image_a = np.array(Image.fromarray((image_a[0] * 255).astype('uint8')).resize((input_image.shape[-1], input_image.shape[-1])))\n",
    "\n",
    "    if constant_color_scale:\n",
    "        reshaped_image_a = np.concatenate((reshaped_image_a, np.zeros((reshaped_image_a.shape[1], 1)), np.ones((reshaped_image_a.shape[1], 1))*255), axis=1)\n",
    "    \n",
    "    normalized_heatmap = (reshaped_image_a - np.min(reshaped_image_a)) / (np.max(reshaped_image_a) - np.min(reshaped_image_a))\n",
    "\n",
    "    heatmap_colormap = plt.get_cmap('jet')\n",
    "    # heatmap_colormap = plt.get_cmap('rainbow')\n",
    "    heatmap_colored = heatmap_colormap(normalized_heatmap)\n",
    "\n",
    "    if constant_color_scale:\n",
    "        heatmap_colored = heatmap_colored[:, :-2]\n",
    "    \n",
    "    heatmap_colored_uint8 = (heatmap_colored[:, :, :3] * 255).astype(np.uint8)\n",
    "    image_a_heatmap_pillow = Image.fromarray(heatmap_colored_uint8)\n",
    "    image_b_pillow = Image.fromarray((image_b * 255).astype('uint8'))\n",
    "    \n",
    "    result_image = Image.blend(image_b_pillow, image_a_heatmap_pillow, alpha=0.3)\n",
    "    \n",
    "    return np.array(result_image)\n",
    "\n",
    "def get_heatmap_uninterpolated(latent_activation, input_image):\n",
    "    image_a = latent_activation.cpu().numpy()\n",
    "    image_a = (image_a - image_a.min()) / (image_a.max() - image_a.min())\n",
    "\n",
    "    input_image = (input_image - input_image.min()) / (input_image.max() - input_image.min())\n",
    "    image_b = input_image.permute(1, 2, 0).cpu().numpy()\n",
    "    \n",
    "    reshaped_image_a = np.array(Image.fromarray((image_a[0] * 255).astype('uint8')).resize((input_image.shape[-1], input_image.shape[-1]), \\\n",
    "                                                                                          resample=Image.NEAREST ))\n",
    "    normalized_heatmap = (reshaped_image_a - np.min(reshaped_image_a)) / (np.max(reshaped_image_a) - np.min(reshaped_image_a))\n",
    "    \n",
    "    heatmap_colormap = plt.get_cmap('jet')\n",
    "    heatmap_colored = heatmap_colormap(normalized_heatmap)\n",
    "    \n",
    "    heatmap_colored_uint8 = (heatmap_colored[:, :, :3] * 255).astype(np.uint8)\n",
    "    image_a_heatmap_pillow = Image.fromarray(heatmap_colored_uint8)\n",
    "    image_b_pillow = Image.fromarray((image_b * 255).astype('uint8'))\n",
    "    \n",
    "    result_image = Image.blend(image_b_pillow, image_a_heatmap_pillow, alpha=0.3)\n",
    "    \n",
    "    return np.array(result_image)\n",
    "\n",
    "from util.data import ModifiedLabelLoader\n",
    "from collections import defaultdict\n",
    "import heapq\n",
    "import pdb\n",
    "from util.vis_pipnet import get_img_coordinates\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import ImageFont, Image, ImageDraw as D\n",
    "import torchvision\n",
    "from datetime import datetime\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import math\n",
    "# txt_file = open(os.path.join(run_path, \"num_proto_details_\"+datetime.now().strftime(\"%m:%d:%H:%M:%S\")+\".txt\"), \"a\")\n",
    "# txt_file.write('\\n')\n",
    "\n",
    "vizloader_name = 'testloader' # projectloader\n",
    "find_non_descendants = False # True, False # param\n",
    "topk = 3\n",
    "save_images = True # True, False\n",
    "font = ImageFont.truetype(\"arial.ttf\", 50)\n",
    "save_activation_as_npy_path = None # 'activation_as_npy'\n",
    "if (save_activation_as_npy_path is not None):\n",
    "    save_activation_as_npy_path = '_'.join(['activation_as_npy', vizloader_name])  # activation_as_npy, added for NUMPY SAVING\n",
    "if find_non_descendants and (save_activation_as_npy_path is not None):\n",
    "    save_activation_as_npy_path = save_activation_as_npy_path + '_non_desc'\n",
    "plot_overspecificity_score = True\n",
    "subtree_root = root.get_node('024+051')\n",
    "\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "# Define the transformation\n",
    "norm_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std)\n",
    "])\n",
    "    \n",
    "from datetime import datetime\n",
    "# txt_file = open(os.path.join(run_path, \"num_proto_details_\"+datetime.now().strftime(\"%m:%d:%H:%M:%S\")+\".txt\"), \"a\")\n",
    "# txt_file.write('\\n')\n",
    "\n",
    "def write_num_proto_details(proto_mean_activations, node_name, net, threshold, txt_file, args):\n",
    "    \n",
    "    rand_input = torch.randn((1, 3, args.image_size, args.image_size))\n",
    "    with torch.no_grad():\n",
    "        *_, pooled, out = net(rand_input)\n",
    "    num_protos = pooled[node_name].shape[1]\n",
    "    used_protos = len(proto_mean_activations)\n",
    "    non_overspecific = 0\n",
    "    for p in proto_mean_activations:\n",
    "        logstr = '\\t'*2 + f'Proto:{p} '\n",
    "        protos_mean_for_all_leaf_descedants = []\n",
    "        for leaf_descendent in proto_mean_activations[p]:\n",
    "            mean_activation = round(np.mean([activation for activation, *_ in proto_mean_activations[p][leaf_descendent]]), 4)\n",
    "            protos_mean_for_all_leaf_descedants.append(mean_activation)\n",
    "            \n",
    "        if all([(mean_activation>0.2) for mean_activation in protos_mean_for_all_leaf_descedants]):\n",
    "            non_overspecific += 1\n",
    "            \n",
    "    txt_file.write(f\"Node:{node_name},Total:{num_protos},Used:{used_protos},Good:{non_overspecific},threshold={threshold}\\n\")\n",
    "\n",
    "\n",
    "def get_heap():\n",
    "    list_ = []\n",
    "    heapq.heapify(list_)\n",
    "    return list_\n",
    "\n",
    "patchsize, skip = get_patch_size(args)\n",
    "\n",
    "\n",
    "vizloader_dict = {'trainloader': trainloader,\n",
    "                 'projectloader': projectloader,\n",
    "                 'testloader': testloader,\n",
    "                 'test_projectloader': test_projectloader}\n",
    "vizloader_dict[vizloader_name] = unshuffle_dataloader(vizloader_dict[vizloader_name])\n",
    "\n",
    "\n",
    "if type(vizloader_dict[vizloader_name].dataset) == ImageFolder:\n",
    "    name2label = vizloader_dict[vizloader_name].dataset.class_to_idx\n",
    "    label2name = {label:name for name, label in name2label.items()}\n",
    "else:\n",
    "    name2label = vizloader_dict[vizloader_name].dataset.dataset.dataset.class_to_idx\n",
    "    label2name = {label:name for name, label in name2label.items()}\n",
    "    \n",
    "overspecificity_score_and_proto_mask = []\n",
    "\n",
    "for node in root.nodes_with_children():\n",
    "#     if node.name == 'root':\n",
    "#         continue\n",
    "#     non_leaf_children_names = [child.name for child in node.children if not child.is_leaf()]\n",
    "#     if len(non_leaf_children_names) == 0: # if all the children are leaf nodes then skip this node\n",
    "#         continue\n",
    "\n",
    "    if (node.name not in subtree_root.descendents) and (node.name != subtree_root.name):\n",
    "        print('Skipping node', node.name)\n",
    "        continue\n",
    "\n",
    "    name2label = vizloader_dict[vizloader_name].dataset.class_to_idx\n",
    "    label2name = {label:name for name, label in name2label.items()}\n",
    "    modifiedLabelLoader = ModifiedLabelLoader(vizloader_dict[vizloader_name], node)\n",
    "    coarse_label2name = modifiedLabelLoader.modifiedlabel2name\n",
    "    node_label_to_children = {label: name for name, label in node.children_to_labels.items()}\n",
    "    \n",
    "    imgs = modifiedLabelLoader.filtered_imgs\n",
    "\n",
    "    img_iter = tqdm(enumerate(modifiedLabelLoader),\n",
    "                    total=len(modifiedLabelLoader),\n",
    "                    mininterval=50.,\n",
    "                    desc='Collecting topk',\n",
    "                    ncols=0)\n",
    "\n",
    "    classification_weights = getattr(net.module, '_'+node.name+'_classification').weight\n",
    "    \n",
    "    # maps proto_number -> grand_child_name (or descendant leaf name) -> list of top-k activations\n",
    "    proto_mean_activations = defaultdict(lambda: defaultdict(get_heap))\n",
    "\n",
    "    # maps class names to the prototypes that belong to that\n",
    "    class_and_prototypes = defaultdict(set)\n",
    "\n",
    "    for i, (xs, orig_y, ys) in img_iter:\n",
    "#         if coarse_label2name[ys.item()] not in non_leaf_children_names:\n",
    "#             continue\n",
    "\n",
    "        xs, ys = xs.to(device), ys.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model_output = net(xs, inference=False)\n",
    "            if len(model_output) == 3:\n",
    "                softmaxes, pooled, _ = model_output\n",
    "            elif len(model_output) == 4:\n",
    "                _, softmaxes, pooled, _ = model_output\n",
    "            pooled = pooled[node.name].squeeze(0) \n",
    "            softmaxes = softmaxes[node.name]#.squeeze(0)\n",
    "\n",
    "            for p in range(pooled.shape[0]): # pooled.shape -> [768] (== num of prototypes)\n",
    "                c_weight = torch.max(classification_weights[:,p]) # classification_weights[:,p].shape -> [200] (== num of classes)\n",
    "                relevant_proto_classes = torch.nonzero(classification_weights[:, p] > 1e-3)\n",
    "                relevant_proto_class_names = [node_label_to_children[class_idx.item()] for class_idx in relevant_proto_classes]\n",
    "                \n",
    "                # Take the max per prototype.                             \n",
    "                max_per_prototype, max_idx_per_prototype = torch.max(softmaxes, dim=0)\n",
    "                max_per_prototype_h, max_idx_per_prototype_h = torch.max(max_per_prototype, dim=1)\n",
    "                max_per_prototype_w, max_idx_per_prototype_w = torch.max(max_per_prototype_h, dim=1) #shape (num_prototypes)\n",
    "                \n",
    "                h_idx = max_idx_per_prototype_h[p, max_idx_per_prototype_w[p]]\n",
    "                w_idx = max_idx_per_prototype_w[p]\n",
    "\n",
    "                if len(relevant_proto_class_names) == 0:\n",
    "                    continue\n",
    "                \n",
    "#                 if (len(relevant_proto_class_names) == 1) and (relevant_proto_class_names[0] not in non_leaf_children_names):\n",
    "#                     continue\n",
    "                \n",
    "                h_coor_min, h_coor_max, w_coor_min, w_coor_max = get_img_coordinates(args.image_size, softmaxes.shape, patchsize, skip, h_idx, w_idx)\n",
    "                latent_activation = softmaxes[:, p, :, :]\n",
    "                \n",
    "                if not find_non_descendants:\n",
    "                    if (coarse_label2name[ys.item()] in relevant_proto_class_names):\n",
    "                        child_node = root.get_node(coarse_label2name[ys.item()])\n",
    "                        leaf_descendent = label2name[orig_y.item()]#[4:7]\n",
    "                        img_to_open = imgs[i][0] # it is a tuple of (path to image, lable)\n",
    "                            # pdb.set_trace()\n",
    "                        # img_to_open = imgs[i][0] # it is a tuple of (path to image, lable)\n",
    "                        \n",
    "                        if topk and (len(proto_mean_activations[p][leaf_descendent]) >= topk):\n",
    "                            try:\n",
    "                                heapq.heappushpop(proto_mean_activations[p][leaf_descendent],\\\n",
    "                                                  (pooled[p].item(), img_to_open,\\\n",
    "                                                   (h_coor_min, h_coor_max, w_coor_min, w_coor_max), latent_activation))\n",
    "                            except:\n",
    "                                print(pooled[p].item())\n",
    "                                print(img_to_open)\n",
    "                                print(h_coor_min, h_coor_max, w_coor_min, w_coor_max)\n",
    "                                print(latent_activation.shape)\n",
    "\n",
    "                                print('\\n', p, leaf_descendent)\n",
    "                                pdb.set_trace()\n",
    "                        else:\n",
    "                            heapq.heappush(proto_mean_activations[p][leaf_descendent],\\\n",
    "                                           (pooled[p].item(), img_to_open,\\\n",
    "                                            (h_coor_min, h_coor_max, w_coor_min, w_coor_max), latent_activation))\n",
    "                else:\n",
    "                    if (coarse_label2name[ys.item()] not in relevant_proto_class_names):\n",
    "                        child_node = root.get_node(coarse_label2name[ys.item()])\n",
    "                        leaf_descendent = label2name[orig_y.item()]#[4:7]\n",
    "                        if label2name[orig_y.item()] in ['cub_053_Western_Grebe', 'cub_052_Pied_billed_Grebe', 'cub_051_Horned_Grebe']:\n",
    "                            img_to_open = image_path\n",
    "                        else:\n",
    "                            img_to_open = imgs[i][0] # it is a tuple of (path to image, lable)\n",
    "                        if topk and (len(proto_mean_activations[p][leaf_descendent]) >= topk):\n",
    "                            heapq.heappushpop(proto_mean_activations[p][leaf_descendent],\\\n",
    "                                              (pooled[p].item(), img_to_open,\\\n",
    "                                               (h_coor_min, h_coor_max, w_coor_min, w_coor_max), latent_activation))\n",
    "                        else:\n",
    "                            heapq.heappush(proto_mean_activations[p][leaf_descendent],\\\n",
    "                                           (pooled[p].item(), img_to_open,\\\n",
    "                                            (h_coor_min, h_coor_max, w_coor_min, w_coor_max), latent_activation))\n",
    "\n",
    "                class_and_prototypes[', '.join(relevant_proto_class_names)].add(p)\n",
    "\n",
    "    # write_num_proto_details(proto_mean_activations, node.name, net, threshold=0.2, txt_file=txt_file, args=args)\n",
    "\n",
    "    if plot_overspecificity_score:\n",
    "        for child_classname in class_and_prototypes:\n",
    "            for p in class_and_prototypes[child_classname]:\n",
    "                mean_activation_of_every_leaf = []\n",
    "                for leaf_descendent in proto_mean_activations[p]:\n",
    "                    mean_activation = round(np.mean([activation for activation, *_ in proto_mean_activations[p][leaf_descendent]]), 4)\n",
    "                    mean_activation_of_every_leaf.append(mean_activation)\n",
    "\n",
    "                overspecificity_score = 1\n",
    "                for mean_act in mean_activation_of_every_leaf:\n",
    "                    overspecificity_score *= mean_act * 1.0\n",
    "                proto_presence = getattr(net.module, '_'+node.name+'_proto_presence')\n",
    "                proto_presence = F.gumbel_softmax(proto_presence, tau=0.5, hard=True, dim=-1)\n",
    "                proto_mask = proto_presence[p, 1].item()\n",
    "                overspecificity_score_and_proto_mask.append((overspecificity_score, len(mean_activation_of_every_leaf), proto_mask))\n",
    "\n",
    "    print('Node', node.name)\n",
    "    for child_classname in class_and_prototypes:\n",
    "        \n",
    "        print('\\t'*1, 'Child:', child_classname)\n",
    "        for p in class_and_prototypes[child_classname]:\n",
    "            \n",
    "            logstr = '\\t'*2 + f'Proto:{p} '\n",
    "            mean_activation_of_every_leaf = []\n",
    "            for leaf_descendent in proto_mean_activations[p]:\n",
    "                mean_activation = round(np.mean([activation for activation, *_ in proto_mean_activations[p][leaf_descendent]]), 4)\n",
    "                num_images = len(proto_mean_activations[p][leaf_descendent])\n",
    "                logstr += f'{leaf_descendent}:({mean_activation}) '\n",
    "                mean_activation_of_every_leaf.append(mean_activation)\n",
    "            print(logstr)\n",
    "            \n",
    "            # # if the mean_activation is less for all leaf descendants skip the node\n",
    "            # if all([mean_act < 0.2 for mean_act in mean_activation_of_every_leaf]):\n",
    "            #     if find_non_descendants:\n",
    "            #         print('\\t'*2 + f'Not skipping proto {p} of {node.name} coz of find_non_descendants')\n",
    "            #     else:\n",
    "            #         print('\\t'*2 + f'Skipping proto {p} of {node.name}')\n",
    "            #         continue\n",
    "            \n",
    "            # have this for NON descendants\n",
    "            if len(proto_mean_activations[p]) == 0:\n",
    "                continue\n",
    "            \n",
    "            if save_images or save_activation_as_npy_path:\n",
    "                patches = []\n",
    "                right_descriptions = []\n",
    "                text_region_width = 3 # 3x the width of a patch\n",
    "\n",
    "                font_size = 40\n",
    "                fnt = ImageFont.truetype(\"arial.ttf\", font_size)\n",
    "                max_width = ImageDraw.Draw(Image.new(\"RGB\", (100, 100), (255, 0, 0))).textlength('-', font=fnt)\n",
    "                \n",
    "                for leaf_descendent in proto_mean_activations[p]:\n",
    "                    for word in leaf_descendent.split('_')[2:]:\n",
    "                        width_of_word = ImageDraw.Draw(Image.new(\"RGB\", (100, 100), (255, 0, 0))).textlength(word, font=fnt)\n",
    "                        max_width = max(max_width, width_of_word)\n",
    "\n",
    "                for leaf_descendent, heap in proto_mean_activations[p].items():\n",
    "                    if 'BUT' in args.dataset:\n",
    "                        species_name = ' '.join(leaf_descendent.split('_')[2:4])\n",
    "                    else:\n",
    "                        species_name = ' '.join(leaf_descendent.split('_')[2:])\n",
    "                    heap = sorted(heap)[::-1]\n",
    "                    mean_activation = round(np.mean([activation for activation, *_ in proto_mean_activations[p][leaf_descendent]]), 4)\n",
    "                    for rank, ele in enumerate(heap):\n",
    "                        activation, img_to_open, (h_coor_min, h_coor_max, w_coor_min, w_coor_max), latent_activation = ele\n",
    "                        image = transforms.Resize(size=(args.image_size, args.image_size))(Image.open(img_to_open))\n",
    "                        img_tensor = transforms.ToTensor()(image)#.unsqueeze_(0) #shape (1, 3, h, w)\n",
    "\n",
    "                        overlayed_image_np = get_heatmap(latent_activation, img_tensor, constant_color_scale=False)\n",
    "                        overlayed_image = torch.tensor(overlayed_image_np).permute(2, 0, 1).float() / 255.\n",
    "                        \n",
    "                        reshaped_latent_activation = np.array(Image.fromarray((latent_activation.cpu().numpy()[0] * 255).astype('uint8')).resize((img_tensor.shape[-1], img_tensor.shape[-1])))\n",
    "                        center = np.unravel_index(np.argmax(reshaped_latent_activation), reshaped_latent_activation.shape)\n",
    "                        # center = ((h_coor_min + h_coor_max) / 2., (w_coor_min + w_coor_max) / 2.)\n",
    "                        patch_size = 64\n",
    "                        h_coor_min = int(max(0, center[0] - (patch_size/2.)))\n",
    "                        h_coor_max = int(min(img_tensor.shape[1], center[0] + (patch_size/2.)))\n",
    "                        w_coor_min = int(max(0, center[1] - (patch_size/2.)))\n",
    "                        w_coor_max = int(min(img_tensor.shape[2], center[1] + (patch_size/2.)))\n",
    "                        img_tensor_patch = img_tensor[:, h_coor_min:h_coor_max, w_coor_min:w_coor_max]\n",
    "\n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                        # overlayed_image = img_tensor\n",
    "\n",
    "                        \n",
    "\n",
    "                        scale_factor = 1.7  # 70% increase\n",
    "\n",
    "                        heatmap_patch = overlayed_image[:, h_coor_min:h_coor_max, w_coor_min:w_coor_max]\n",
    "                        resized_heatmap_patch = F.interpolate(heatmap_patch.unsqueeze(0), scale_factor=scale_factor, \\\n",
    "                                                      mode='bilinear', align_corners=False).squeeze(0)\n",
    "                        resized_heatmap_patch = torchvision.utils.draw_bounding_boxes((resized_heatmap_patch * 255).to(torch.uint8), \\\n",
    "                                                                                torch.tensor([[0, 0, resized_heatmap_patch.shape[2], resized_heatmap_patch.shape[1]]]), \\\n",
    "                                                                                width=4, colors=(255, 0, 0))\n",
    "                        resized_heatmap_patch = resized_heatmap_patch.float() / 255.\n",
    "                        \n",
    "                        resized_img_patch = F.interpolate(img_tensor_patch.unsqueeze(0), scale_factor=scale_factor, \\\n",
    "                                                      mode='bilinear', align_corners=False).squeeze(0)\n",
    "                        resized_img_patch = torchvision.utils.draw_bounding_boxes((resized_img_patch * 255).to(torch.uint8), \\\n",
    "                                                                                torch.tensor([[0, 0, resized_img_patch.shape[2], resized_img_patch.shape[1]]]), \\\n",
    "                                                                                width=4, colors=(255, 255, 0))\n",
    "                        resized_img_patch = resized_img_patch.float() / 255.\n",
    "                        \n",
    "                        resized_patch = torchvision.utils.make_grid([resized_img_patch, resized_heatmap_patch], nrow=1, padding=1, pad_value=1., border=1)\n",
    "                        white_image = torch.ones(3, img_tensor.shape[1], img_tensor.shape[2])\n",
    "                        patch_height = resized_patch.shape[1]\n",
    "                        y_start = (white_image.shape[1] - patch_height) // 2                        \n",
    "                        x_start = 10  # 10 pixels from the left\n",
    "                        white_image[:, y_start:y_start+patch_height, x_start:x_start+resized_patch.shape[2]] = resized_patch\n",
    "\n",
    "                        # Bounding box on original image\n",
    "                        img_tensor = torchvision.utils.draw_bounding_boxes((img_tensor * 255).to(torch.uint8), \\\n",
    "                                                                                torch.tensor([[w_coor_min, h_coor_min, w_coor_max, h_coor_max]]), \\\n",
    "                                                                                width=2, colors=(255, 255, 0))\n",
    "                        img_tensor = img_tensor.float() / 255.\n",
    "\n",
    "                        # Bounding box on overlayed image\n",
    "                        overlayed_image = torchvision.utils.draw_bounding_boxes((overlayed_image * 255).to(torch.uint8), \\\n",
    "                                                                                torch.tensor([[w_coor_min, h_coor_min, w_coor_max, h_coor_max]]), \\\n",
    "                                                                                width=2, colors=(255, 0, 0))\n",
    "                        overlayed_image = overlayed_image.float() / 255.\n",
    "\n",
    "                        grid_cell = torchvision.utils.make_grid([overlayed_image, img_tensor, white_image], nrow=3, padding=5, pad_value=1., border=1)\n",
    "\n",
    "                        patches.append(grid_cell)\n",
    "                        \n",
    "                        # added for NUMPY SAVING\n",
    "                        \n",
    "                        if save_activation_as_npy_path:\n",
    "#                             upscaled_similarity_interpolated = get_upscaled_activation_interpolated(latent_activation,\n",
    "#                                                                                        image_size=(args.image_size, args.image_size))\n",
    "                            latent_activation_npy = latent_activation.squeeze().cpu().numpy()\n",
    "                            data = {'node_name': node.name,\n",
    "                                    'proto_num': p,\n",
    "                                    'child_name': child_classname,\n",
    "                                    'leaf_desc': leaf_descendent,\n",
    "                                     'rank': rank,\n",
    "                                     'img_path': img_to_open,\n",
    "                                     'img_filename': ntpath.basename(img_to_open),\n",
    "                                     'activation': latent_activation_npy,\n",
    "                                     'max_activation': activation,\n",
    "                                     'model_type': 'NAIVE-HPIPNET'}\n",
    "                            filename = str(rank)+ '-' + ntpath.basename(img_to_open) + '.npy'\n",
    "                            save_path = os.path.join(run_path, save_activation_as_npy_path, \\\n",
    "                                                     node.name, str(p), leaf_descendent,\n",
    "                                                     filename)\n",
    "                            os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "                            np.save(save_path, data, allow_pickle=True)\n",
    "\n",
    "                    # # description on the right hand side\n",
    "                    # text = f'{mean_activation}, {leaf_descendent}'\n",
    "                    # txtimage = Image.new(\"RGB\", (patches[0].shape[-2]*text_region_width,patches[0].shape[-1]), (255, 255, 255))\n",
    "                    # draw = D.Draw(txtimage)\n",
    "                    # draw.text((150, patches[0].shape[1]//2), text, anchor='mm', fill=\"black\", font=font)\n",
    "                    # pdb.set_trace()\n",
    "                    # txttensor = transforms.ToTensor()(txtimage)#.unsqueeze_(0)\n",
    "                    # right_descriptions.append(txttensor)\n",
    "\n",
    "                    text = '\\n'.join(species_name.split(' '))\n",
    "                    image_size = (math.ceil(max_width) + 10, patches[0].shape[1])\n",
    "                    txtimage = Image.new(\"RGB\", image_size, (255, 255, 255))\n",
    "                    d = ImageDraw.Draw(txtimage)\n",
    "                    d.multiline_text((image_size[0]/2, image_size[1]/2), text, font=fnt, fill=(0, 0, 0), align =\"center\", anchor=\"mm\")\n",
    "                    txttensor = transforms.ToTensor()(txtimage)#.unsqueeze_(0)\n",
    "                    right_descriptions.append(txttensor)\n",
    "                    \n",
    "\n",
    "                padding = 0\n",
    "\n",
    "                # grid = torchvision.utils.make_grid(patches, nrow=topk, padding=padding, border=0)\n",
    "                # grid_right_descriptions = torchvision.utils.make_grid(right_descriptions, nrow=1, padding=padding, border=0)\n",
    "                # grid = torch.cat([grid_right_descriptions, grid], dim=-1)\n",
    "\n",
    "                grid_rows = []\n",
    "                for k in range(len(proto_mean_activations[p])):\n",
    "                    try:\n",
    "                        grid_row = torchvision.utils.make_grid(patches[k*topk:(k+1)*topk], nrow=topk, padding=padding, border=0)\n",
    "                    except:\n",
    "                        pdb.set_trace()\n",
    "                    grid_right_description = torchvision.utils.make_grid(right_descriptions[k], nrow=1, padding=padding, border=0)\n",
    "                    # pdb.set_trace()\n",
    "                    grid_row = torch.cat([grid_right_description, grid_row], dim=-1)\n",
    "                    grid_rows.append(grid_row)\n",
    "                # grid = torch.cat(grid_rows, dim=0)\n",
    "                grid = torchvision.utils.make_grid(grid_rows, nrow=1, padding=5, pad_value=1.)\n",
    "                    \n",
    "                # # description on the top\n",
    "                # text = f'Node:{node.name}, p{p}, Child:{child_classname}'\n",
    "                # txtimage = Image.new(\"RGB\", (grid.shape[-1], args.wshape), (0, 0, 0))\n",
    "                # draw = D.Draw(txtimage)\n",
    "                # draw.text((150, patches[0].shape[1]//2), text, anchor='mm', fill=\"white\", font=font)\n",
    "                # txttensor = transforms.ToTensor()(txtimage)#.unsqueeze_(0)\n",
    "\n",
    "                # merging top description with the grid of images\n",
    "                # grid = torch.cat([grid, txttensor], dim=1)\n",
    "                \n",
    "                if save_images:\n",
    "                    prefix = 'non_' if find_non_descendants else ''\n",
    "                    os.makedirs(os.path.join(run_path, prefix+f'descendent_specific_topk={topk}_heatmap_withbb_ep={epoch}_{subtree_root.name}', node.name), exist_ok=True)\n",
    "                    torchvision.utils.save_image(grid, os.path.join(run_path, prefix+f'descendent_specific_topk={topk}_heatmap_withbb_ep={epoch}_{subtree_root.name}', node.name, f'{child_classname}-p{p}.png'), border=0) # , border_color=(255, 255, 255), border=10\n",
    "\n",
    "# txt_file.write('\\n')\n",
    "# txt_file.close()\n",
    "print('Done !!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_indices = []\n",
    "for i in range(len(testloader.dataset)):\n",
    "    temp = testloader.dataset[i]\n",
    "    break\n",
    "    # if label in idx_of_classes_to_keep:\n",
    "    #     target_indices.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp[0].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
