{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0594237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heatmaps showing where a prototype is found will not be generated because OpenCV is not installed.\n"
     ]
    }
   ],
   "source": [
    "from pipnet.pipnet import PIPNet, get_network\n",
    "from util.log import Log\n",
    "import torch.nn as nn\n",
    "from util.args import get_args, save_args, get_optimizer_nn\n",
    "from util.data import get_dataloaders\n",
    "from util.func import init_weights_xavier\n",
    "from pipnet.train import train_pipnet, test_pipnet\n",
    "# from pipnet.test import eval_pipnet, get_thresholds, eval_ood\n",
    "from util.eval_cub_csv import eval_prototypes_cub_parts_csv, get_topk_cub, get_proto_patches_cub\n",
    "import torch\n",
    "from util.vis_pipnet import visualize, visualize_topk\n",
    "from util.visualize_prediction import vis_pred, vis_pred_experiments\n",
    "import sys, os\n",
    "import random\n",
    "import numpy as np\n",
    "from shutil import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "from util.node import Node\n",
    "import shutil\n",
    "from util.phylo_utils import construct_phylo_tree, construct_discretized_phylo_tree\n",
    "import pickle\n",
    "from util.func import get_patch_size\n",
    "import random\n",
    "from util.data import ModifiedLabelLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a92291c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------- No discretization -------------------------\n"
     ]
    }
   ],
   "source": [
    "# run_path = '/home/harishbabu/projects/PIPNet/runs/010-CUB-27-imgnet_OOD_cnext26_img=224_nprotos=20'\n",
    "# run_path = '/home/harishbabu/projects/PIPNet/runs/031-CUB-18-imgnet_cnext26_img=224_nprotos=20_orth-on-rel'\n",
    "# run_path = '/home/harishbabu/projects/PIPNet/runs/032-CUB-18-imgnet_cnext26_img=224_nprotos=20_orth-on-rel'\n",
    "# run_path = '/home/harishbabu/projects/PIPNet/runs/035-CUB-18-imgnet_OOD_cnext26_img=224_nprotos=20_orth-on-rel'\n",
    "# run_path = '/home/harishbabu/projects/PIPNet/runs/043-035_clone-CUB-18-imgnet_OOD_cnext26_img=224_nprotos=20_orth-on-rel'\n",
    "# run_path = \"/home/harishbabu/projects/PIPNet/runs/036-CUB-18-imgnet_OOD_cnext26_img=224_nprotos=20_orth-on-rel_uniformity\"\n",
    "# run_path = \"/home/harishbabu/projects/PIPNet/runs/041-035_clone-CUB-18-imgnet_OOD_cnext26_img=224_nprotos=20_orth-on-rel\"\n",
    "# run_path = \"/home/harishbabu/projects/PIPNet/runs/042-035_clone-CUB-18-imgnet_OOD_cnext26_img=224_nprotos=20_orth-on-rel\"\n",
    "run_path = \"/home/harishbabu/projects/PIPNet/runs/044-CUB-18-imgnet_OOD_cnext26_img=224_nprotos=20-or-4per-desc_orth-on-rel\"\n",
    "# run_path = \"/home/harishbabu/projects/PIPNet/runs/046-CUB-18-imgnet_OOD_cnext26_img=224_nprotos=10per-desc_orth-on-rel\"\n",
    "args_file = open(os.path.join(run_path, 'metadata', 'args.pickle'), 'rb')\n",
    "args = pickle.load(args_file)\n",
    "\n",
    "if args.phylo_config:\n",
    "    phylo_config = OmegaConf.load(args.phylo_config)\n",
    "\n",
    "if args.phylo_config:\n",
    "    # construct the phylo tree\n",
    "    if phylo_config.phyloDistances_string == 'None':\n",
    "        if '031' in run_path: # this run uses a different phylogeny file that had an extra root node which is a mistake\n",
    "            root = construct_phylo_tree('/home/harishbabu/data/phlyogenyCUB/18Species-with-extra-root-node/1_tree-consensus-Hacket-18Species-modified_cub-names_v1.phy')\n",
    "        else:\n",
    "            root = construct_phylo_tree(phylo_config.phylogeny_path)\n",
    "        print('-'*25 + ' No discretization ' + '-'*25)\n",
    "    else:\n",
    "        root = construct_discretized_phylo_tree(phylo_config.phylogeny_path, phylo_config.phyloDistances_string)\n",
    "        print('-'*25 + ' Discretized ' + '-'*25)\n",
    "else:\n",
    "    # construct the tree (original hierarchy as described in the paper)\n",
    "    root = Node(\"root\")\n",
    "    root.add_children(['animal','vehicle','everyday_object','weapon','scuba_diver'])\n",
    "    root.add_children_to('animal',['non_primate','primate'])\n",
    "    root.add_children_to('non_primate',['African_elephant','giant_panda','lion'])\n",
    "    root.add_children_to('primate',['capuchin','gibbon','orangutan'])\n",
    "    root.add_children_to('vehicle',['ambulance','pickup','sports_car'])\n",
    "    root.add_children_to('everyday_object',['laptop','sandal','wine_bottle'])\n",
    "    root.add_children_to('weapon',['assault_rifle','rifle'])\n",
    "    # flat root\n",
    "    # root.add_children(['scuba_diver','African_elephant','giant_panda','lion','capuchin','gibbon','orangutan','ambulance','pickup','sports_car','laptop','sandal','wine_bottle','assault_rifle','rifle'])\n",
    "root.assign_all_descendents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36de564b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      "\t052+053\n",
      "\t\tcub_052_Pied_billed_Grebe\n",
      "\t\t053+050\n",
      "\t\t\tcub_053_Western_Grebe\n",
      "\t\t\t050+051\n",
      "\t\t\t\tcub_050_Eared_Grebe\n",
      "\t\t\t\tcub_051_Horned_Grebe\n",
      "\t004+086\n",
      "\t\t004+032\n",
      "\t\t\tcub_004_Groove_billed_Ani\n",
      "\t\t\t032+033\n",
      "\t\t\t\tcub_032_Mangrove_Cuckoo\n",
      "\t\t\t\t033+031\n",
      "\t\t\t\t\tcub_033_Yellow_billed_Cuckoo\n",
      "\t\t\t\t\tcub_031_Black_billed_Cuckoo\n",
      "\t\t086+045\n",
      "\t\t\tcub_086_Pacific_Loon\n",
      "\t\t\t045+101\n",
      "\t\t\t\t045+003\n",
      "\t\t\t\t\tcub_045_Northern_Fulmar\n",
      "\t\t\t\t\t003+002\n",
      "\t\t\t\t\t\tcub_003_Sooty_Albatross\n",
      "\t\t\t\t\t\t002+001\n",
      "\t\t\t\t\t\t\tcub_002_Laysan_Albatross\n",
      "\t\t\t\t\t\t\tcub_001_Black_footed_Albatross\n",
      "\t\t\t\t101+023\n",
      "\t\t\t\t\t101+100\n",
      "\t\t\t\t\t\tcub_101_White_Pelican\n",
      "\t\t\t\t\t\tcub_100_Brown_Pelican\n",
      "\t\t\t\t\t023+025\n",
      "\t\t\t\t\t\tcub_023_Brandt_Cormorant\n",
      "\t\t\t\t\t\t025+024\n",
      "\t\t\t\t\t\t\tcub_025_Pelagic_Cormorant\n",
      "\t\t\t\t\t\t\tcub_024_Red_faced_Cormorant\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7d7356f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num classes (k) =  18 ['cub_001_Black_footed_Albatross', 'cub_002_Laysan_Albatross', 'cub_003_Sooty_Albatross', 'cub_004_Groove_billed_Ani', 'cub_023_Brandt_Cormorant'] etc.\n",
      "Classes:  {'cub_001_Black_footed_Albatross': 0, 'cub_002_Laysan_Albatross': 1, 'cub_003_Sooty_Albatross': 2, 'cub_004_Groove_billed_Ani': 3, 'cub_023_Brandt_Cormorant': 4, 'cub_024_Red_faced_Cormorant': 5, 'cub_025_Pelagic_Cormorant': 6, 'cub_031_Black_billed_Cuckoo': 7, 'cub_032_Mangrove_Cuckoo': 8, 'cub_033_Yellow_billed_Cuckoo': 9, 'cub_045_Northern_Fulmar': 10, 'cub_050_Eared_Grebe': 11, 'cub_051_Horned_Grebe': 12, 'cub_052_Pied_billed_Grebe': 13, 'cub_053_Western_Grebe': 14, 'cub_086_Pacific_Loon': 15, 'cub_100_Brown_Pelican': 16, 'cub_101_White_Pelican': 17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harishbabu/.conda/envs/hpnet1/lib/python3.8/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of prototypes:  20\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Namespace' object has no attribute 'num_protos_per_descendant'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 31\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClasses: \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mstr\u001b[39m(classes), flush\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Create a convolutional network based on arguments and add 1x1 conv layer\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m feature_net, add_on_layers, pool_layer, classification_layers, num_prototypes \u001b[38;5;241m=\u001b[39m \u001b[43mget_network\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mclasses\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mroot\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Create a PIP-Net\u001b[39;00m\n\u001b[1;32m     34\u001b[0m net \u001b[38;5;241m=\u001b[39m PIPNet(num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(classes),\n\u001b[1;32m     35\u001b[0m                     num_prototypes\u001b[38;5;241m=\u001b[39mnum_prototypes,\n\u001b[1;32m     36\u001b[0m                     feature_net \u001b[38;5;241m=\u001b[39m feature_net,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     42\u001b[0m                     root \u001b[38;5;241m=\u001b[39m root\n\u001b[1;32m     43\u001b[0m                     )\n",
      "File \u001b[0;32m~/projects/PIPNet/pipnet/pipnet.py:200\u001b[0m, in \u001b[0;36mget_network\u001b[0;34m(num_classes, args, root)\u001b[0m\n\u001b[1;32m    197\u001b[0m add_on_layers \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# prototypes_per_descendant = args.num_protos_per_descendant\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# change 0 to num_prototypes for having minimum num of protos at any node\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[38;5;28mprint\u001b[39m((\u001b[38;5;241m10\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPrototypes per descendant: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;241m.\u001b[39mnum_protos_per_descendant\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m(\u001b[38;5;241m10\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m parent_nodes:\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;66;03m# add_on_layers[node.name] = nn.Conv2d(in_channels=first_add_on_layer_in_channels, out_channels=max(20, (node.num_descendents() * prototypes_per_descendant)), \\\u001b[39;00m\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;66;03m#                                      kernel_size=1, stride = 1, padding=0, bias=True)\u001b[39;00m\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;66;03m# proto_count = max(0, (node.num_descendents() * prototypes_per_descendant))\u001b[39;00m\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;66;03m# print(f'Assigned {proto_count} protos to node {node.name}')\u001b[39;00m\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m child_node \u001b[38;5;129;01min\u001b[39;00m node\u001b[38;5;241m.\u001b[39mchildren:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Namespace' object has no attribute 'num_protos_per_descendant'"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    device_ids = [torch.cuda.current_device()]\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    device_ids = []\n",
    "\n",
    "args_file = open(os.path.join(run_path, 'metadata', 'args.pickle'), 'rb')\n",
    "args = pickle.load(args_file)\n",
    "\n",
    "ckpt_file_name = 'net_trained_last'\n",
    "# ckpt_file_name = 'net_trained_10'\n",
    "# ckpt_file_name = 'net_pretrained'\n",
    "epoch = ckpt_file_name.split('_')[-1]\n",
    "\n",
    "ckpt_path = os.path.join(run_path, 'checkpoints', ckpt_file_name)\n",
    "checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "\n",
    "if ckpt_file_name != 'net_trained_last':\n",
    "    print('\\n', (10*'-')+'WARNING: Not using the final trained model'+(10*'-'), '\\n')\n",
    "\n",
    "# Obtain the dataset and dataloaders\n",
    "trainloader, trainloader_pretraining, trainloader_normal, trainloader_normal_augment, projectloader, testloader, test_projectloader, classes = get_dataloaders(args, device)\n",
    "if len(classes)<=20:\n",
    "    if args.validation_size == 0.:\n",
    "        print(\"Classes: \", testloader.dataset.class_to_idx, flush=True)\n",
    "    else:\n",
    "        print(\"Classes: \", str(classes), flush=True)\n",
    "\n",
    "# Create a convolutional network based on arguments and add 1x1 conv layer\n",
    "feature_net, add_on_layers, pool_layer, classification_layers, num_prototypes = get_network(len(classes), args, root=root)\n",
    "   \n",
    "# Create a PIP-Net\n",
    "net = PIPNet(num_classes=len(classes),\n",
    "                    num_prototypes=num_prototypes,\n",
    "                    feature_net = feature_net,\n",
    "                    args = args,\n",
    "                    add_on_layers = add_on_layers,\n",
    "                    pool_layer = pool_layer,\n",
    "                    classification_layers = classification_layers,\n",
    "                    num_parent_nodes = len(root.nodes_with_children()),\n",
    "                    root = root\n",
    "                    )\n",
    "net = net.to(device=device)\n",
    "net = nn.DataParallel(net, device_ids = device_ids)    \n",
    "net.load_state_dict(checkpoint['model_state_dict'],strict=True)\n",
    "net.eval()\n",
    "criterion = nn.NLLLoss(reduction='mean').to(device)\n",
    "\n",
    "# Forward one batch through the backbone to get the latent output size\n",
    "# with torch.no_grad():\n",
    "#     xs1, _, _ = next(iter(trainloader))\n",
    "#     xs1 = xs1.to(device)\n",
    "#     proto_features, _, _ = net(xs1)\n",
    "#     wshape = proto_features['root'].shape[-1]\n",
    "#     args.wshape = wshape #needed for calculating image patch size\n",
    "#     print(\"Output shape: \", proto_features['root'].shape, flush=True)\n",
    "    \n",
    "args.wshape = 26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4b1184e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chosen network is convnext\n",
      "Num classes (k) =  172 ['cub_005_Crested_Auklet', 'cub_006_Least_Auklet', 'cub_007_Parakeet_Auklet', 'cub_008_Rhinoceros_Auklet', 'cub_009_Brewer_Blackbird'] etc.\n",
      "-------------------------Using OOD data-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Epochlast: 100% 8/8 [00:13<00:00,  1.72s/it, L:8.440,LC:0.118, LA:0.02, LT:0.012, L_OOD:0.738, L_ORTH:0.001]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tFine accuracy: 0.84\n",
      "\tNode name: root, acc: 99.02, f1:99.02, samples: 1022, 052+053=236/240=0.98, 004+086=776/782=0.99\n",
      "\tNode name: 052+053, acc: 99.17, f1:99.17, samples: 240, cub_052_Pied_billed_Grebe=60/60=1.0, 053+050=178/180=0.99\n",
      "\tNode name: 004+086, acc: 98.47, f1:98.47, samples: 782, 004+032=218/224=0.97, 086+045=552/558=0.99\n",
      "\tNode name: 053+050, acc: 98.89, f1:98.88, samples: 180, cub_053_Western_Grebe=58/60=0.97, 050+051=120/120=1.0\n",
      "\tNode name: 004+032, acc: 94.64, f1:94.78, samples: 224, cub_004_Groove_billed_Ani=60/60=1.0, 032+033=152/164=0.93\n",
      "\tNode name: 086+045, acc: 97.85, f1:97.85, samples: 558, cub_086_Pacific_Loon=54/60=0.9, 045+101=492/498=0.99\n",
      "\tNode name: 050+051, acc: 83.33, f1:83.33, samples: 120, cub_050_Eared_Grebe=50/60=0.83, cub_051_Horned_Grebe=50/60=0.83\n",
      "\tNode name: 032+033, acc: 92.68, f1:92.77, samples: 164, cub_032_Mangrove_Cuckoo=42/46=0.91, 033+031=110/118=0.93\n",
      "\tNode name: 045+101, acc: 98.39, f1:98.39, samples: 498, 045+003=230/236=0.97, 101+023=260/262=0.99\n",
      "\tNode name: 033+031, acc: 88.14, f1:88.13, samples: 118, cub_033_Yellow_billed_Cuckoo=50/58=0.86, cub_031_Black_billed_Cuckoo=54/60=0.9\n",
      "\tNode name: 045+003, acc: 96.61, f1:96.65, samples: 236, cub_045_Northern_Fulmar=58/60=0.97, 003+002=170/176=0.97\n",
      "\tNode name: 101+023, acc: 97.71, f1:97.71, samples: 262, 101+100=96/100=0.96, 023+025=160/162=0.99\n",
      "\tNode name: 003+002, acc: 90.91, f1:91.11, samples: 176, cub_003_Sooty_Albatross=54/56=0.96, 002+001=106/120=0.88\n",
      "\tNode name: 101+100, acc: 94.0, f1:93.97, samples: 100, cub_101_White_Pelican=36/40=0.9, cub_100_Brown_Pelican=58/60=0.97\n",
      "\tNode name: 023+025, acc: 83.95, f1:83.56, samples: 162, cub_023_Brandt_Cormorant=40/58=0.69, 025+024=96/104=0.92\n",
      "\tNode name: 002+001, acc: 93.33, f1:93.33, samples: 120, cub_002_Laysan_Albatross=56/60=0.93, cub_001_Black_footed_Albatross=56/60=0.93\n",
      "\tNode name: 025+024, acc: 92.31, f1:92.31, samples: 104, cub_025_Pelagic_Cormorant=56/60=0.93, cub_024_Red_faced_Cormorant=40/44=0.91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer_net, optimizer_classifier, params_to_freeze, params_to_train, params_backbone = get_optimizer_nn(net, args)\n",
    "scheduler_net = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_net, T_max=len(trainloader_pretraining)*args.epochs_pretrain, eta_min=args.lr_block/100., last_epoch=-1)\n",
    "\n",
    "if args.epochs<=30:\n",
    "    scheduler_classifier = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer_classifier, T_0=5, eta_min=0.001, T_mult=1, verbose=False)\n",
    "else:\n",
    "    scheduler_classifier = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer_classifier, T_0=10, eta_min=0.001, T_mult=1, verbose=False)\n",
    "finetune = False\n",
    "\n",
    "if args.OOD_dataset:\n",
    "    trainloader_OOD, trainloader_pretraining_OOD, trainloader_normal_OOD, trainloader_normal_augment_OOD, projectloader_OOD, testloader_OOD, test_projectloader_OOD, _ = get_dataloaders(args, device, OOD=True)\n",
    "    print('-'*25 + 'Using OOD data' + '-'*25)\n",
    "else:\n",
    "    trainloader_OOD = trainloader_pretraining_OOD = trainloader_normal_OOD = trainloader_normal_augment_OOD = projectloader_OOD = testloader_OOD = test_projectloader_OOD = None\n",
    "    print('-'*25 + 'Not using OOD data' + '-'*25)\n",
    "\n",
    "test_info, log_dict = test_pipnet(net, testloader, optimizer_net, optimizer_classifier, \\\n",
    "                                  scheduler_net, scheduler_classifier, criterion, epoch, \\\n",
    "                                    args.epochs, device, pretrain=False, finetune=finetune, \\\n",
    "                                    test_loader_OOD=testloader_OOD, kernel_orth=args.kernel_orth == 'y', \\\n",
    "                                        wandb_run=None, pretrain_epochs=args.epochs_pretrain, log=None, wandb_logging=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09030676",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 540it [00:23, 22.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node root\n",
      "\t Child: 052+053\n",
      "OVERSPECIFIC \t\tProto:0 050:(0.3723) 051:(0.7658) 052:(0.0123) 053:(0.0155) \n",
      "OVERSPECIFIC \t\tProto:128 050:(0.2763) 051:(0.2181) 052:(0.0174) 053:(0.3347) \n",
      "OVERSPECIFIC \t\tProto:4 050:(0.2687) 051:(0.5082) 052:(0.0011) 053:(0.611) \n",
      "OVERSPECIFIC \t\tProto:7 050:(0.1752) 051:(0.7296) 052:(0.0082) 053:(0.5884) \n",
      "OVERSPECIFIC \t\tProto:15 050:(0.5338) 051:(0.0025) 052:(0.8759) 053:(0.0795) \n",
      "OVERSPECIFIC \t\tProto:34 050:(0.2063) 051:(0.3641) 052:(0.0377) 053:(0.7544) \n",
      "OVERSPECIFIC \t\tProto:36 050:(0.0453) 051:(0.6006) 052:(0.0024) 053:(0.9617) \n",
      "OVERSPECIFIC \t\tProto:42 050:(0.4489) 051:(0.4824) 052:(0.7552) 053:(0.0216) \n",
      "OVERSPECIFIC \t\tProto:43 050:(0.0287) 051:(0.0283) 052:(0.0284) 053:(0.9671) \n",
      "OVERSPECIFIC \t\tProto:44 050:(0.0125) 051:(0.0113) 052:(0.9011) 053:(0.0253) \n",
      "OVERSPECIFIC \t\tProto:49 050:(0.3413) 051:(0.862) 052:(0.0011) 053:(0.8667) \n",
      "OVERSPECIFIC \t\tProto:70 050:(0.0916) 051:(0.6271) 052:(0.003) 053:(0.9891) \n",
      "OVERSPECIFIC \t\tProto:72 050:(0.6494) 051:(0.3755) 052:(0.1923) 053:(0.524) \n",
      "OVERSPECIFIC \t\tProto:79 050:(0.3805) 051:(0.5412) 052:(0.0211) 053:(0.0024) \n",
      "OVERSPECIFIC \t\tProto:81 050:(0.1693) 051:(0.6493) 052:(0.0013) 053:(0.0033) \n",
      "OVERSPECIFIC \t\tProto:96 050:(0.0876) 051:(0.7198) 052:(0.0055) 053:(0.9496) \n",
      "OVERSPECIFIC \t\tProto:111 050:(0.0467) 051:(0.264) 052:(0.8635) 053:(0.0252) \n",
      "OVERSPECIFIC \t\tProto:112 050:(0.3874) 051:(0.3102) 052:(0.6192) 053:(0.0015) \n",
      "OVERSPECIFIC \t\tProto:117 050:(0.7415) 051:(0.593) 052:(0.0011) 053:(0.6112) \n",
      "OVERSPECIFIC \t\tProto:123 050:(0.0629) 051:(0.0073) 052:(0.7525) 053:(0.0073) \n",
      "\t Child: 004+086\n",
      "OVERSPECIFIC \t\tProto:1 001:(0.1089) 002:(0.4405) 003:(0.0056) 004:(0.001) 023:(0.0287) 024:(0.1193) 025:(0.0025) 031:(0.001) 032:(0.0009) 033:(0.001) 045:(0.0012) 086:(0.0049) 100:(0.9867) 101:(0.9888) \n",
      "OVERSPECIFIC \t\tProto:130 001:(0.0701) 002:(0.2826) 003:(0.6609) 004:(0.0339) 023:(0.0725) 024:(0.284) 025:(0.0281) 031:(0.0021) 032:(0.0021) 033:(0.008) 045:(0.2649) 086:(0.0961) 100:(0.3728) 101:(0.0937) \n",
      "OVERSPECIFIC \t\tProto:131 001:(0.0132) 002:(0.0013) 003:(0.0106) 004:(0.009) 023:(0.9928) 024:(0.9895) 025:(0.9994) 031:(0.0011) 032:(0.0011) 033:(0.0011) 045:(0.0025) 086:(0.1952) 100:(0.7577) 101:(0.0652) \n",
      "OVERSPECIFIC \t\tProto:132 001:(0.3049) 002:(0.0038) 003:(0.0053) 004:(0.4401) 023:(0.2096) 024:(0.0013) 025:(0.0044) 031:(0.0013) 032:(0.0016) 033:(0.0014) 045:(0.003) 086:(0.874) 100:(0.0139) 101:(0.0024) \n",
      "OVERSPECIFIC \t\tProto:5 001:(0.0042) 002:(0.0145) 003:(0.0037) 004:(0.0049) 023:(0.0015) 024:(0.0023) 025:(0.0015) 031:(0.0015) 032:(0.0017) 033:(0.0045) 045:(0.0023) 086:(0.0067) 100:(0.9089) 101:(0.9917) \n",
      "OVERSPECIFIC \t\tProto:6 001:(0.6906) 002:(0.1033) 003:(0.1372) 004:(0.0052) 023:(0.225) 024:(0.0106) 025:(0.0105) 031:(0.0032) 032:(0.0009) 033:(0.0043) 045:(0.0501) 086:(0.9452) 100:(0.0069) 101:(0.0026) \n",
      "OVERSPECIFIC \t\tProto:133 001:(0.0226) 002:(0.3944) 003:(0.0526) 004:(0.0016) 023:(0.0488) 024:(0.9909) 025:(0.0019) 031:(0.0016) 032:(0.0016) 033:(0.0024) 045:(0.009) 086:(0.0075) 100:(0.0019) 101:(0.9759) \n",
      "OVERSPECIFIC \t\tProto:11 001:(0.1905) 002:(0.9656) 003:(0.8766) 004:(0.0027) 023:(0.002) 024:(0.0024) 025:(0.0017) 031:(0.0015) 032:(0.0013) 033:(0.0017) 045:(0.933) 086:(0.0061) 100:(0.0048) 101:(0.0114) \n",
      "OVERSPECIFIC \t\tProto:12 001:(0.0398) 002:(0.0647) 003:(0.4696) 004:(0.2302) 023:(0.095) 024:(0.2429) 025:(0.7233) 031:(0.0128) 032:(0.0292) 033:(0.0634) 045:(0.0545) 086:(0.0497) 100:(0.0252) 101:(0.0059) \n",
      "OVERSPECIFIC \t\tProto:14 001:(0.724) 002:(0.5156) 003:(0.0836) 004:(0.0015) 023:(0.0637) 024:(0.4049) 025:(0.0029) 031:(0.0012) 032:(0.0012) 033:(0.0017) 045:(0.061) 086:(0.0162) 100:(0.2245) 101:(0.2845) \n",
      "OVERSPECIFIC \t\tProto:18 001:(0.0139) 002:(0.0027) 003:(0.0017) 004:(0.0133) 023:(0.0778) 024:(0.1651) 025:(0.1377) 031:(0.9417) 032:(0.0077) 033:(0.4284) 045:(0.0016) 086:(0.2208) 100:(0.0593) 101:(0.0029) \n",
      "OVERSPECIFIC \t\tProto:20 001:(0.0577) 002:(0.0491) 003:(0.0156) 004:(0.0288) 023:(0.1364) 024:(0.1283) 025:(0.0907) 031:(0.0613) 032:(0.0959) 033:(0.2385) 045:(0.0288) 086:(0.1303) 100:(0.7852) 101:(0.0214) \n",
      "OVERSPECIFIC \t\tProto:25 001:(0.0073) 002:(0.0083) 003:(0.004) 004:(0.8939) 023:(0.2256) 024:(0.0103) 025:(0.3337) 031:(0.0041) 032:(0.0035) 033:(0.0117) 045:(0.0078) 086:(0.0546) 100:(0.0136) 101:(0.0034) \n",
      "OVERSPECIFIC \t\tProto:26 001:(0.0396) 002:(0.005) 003:(0.0979) 004:(0.9512) 023:(0.0154) 024:(0.0037) 025:(0.107) 031:(0.0195) 032:(0.6283) 033:(0.2539) 045:(0.2678) 086:(0.0204) 100:(0.0015) 101:(0.0253) \n",
      "OVERSPECIFIC \t\tProto:29 001:(0.1013) 002:(0.0013) 003:(0.0051) 004:(0.0011) 023:(0.9408) 024:(0.9841) 025:(0.9583) 031:(0.0011) 032:(0.0016) 033:(0.0011) 045:(0.0023) 086:(0.0182) 100:(0.004) 101:(0.0011) \n",
      "OVERSPECIFIC \t\tProto:31 001:(0.0112) 002:(0.0024) 003:(0.0264) 004:(0.0019) 023:(0.8927) 024:(0.2352) 025:(0.9442) 031:(0.0007) 032:(0.0007) 033:(0.0008) 045:(0.0051) 086:(0.8595) 100:(0.1166) 101:(0.0397) \n",
      "OVERSPECIFIC \t\tProto:32 001:(0.0654) 002:(0.1885) 003:(0.041) 004:(0.0022) 023:(0.0843) 024:(0.0062) 025:(0.0034) 031:(0.0016) 032:(0.0016) 033:(0.0097) 045:(0.0216) 086:(0.0066) 100:(0.8247) 101:(0.9718) \n",
      "OVERSPECIFIC \t\tProto:35 001:(0.0159) 002:(0.0122) 003:(0.002) 004:(0.0446) 023:(0.3944) 024:(0.0435) 025:(0.4933) 031:(0.0014) 032:(0.0015) 033:(0.0015) 045:(0.0052) 086:(0.491) 100:(0.1454) 101:(0.134) \n",
      "OVERSPECIFIC \t\tProto:37 001:(0.7274) 002:(0.649) 003:(0.9731) 004:(0.0015) 023:(0.0055) 024:(0.0014) 025:(0.0016) 031:(0.0011) 032:(0.0011) 033:(0.0017) 045:(0.5265) 086:(0.0028) 100:(0.0177) 101:(0.004) \n",
      "OVERSPECIFIC \t\tProto:39 001:(0.0487) 002:(0.0021) 003:(0.0045) 004:(0.0088) 023:(0.6318) 024:(0.9721) 025:(0.498) 031:(0.0017) 032:(0.0017) 033:(0.0017) 045:(0.0046) 086:(0.014) 100:(0.0151) 101:(0.0049) \n",
      "OVERSPECIFIC \t\tProto:40 001:(0.138) 002:(0.0993) 003:(0.031) 004:(0.0078) 023:(0.9737) 024:(0.0186) 025:(0.4297) 031:(0.0376) 032:(0.0315) 033:(0.0526) 045:(0.0597) 086:(0.6121) 100:(0.0083) 101:(0.0008) \n",
      "OVERSPECIFIC \t\tProto:45 001:(0.0303) 002:(0.1935) 003:(0.0096) 004:(0.0027) 023:(0.0016) 024:(0.0014) 025:(0.0013) 031:(0.9914) 032:(0.0089) 033:(0.8327) 045:(0.0096) 086:(0.0916) 100:(0.009) 101:(0.0016) \n",
      "OVERSPECIFIC \t\tProto:47 001:(0.3088) 002:(0.2688) 003:(0.0073) 004:(0.0015) 023:(0.0018) 024:(0.0016) 025:(0.0016) 031:(0.8769) 032:(0.0015) 033:(0.8964) 045:(0.0068) 086:(0.407) 100:(0.0015) 101:(0.0032) \n",
      "OVERSPECIFIC \t\tProto:50 001:(0.0221) 002:(0.9485) 003:(0.0343) 004:(0.0011) 023:(0.0012) 024:(0.0012) 025:(0.001) 031:(0.0011) 032:(0.001) 033:(0.0027) 045:(0.941) 086:(0.0107) 100:(0.0014) 101:(0.0011) \n",
      "OVERSPECIFIC \t\tProto:53 001:(0.0016) 002:(0.1282) 003:(0.1543) 004:(0.0037) 023:(0.0015) 024:(0.0051) 025:(0.001) 031:(0.0024) 032:(0.0138) 033:(0.0124) 045:(0.1983) 086:(0.0875) 100:(0.7825) 101:(0.9991) \n",
      "OVERSPECIFIC \t\tProto:56 001:(0.006) 002:(0.0388) 003:(0.0062) 004:(0.0055) 023:(0.0054) 024:(0.006) 025:(0.0055) 031:(0.8454) 032:(0.0056) 033:(0.7445) 045:(0.0074) 086:(0.017) 100:(0.0055) 101:(0.0082) \n",
      "OVERSPECIFIC \t\tProto:59 001:(0.959) 002:(0.5877) 003:(0.0798) 004:(0.0016) 023:(0.1614) 024:(0.0018) 025:(0.0024) 031:(0.0026) 032:(0.0016) 033:(0.0019) 045:(0.6285) 086:(0.347) 100:(0.0017) 101:(0.0016) \n",
      "OVERSPECIFIC \t\tProto:60 001:(0.1) 002:(0.5907) 003:(0.0856) 004:(0.0011) 023:(0.0106) 024:(0.0024) 025:(0.058) 031:(0.0011) 032:(0.2097) 033:(0.8043) 045:(0.0213) 086:(0.0336) 100:(0.0011) 101:(0.062) \n",
      "OVERSPECIFIC \t\tProto:61 001:(0.0108) 002:(0.0047) 003:(0.002) 004:(0.9592) 023:(0.0045) 024:(0.9883) 025:(0.0661) 031:(0.0012) 032:(0.0012) 033:(0.0016) 045:(0.0109) 086:(0.0028) 100:(0.0012) 101:(0.0012) \n",
      "OVERSPECIFIC \t\tProto:63 001:(0.8561) 002:(0.2905) 003:(0.9716) 004:(0.0014) 023:(0.1635) 024:(0.0044) 025:(0.0142) 031:(0.0014) 032:(0.0011) 033:(0.0517) 045:(0.607) 086:(0.5269) 100:(0.0063) 101:(0.0667) \n",
      "OVERSPECIFIC \t\tProto:67 001:(0.412) 002:(0.3944) 003:(0.4037) 004:(0.0247) 023:(0.1142) 024:(0.6526) 025:(0.0306) 031:(0.0025) 032:(0.0022) 033:(0.0467) 045:(0.0253) 086:(0.1191) 100:(0.8457) 101:(0.1894) \n",
      "OVERSPECIFIC \t\tProto:71 001:(0.0018) 002:(0.0024) 003:(0.0042) 004:(0.0018) 023:(0.0027) 024:(0.0018) 025:(0.0029) 031:(0.7857) 032:(0.8503) 033:(0.8817) 045:(0.0021) 086:(0.0316) 100:(0.0018) 101:(0.0018) \n",
      "OVERSPECIFIC \t\tProto:74 001:(0.9847) 002:(0.9841) 003:(0.9741) 004:(0.0047) 023:(0.0617) 024:(0.0104) 025:(0.0767) 031:(0.0029) 032:(0.0009) 033:(0.0085) 045:(0.3593) 086:(0.0445) 100:(0.0072) 101:(0.0101) \n",
      "OVERSPECIFIC \t\tProto:75 001:(0.0076) 002:(0.0014) 003:(0.0011) 004:(0.6976) 023:(0.9546) 024:(0.4869) 025:(0.9619) 031:(0.0013) 032:(0.0249) 033:(0.0079) 045:(0.0024) 086:(0.1189) 100:(0.005) 101:(0.0021) \n",
      "OVERSPECIFIC \t\tProto:76 001:(0.0027) 002:(0.0021) 003:(0.0022) 004:(0.0021) 023:(0.0022) 024:(0.0021) 025:(0.0021) 031:(0.2919) 032:(0.9225) 033:(0.2133) 045:(0.0021) 086:(0.0021) 100:(0.0021) 101:(0.0021) \n",
      "OVERSPECIFIC \t\tProto:77 001:(0.0221) 002:(0.0279) 003:(0.9805) 004:(0.0009) 023:(0.0018) 024:(0.0019) 025:(0.0034) 031:(0.8135) 032:(0.0013) 033:(0.0858) 045:(0.0056) 086:(0.0291) 100:(0.0488) 101:(0.0053) \n",
      "OVERSPECIFIC \t\tProto:78 001:(0.7514) 002:(0.0096) 003:(0.016) 004:(0.0009) 023:(0.0558) 024:(0.9913) 025:(0.0838) 031:(0.0021) 032:(0.0009) 033:(0.0021) 045:(0.0173) 086:(0.0616) 100:(0.001) 101:(0.001) \n",
      "OVERSPECIFIC \t\tProto:80 001:(0.093) 002:(0.0025) 003:(0.006) 004:(0.0013) 023:(0.037) 024:(0.1374) 025:(0.021) 031:(0.001) 032:(0.0025) 033:(0.0034) 045:(0.0723) 086:(0.0017) 100:(0.9161) 101:(0.945) \n",
      "OVERSPECIFIC \t\tProto:82 001:(0.0369) 002:(0.002) 003:(0.0351) 004:(0.0446) 023:(0.8172) 024:(0.9768) 025:(0.8758) 031:(0.0017) 032:(0.0017) 033:(0.0017) 045:(0.0021) 086:(0.0561) 100:(0.0343) 101:(0.002) \n",
      "OVERSPECIFIC \t\tProto:85 001:(0.0475) 002:(0.0354) 003:(0.0019) 004:(0.0011) 023:(0.0065) 024:(0.018) 025:(0.0021) 031:(0.001) 032:(0.001) 033:(0.0013) 045:(0.0017) 086:(0.0055) 100:(0.9946) 101:(0.8474) \n",
      "OVERSPECIFIC \t\tProto:86 001:(0.7027) 002:(0.3058) 003:(0.3534) 004:(0.2021) 023:(0.3857) 024:(0.1843) 025:(0.7178) 031:(0.0642) 032:(0.1329) 033:(0.0273) 045:(0.5679) 086:(0.2001) 100:(0.0036) 101:(0.0734) \n",
      "OVERSPECIFIC \t\tProto:88 001:(0.0072) 002:(0.2067) 003:(0.0073) 004:(0.0073) 023:(0.0074) 024:(0.0072) 025:(0.0072) 031:(0.9948) 032:(0.0074) 033:(0.9165) 045:(0.0072) 086:(0.0929) 100:(0.0078) 101:(0.0193) \n",
      "OVERSPECIFIC \t\tProto:90 001:(0.9042) 002:(0.0606) 003:(0.2225) 004:(0.0734) 023:(0.5029) 024:(0.0145) 025:(0.048) 031:(0.0024) 032:(0.0023) 033:(0.0045) 045:(0.1629) 086:(0.9492) 100:(0.0205) 101:(0.0071) \n",
      "OVERSPECIFIC \t\tProto:93 001:(0.9793) 002:(0.9762) 003:(0.0369) 004:(0.0009) 023:(0.0013) 024:(0.0011) 025:(0.0015) 031:(0.0022) 032:(0.0009) 033:(0.0011) 045:(0.8979) 086:(0.0009) 100:(0.0138) 101:(0.0024) \n",
      "OVERSPECIFIC \t\tProto:100 001:(0.0441) 002:(0.0223) 003:(0.0051) 004:(0.0015) 023:(0.0607) 024:(0.4442) 025:(0.1979) 031:(0.1058) 032:(0.508) 033:(0.6764) 045:(0.01) 086:(0.0392) 100:(0.0021) 101:(0.0174) \n",
      "OVERSPECIFIC \t\tProto:101 001:(0.0123) 002:(0.0157) 003:(0.0179) 004:(0.0426) 023:(0.0035) 024:(0.0252) 025:(0.0228) 031:(0.6205) 032:(0.4766) 033:(0.8292) 045:(0.0029) 086:(0.0032) 100:(0.0166) 101:(0.0218) \n",
      "OVERSPECIFIC \t\tProto:106 001:(0.0234) 002:(0.003) 003:(0.005) 004:(0.0021) 023:(0.1597) 024:(0.0046) 025:(0.0183) 031:(0.3259) 032:(0.899) 033:(0.0462) 045:(0.0184) 086:(0.3088) 100:(0.0014) 101:(0.0014) \n",
      "OVERSPECIFIC \t\tProto:109 001:(0.8532) 002:(0.3836) 003:(0.5894) 004:(0.0023) 023:(0.3597) 024:(0.2411) 025:(0.1902) 031:(0.1532) 032:(0.025) 033:(0.0365) 045:(0.0767) 086:(0.0098) 100:(0.0174) 101:(0.0331) \n",
      "OVERSPECIFIC \t\tProto:110 001:(0.0932) 002:(0.1332) 003:(0.0679) 004:(0.0092) 023:(0.1276) 024:(0.1567) 025:(0.0025) 031:(0.0015) 032:(0.0015) 033:(0.0025) 045:(0.0115) 086:(0.0083) 100:(0.7956) 101:(0.8628) \n",
      "OVERSPECIFIC \t\tProto:116 001:(0.717) 002:(0.4057) 003:(0.0811) 004:(0.015) 023:(0.0703) 024:(0.3896) 025:(0.0625) 031:(0.0214) 032:(0.0405) 033:(0.0318) 045:(0.292) 086:(0.064) 100:(0.1515) 101:(0.9443) \n",
      "OVERSPECIFIC \t\tProto:118 001:(0.1897) 002:(0.1794) 003:(0.2491) 004:(0.0094) 023:(0.7607) 024:(0.0225) 025:(0.7589) 031:(0.1711) 032:(0.0589) 033:(0.0842) 045:(0.1905) 086:(0.5462) 100:(0.0366) 101:(0.0067) \n",
      "OVERSPECIFIC \t\tProto:124 001:(0.0011) 002:(0.0012) 003:(0.0011) 004:(0.0012) 023:(0.0012) 024:(0.0011) 025:(0.0011) 031:(0.5533) 032:(0.9516) 033:(0.8473) 045:(0.0011) 086:(0.0011) 100:(0.0012) 101:(0.0011) \n",
      "OVERSPECIFIC \t\tProto:125 001:(0.2118) 002:(0.2882) 003:(0.0393) 004:(0.0014) 023:(0.1814) 024:(0.0026) 025:(0.0014) 031:(0.0014) 032:(0.0032) 033:(0.0019) 045:(0.0062) 086:(0.0078) 100:(0.9779) 101:(0.679) \n",
      "OVERSPECIFIC \t\tProto:127 001:(0.1672) 002:(0.3166) 003:(0.2194) 004:(0.0707) 023:(0.6062) 024:(0.365) 025:(0.866) 031:(0.0033) 032:(0.0273) 033:(0.0025) 045:(0.094) 086:(0.1699) 100:(0.1198) 101:(0.0212) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 120it [00:02, 40.71it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 052+053\n",
      "\t Child: 053+050\n",
      "SHARED \t\tProto:3 050:(0.7745) 051:(0.8589) 053:(0.9539) \n",
      "OVERSPECIFIC \t\tProto:4 050:(0.4065) 051:(0.8929) 053:(0.9978) \n",
      "OVERSPECIFIC \t\tProto:9 050:(0.7372) 051:(0.9643) 053:(0.4975) \n",
      "SHARED \t\tProto:12 050:(0.8935) 051:(0.7556) 053:(0.9578) \n",
      "SHARED \t\tProto:13 050:(0.796) 051:(0.8591) 053:(0.8746) \n",
      "SHARED \t\tProto:16 050:(0.7435) 051:(0.8951) 053:(0.8183) \n",
      "OVERSPECIFIC \t\tProto:17 050:(0.1952) 051:(0.4513) 053:(0.9874) \n",
      "OVERSPECIFIC \t\tProto:19 050:(0.4399) 051:(0.9609) 053:(0.8205) \n",
      "SHARED \t\tProto:21 050:(0.6943) 051:(0.9255) 053:(0.9824) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 420it [00:14, 28.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 004+086\n",
      "\t Child: 086+045\n",
      "OVERSPECIFIC \t\tProto:0 001:(0.7419) 002:(0.8428) 003:(0.0661) 023:(0.0023) 024:(0.0022) 025:(0.0023) 045:(0.8792) 086:(0.0028) 100:(0.0023) 101:(0.0041) \n",
      "OVERSPECIFIC \t\tProto:1 001:(0.0394) 002:(0.0155) 003:(0.063) 023:(0.9403) 024:(0.8906) 025:(0.997) 045:(0.0051) 086:(0.5115) 100:(0.2262) 101:(0.0401) \n",
      "OVERSPECIFIC \t\tProto:5 001:(0.3691) 002:(0.724) 003:(0.6323) 023:(0.0441) 024:(0.0777) 025:(0.0075) 045:(0.002) 086:(0.0542) 100:(0.8595) 101:(0.5693) \n",
      "OVERSPECIFIC \t\tProto:10 001:(0.3339) 002:(0.4376) 003:(0.3306) 023:(0.6726) 024:(0.0813) 025:(0.3724) 045:(0.0378) 086:(0.3658) 100:(0.0471) 101:(0.0018) \n",
      "OVERSPECIFIC \t\tProto:11 001:(0.3273) 002:(0.0041) 003:(0.0136) 023:(0.9386) 024:(0.0702) 025:(0.3309) 045:(0.0354) 086:(0.5285) 100:(0.0091) 101:(0.0015) \n",
      "OVERSPECIFIC \t\tProto:17 001:(0.9544) 002:(0.9707) 003:(0.2135) 023:(0.1767) 024:(0.0125) 025:(0.0049) 045:(0.0402) 086:(0.1349) 100:(0.9247) 101:(0.9685) \n",
      "OVERSPECIFIC \t\tProto:18 001:(0.521) 002:(0.3962) 003:(0.3431) 023:(0.0978) 024:(0.0457) 025:(0.0066) 045:(0.0449) 086:(0.0017) 100:(0.427) 101:(0.3117) \n",
      "OVERSPECIFIC \t\tProto:20 001:(0.8733) 002:(0.4059) 003:(0.6519) 023:(0.7823) 024:(0.1046) 025:(0.419) 045:(0.0894) 086:(0.5507) 100:(0.5557) 101:(0.048) \n",
      "OVERSPECIFIC \t\tProto:28 001:(0.015) 002:(0.0063) 003:(0.0057) 023:(0.0075) 024:(0.5626) 025:(0.0545) 045:(0.0025) 086:(0.0046) 100:(0.9223) 101:(0.9918) \n",
      "OVERSPECIFIC \t\tProto:30 001:(0.7274) 002:(0.0879) 003:(0.5952) 023:(0.0092) 024:(0.8717) 025:(0.0576) 045:(0.2693) 086:(0.0495) 100:(0.0036) 101:(0.0018) \n",
      "OVERSPECIFIC \t\tProto:32 001:(0.2321) 002:(0.0299) 003:(0.1084) 023:(0.7558) 024:(0.6047) 025:(0.6853) 045:(0.0145) 086:(0.3688) 100:(0.5053) 101:(0.1737) \n",
      "OVERSPECIFIC \t\tProto:34 001:(0.072) 002:(0.0056) 003:(0.0403) 023:(0.9844) 024:(0.9423) 025:(0.9916) 045:(0.0077) 086:(0.9525) 100:(0.1623) 101:(0.0924) \n",
      "OVERSPECIFIC \t\tProto:36 001:(0.0025) 002:(0.0025) 003:(0.0024) 023:(0.0258) 024:(0.9979) 025:(0.0055) 045:(0.0083) 086:(0.0028) 100:(0.7169) 101:(0.9667) \n",
      "OVERSPECIFIC \t\tProto:37 001:(0.4008) 002:(0.1781) 003:(0.5812) 023:(0.3179) 024:(0.8923) 025:(0.2643) 045:(0.3178) 086:(0.1354) 100:(0.4652) 101:(0.081) \n",
      "OVERSPECIFIC \t\tProto:39 001:(0.2228) 002:(0.2463) 003:(0.0066) 023:(0.3312) 024:(0.9991) 025:(0.1218) 045:(0.0249) 086:(0.6859) 100:(0.0247) 101:(0.006) \n",
      "OVERSPECIFIC \t\tProto:43 001:(0.4451) 002:(0.3753) 003:(0.6632) 023:(0.0033) 024:(0.0009) 025:(0.0012) 045:(0.5711) 086:(0.2037) 100:(0.0013) 101:(0.0015) \n",
      "OVERSPECIFIC \t\tProto:44 001:(0.8141) 002:(0.2414) 003:(0.284) 023:(0.0034) 024:(0.0218) 025:(0.0078) 045:(0.0488) 086:(0.0822) 100:(0.8868) 101:(0.2304) \n",
      "OVERSPECIFIC \t\tProto:45 001:(0.0741) 002:(0.3971) 003:(0.0162) 023:(0.0324) 024:(0.0066) 025:(0.0029) 045:(0.0015) 086:(0.0066) 100:(0.9955) 101:(0.9892) \n",
      "OVERSPECIFIC \t\tProto:48 001:(0.0105) 002:(0.9479) 003:(0.0809) 023:(0.0021) 024:(0.0018) 025:(0.0019) 045:(0.9773) 086:(0.0025) 100:(0.1668) 101:(0.5661) \n",
      "OVERSPECIFIC \t\tProto:50 001:(0.9862) 002:(0.2634) 003:(0.7581) 023:(0.0335) 024:(0.0052) 025:(0.0271) 045:(0.028) 086:(0.7358) 100:(0.1655) 101:(0.1161) \n",
      "OVERSPECIFIC \t\tProto:54 001:(0.6926) 002:(0.8209) 003:(0.983) 023:(0.0549) 024:(0.0215) 025:(0.0137) 045:(0.3805) 086:(0.0024) 100:(0.0083) 101:(0.1894) \n",
      "OVERSPECIFIC \t\tProto:59 001:(0.0186) 002:(0.2575) 003:(0.1243) 023:(0.004) 024:(0.01) 025:(0.0473) 045:(0.8396) 086:(0.0063) 100:(0.0434) 101:(0.993) \n",
      "OVERSPECIFIC \t\tProto:62 001:(0.8321) 002:(0.6189) 003:(0.0663) 023:(0.1995) 024:(0.0105) 025:(0.1552) 045:(0.4959) 086:(0.2328) 100:(0.0027) 101:(0.0026) \n",
      "OVERSPECIFIC \t\tProto:67 001:(0.419) 002:(0.1765) 003:(0.0464) 023:(0.115) 024:(0.957) 025:(0.403) 045:(0.1318) 086:(0.4701) 100:(0.0044) 101:(0.0031) \n",
      "OVERSPECIFIC \t\tProto:70 001:(0.3118) 002:(0.0232) 003:(0.0793) 023:(0.149) 024:(0.9378) 025:(0.1836) 045:(0.0254) 086:(0.0041) 100:(0.0067) 101:(0.0322) \n",
      "OVERSPECIFIC \t\tProto:77 001:(0.0012) 002:(0.0018) 003:(0.004) 023:(0.6918) 024:(0.9917) 025:(0.954) 045:(0.0051) 086:(0.1272) 100:(0.3403) 101:(0.3741) \n",
      "OVERSPECIFIC \t\tProto:78 001:(0.0445) 002:(0.9843) 003:(0.9561) 023:(0.0025) 024:(0.0012) 025:(0.0048) 045:(0.8104) 086:(0.0012) 100:(0.004) 101:(0.0082) \n",
      "OVERSPECIFIC \t\tProto:82 001:(0.1254) 002:(0.8191) 003:(0.0753) 023:(0.0026) 024:(0.003) 025:(0.0274) 045:(0.0632) 086:(0.0023) 100:(0.9412) 101:(0.3796) \n",
      "OVERSPECIFIC \t\tProto:83 001:(0.093) 002:(0.1751) 003:(0.1974) 023:(0.0431) 024:(0.051) 025:(0.2559) 045:(0.2111) 086:(0.4105) 100:(0.3897) 101:(0.7603) \n",
      "OVERSPECIFIC \t\tProto:86 001:(0.0098) 002:(0.0022) 003:(0.1469) 023:(0.8825) 024:(0.7975) 025:(0.9669) 045:(0.0886) 086:(0.1152) 100:(0.2964) 101:(0.0016) \n",
      "OVERSPECIFIC \t\tProto:88 001:(0.3574) 002:(0.4043) 003:(0.0828) 023:(0.0021) 024:(0.9591) 025:(0.0014) 045:(0.0055) 086:(0.0018) 100:(0.2161) 101:(0.13) \n",
      "OVERSPECIFIC \t\tProto:92 001:(0.3295) 002:(0.7531) 003:(0.4256) 023:(0.0064) 024:(0.0036) 025:(0.004) 045:(0.2215) 086:(0.119) 100:(0.5729) 101:(0.6617) \n",
      "OVERSPECIFIC \t\tProto:95 001:(0.1956) 002:(0.0045) 003:(0.0637) 023:(0.9976) 024:(0.5933) 025:(0.9663) 045:(0.0026) 086:(0.9568) 100:(0.0744) 101:(0.0033) \n",
      "OVERSPECIFIC \t\tProto:99 001:(0.9835) 002:(0.013) 003:(0.2032) 023:(0.0807) 024:(0.0029) 025:(0.0113) 045:(0.0255) 086:(0.2278) 100:(0.1066) 101:(0.0068) \n",
      "OVERSPECIFIC \t\tProto:101 001:(0.317) 002:(0.0824) 003:(0.9449) 023:(0.2999) 024:(0.009) 025:(0.0107) 045:(0.7688) 086:(0.5478) 100:(0.0104) 101:(0.0048) \n",
      "\t Child: 004+032\n",
      "OVERSPECIFIC \t\tProto:3 004:(0.0023) 031:(0.9758) 032:(0.1344) 033:(0.6945) \n",
      "OVERSPECIFIC \t\tProto:35 004:(0.0029) 031:(0.433) 032:(0.9707) 033:(0.823) \n",
      "OVERSPECIFIC \t\tProto:100 004:(0.0022) 031:(0.2279) 032:(0.9139) 033:(0.6303) \n",
      "OVERSPECIFIC \t\tProto:9 004:(0.9283) 031:(0.9855) 032:(0.005) 033:(0.8291) \n",
      "OVERSPECIFIC \t\tProto:75 004:(0.0013) 031:(0.5605) 032:(0.9714) 033:(0.6806) \n",
      "OVERSPECIFIC \t\tProto:21 004:(0.7229) 031:(0.5821) 032:(0.0018) 033:(0.384) \n",
      "OVERSPECIFIC \t\tProto:22 004:(0.9986) 031:(0.0116) 032:(0.0114) 033:(0.0113) \n",
      "OVERSPECIFIC \t\tProto:23 004:(0.9606) 031:(0.011) 032:(0.0109) 033:(0.0108) \n",
      "OVERSPECIFIC \t\tProto:58 004:(0.0026) 031:(0.9207) 032:(0.025) 033:(0.8247) \n",
      "OVERSPECIFIC \t\tProto:60 004:(0.1137) 031:(0.0901) 032:(0.6922) 033:(0.9081) \n",
      "OVERSPECIFIC \t\tProto:61 004:(0.0017) 031:(0.1266) 032:(0.9614) 033:(0.953) \n",
      "OVERSPECIFIC \t\tProto:94 004:(0.9832) 031:(0.087) 032:(0.0542) 033:(0.0276) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 90it [00:02, 33.82it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 053+050\n",
      "\t Child: 050+051\n",
      "OVERSPECIFIC \t\tProto:0 050:(0.4518) 051:(0.6865) \n",
      "SHARED \t\tProto:3 050:(0.8887) 051:(0.9972) \n",
      "OVERSPECIFIC \t\tProto:4 050:(0.262) 051:(0.7437) \n",
      "SHARED \t\tProto:6 050:(0.9723) 051:(0.9659) \n",
      "SHARED \t\tProto:8 050:(0.9204) 051:(0.9509) \n",
      "OVERSPECIFIC \t\tProto:16 050:(0.3581) 051:(0.8679) \n",
      "SHARED \t\tProto:18 050:(0.8088) 051:(0.9333) \n",
      "SHARED \t\tProto:19 050:(0.9359) 051:(0.8258) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 120it [00:02, 41.50it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 004+032\n",
      "\t Child: 032+033\n",
      "SHARED \t\tProto:0 031:(0.9532) 032:(0.8901) 033:(0.9248) \n",
      "SHARED \t\tProto:4 031:(0.9077) 032:(0.9834) 033:(0.8925) \n",
      "OVERSPECIFIC \t\tProto:6 031:(0.2572) 032:(0.7965) 033:(0.9404) \n",
      "SHARED \t\tProto:8 031:(0.8281) 032:(0.9778) 033:(0.9613) \n",
      "OVERSPECIFIC \t\tProto:12 031:(0.9947) 032:(0.4127) 033:(0.9744) \n",
      "SHARED \t\tProto:14 031:(0.9221) 032:(0.5728) 033:(0.8774) \n",
      "OVERSPECIFIC \t\tProto:18 031:(0.9827) 032:(0.0461) 033:(0.9177) \n",
      "SHARED \t\tProto:19 031:(0.9988) 032:(0.9965) 033:(0.9594) \n",
      "SHARED \t\tProto:22 031:(0.5851) 032:(0.9314) 033:(0.9901) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 300it [00:08, 33.38it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 086+045\n",
      "\t Child: 045+101\n",
      "OVERSPECIFIC \t\tProto:1 001:(0.6688) 002:(0.3588) 003:(0.0524) 023:(0.1526) 024:(0.0522) 025:(0.0331) 045:(0.0111) 100:(0.6325) 101:(0.7598) \n",
      "OVERSPECIFIC \t\tProto:2 001:(0.1888) 002:(0.4347) 003:(0.1293) 023:(0.0092) 024:(0.078) 025:(0.0019) 045:(0.0067) 100:(0.9549) 101:(0.7545) \n",
      "OVERSPECIFIC \t\tProto:3 001:(0.008) 002:(0.1964) 003:(0.205) 023:(0.007) 024:(0.4859) 025:(0.0017) 045:(0.0934) 100:(0.7563) 101:(0.8894) \n",
      "OVERSPECIFIC \t\tProto:5 001:(0.1554) 002:(0.0238) 003:(0.0047) 023:(0.9979) 024:(0.9153) 025:(0.7618) 045:(0.0442) 100:(0.0129) 101:(0.0009) \n",
      "OVERSPECIFIC \t\tProto:7 001:(0.1286) 002:(0.0809) 003:(0.0355) 023:(0.4822) 024:(0.991) 025:(0.8978) 045:(0.0243) 100:(0.0013) 101:(0.0086) \n",
      "OVERSPECIFIC \t\tProto:10 001:(0.077) 002:(0.9076) 003:(0.9405) 023:(0.0007) 024:(0.0014) 025:(0.0008) 045:(0.8228) 100:(0.0063) 101:(0.0089) \n",
      "OVERSPECIFIC \t\tProto:12 001:(0.0043) 002:(0.001) 003:(0.0015) 023:(0.8548) 024:(0.9895) 025:(0.9846) 045:(0.0011) 100:(0.001) 101:(0.0008) \n",
      "OVERSPECIFIC \t\tProto:13 001:(0.3276) 002:(0.1167) 003:(0.1509) 023:(0.9763) 024:(0.6949) 025:(0.9723) 045:(0.0218) 100:(0.119) 101:(0.0033) \n",
      "OVERSPECIFIC \t\tProto:14 001:(0.3225) 002:(0.0746) 003:(0.1253) 023:(0.8066) 024:(0.9374) 025:(0.8301) 045:(0.0141) 100:(0.358) 101:(0.0456) \n",
      "OVERSPECIFIC \t\tProto:15 001:(0.01) 002:(0.0022) 003:(0.0032) 023:(0.221) 024:(0.0156) 025:(0.0048) 045:(0.0019) 100:(0.8232) 101:(0.9791) \n",
      "OVERSPECIFIC \t\tProto:16 001:(0.9064) 002:(0.0674) 003:(0.0668) 023:(0.0066) 024:(0.0087) 025:(0.0027) 045:(0.388) 100:(0.02) 101:(0.7231) \n",
      "OVERSPECIFIC \t\tProto:17 001:(0.9843) 002:(0.5501) 003:(0.8238) 023:(0.3559) 024:(0.5031) 025:(0.7867) 045:(0.5532) 100:(0.0065) 101:(0.0032) \n",
      "OVERSPECIFIC \t\tProto:19 001:(0.0238) 002:(0.005) 003:(0.0043) 023:(0.9646) 024:(0.6984) 025:(0.9982) 045:(0.011) 100:(0.0957) 101:(0.1335) \n",
      "OVERSPECIFIC \t\tProto:20 001:(0.5151) 002:(0.5786) 003:(0.0354) 023:(0.0032) 024:(0.0292) 025:(0.0019) 045:(0.0011) 100:(0.9237) 101:(0.9114) \n",
      "OVERSPECIFIC \t\tProto:21 001:(0.3874) 002:(0.0109) 003:(0.0456) 023:(0.8497) 024:(0.9229) 025:(0.9162) 045:(0.0124) 100:(0.0217) 101:(0.0025) \n",
      "OVERSPECIFIC \t\tProto:26 001:(0.0057) 002:(0.0271) 003:(0.0142) 023:(0.0563) 024:(0.1976) 025:(0.0383) 045:(0.0155) 100:(0.3577) 101:(0.9016) \n",
      "OVERSPECIFIC \t\tProto:27 001:(0.4044) 002:(0.5695) 003:(0.3848) 023:(0.1351) 024:(0.9367) 025:(0.2363) 045:(0.0205) 100:(0.0122) 101:(0.0045) \n",
      "OVERSPECIFIC \t\tProto:28 001:(0.3553) 002:(0.0723) 003:(0.2668) 023:(0.1225) 024:(0.797) 025:(0.1144) 045:(0.0496) 100:(0.0433) 101:(0.1507) \n",
      "OVERSPECIFIC \t\tProto:30 001:(0.7289) 002:(0.725) 003:(0.8496) 023:(0.0031) 024:(0.0021) 025:(0.003) 045:(0.4166) 100:(0.0159) 101:(0.0033) \n",
      "OVERSPECIFIC \t\tProto:31 001:(0.0159) 002:(0.0736) 003:(0.0029) 023:(0.0049) 024:(0.0066) 025:(0.0044) 045:(0.0008) 100:(0.9949) 101:(0.985) \n",
      "OVERSPECIFIC \t\tProto:32 001:(0.8499) 002:(0.1423) 003:(0.9355) 023:(0.19) 024:(0.0855) 025:(0.2025) 045:(0.9169) 100:(0.0727) 101:(0.1032) \n",
      "OVERSPECIFIC \t\tProto:33 001:(0.9788) 002:(0.4275) 003:(0.8518) 023:(0.343) 024:(0.0129) 025:(0.0055) 045:(0.0534) 100:(0.0414) 101:(0.0498) \n",
      "OVERSPECIFIC \t\tProto:34 001:(0.1952) 002:(0.0395) 003:(0.2953) 023:(0.0616) 024:(0.0443) 025:(0.0734) 045:(0.2227) 100:(0.9364) 101:(0.6587) \n",
      "OVERSPECIFIC \t\tProto:35 001:(0.2257) 002:(0.1491) 003:(0.3421) 023:(0.4005) 024:(0.5734) 025:(0.594) 045:(0.1104) 100:(0.1767) 101:(0.1583) \n",
      "OVERSPECIFIC \t\tProto:36 001:(0.0444) 002:(0.0089) 003:(0.0294) 023:(0.9392) 024:(0.9421) 025:(0.9752) 045:(0.005) 100:(0.0646) 101:(0.0464) \n",
      "OVERSPECIFIC \t\tProto:39 001:(0.3125) 002:(0.7699) 003:(0.3274) 023:(0.0438) 024:(0.0178) 025:(0.1716) 045:(0.7878) 100:(0.0092) 101:(0.1558) \n",
      "OVERSPECIFIC \t\tProto:43 001:(0.0039) 002:(0.9818) 003:(0.0827) 023:(0.0011) 024:(0.0009) 025:(0.0012) 045:(0.877) 100:(0.5883) 101:(0.0673) \n",
      "OVERSPECIFIC \t\tProto:44 001:(0.7777) 002:(0.5939) 003:(0.3278) 023:(0.0411) 024:(0.0403) 025:(0.0183) 045:(0.1298) 100:(0.35) 101:(0.3078) \n",
      "OVERSPECIFIC \t\tProto:45 001:(0.1375) 002:(0.5891) 003:(0.1029) 023:(0.1589) 024:(0.937) 025:(0.4953) 045:(0.2117) 100:(0.7255) 101:(0.1569) \n",
      "OVERSPECIFIC \t\tProto:47 001:(0.0051) 002:(0.441) 003:(0.0028) 023:(0.003) 024:(0.7603) 025:(0.0076) 045:(0.0216) 100:(0.0012) 101:(0.901) \n",
      "OVERSPECIFIC \t\tProto:48 001:(0.9766) 002:(0.3709) 003:(0.7747) 023:(0.5226) 024:(0.2176) 025:(0.4835) 045:(0.2276) 100:(0.1343) 101:(0.1153) \n",
      "OVERSPECIFIC \t\tProto:49 001:(0.7673) 002:(0.4569) 003:(0.8003) 023:(0.8163) 024:(0.3858) 025:(0.5287) 045:(0.1397) 100:(0.2636) 101:(0.054) \n",
      "OVERSPECIFIC \t\tProto:50 001:(0.5968) 002:(0.2685) 003:(0.7449) 023:(0.016) 024:(0.0616) 025:(0.0396) 045:(0.2599) 100:(0.6518) 101:(0.1286) \n",
      "OVERSPECIFIC \t\tProto:51 001:(0.0595) 002:(0.481) 003:(0.3093) 023:(0.0605) 024:(0.0444) 025:(0.0109) 045:(0.0202) 100:(0.2368) 101:(0.5789) \n",
      "OVERSPECIFIC \t\tProto:52 001:(0.8946) 002:(0.5082) 003:(0.8074) 023:(0.2599) 024:(0.0195) 025:(0.3072) 045:(0.5224) 100:(0.0011) 101:(0.0007) \n",
      "OVERSPECIFIC \t\tProto:53 001:(0.806) 002:(0.1453) 003:(0.093) 023:(0.0614) 024:(0.0071) 025:(0.0101) 045:(0.1017) 100:(0.9691) 101:(0.2673) \n",
      "OVERSPECIFIC \t\tProto:54 001:(0.4111) 002:(0.4148) 003:(0.39) 023:(0.5049) 024:(0.02) 025:(0.3532) 045:(0.3228) 100:(0.2356) 101:(0.2164) \n",
      "OVERSPECIFIC \t\tProto:55 001:(0.0403) 002:(0.9165) 003:(0.729) 023:(0.0028) 024:(0.0006) 025:(0.0008) 045:(0.8867) 100:(0.0015) 101:(0.0029) \n",
      "OVERSPECIFIC \t\tProto:56 001:(0.9108) 002:(0.8696) 003:(0.4579) 023:(0.0015) 024:(0.0117) 025:(0.0014) 045:(0.7264) 100:(0.0015) 101:(0.0041) \n",
      "OVERSPECIFIC \t\tProto:61 001:(0.0771) 002:(0.647) 003:(0.1757) 023:(0.0128) 024:(0.8317) 025:(0.2395) 045:(0.0102) 100:(0.3179) 101:(0.1765) \n",
      "OVERSPECIFIC \t\tProto:62 001:(0.0357) 002:(0.0231) 003:(0.0123) 023:(0.0136) 024:(0.7517) 025:(0.0054) 045:(0.0029) 100:(0.946) 101:(0.9048) \n",
      "OVERSPECIFIC \t\tProto:67 001:(0.9744) 002:(0.014) 003:(0.0633) 023:(0.0241) 024:(0.0508) 025:(0.0139) 045:(0.1669) 100:(0.6665) 101:(0.0092) \n",
      "OVERSPECIFIC \t\tProto:68 001:(0.0226) 002:(0.1386) 003:(0.6201) 023:(0.02) 024:(0.0128) 025:(0.0484) 045:(0.1288) 100:(0.2989) 101:(0.883) \n",
      "OVERSPECIFIC \t\tProto:69 001:(0.9917) 002:(0.9645) 003:(0.4484) 023:(0.0177) 024:(0.0032) 025:(0.0896) 045:(0.9563) 100:(0.0009) 101:(0.0013) \n",
      "OVERSPECIFIC \t\tProto:70 001:(0.2233) 002:(0.1443) 003:(0.3741) 023:(0.0046) 024:(0.9768) 025:(0.0104) 045:(0.007) 100:(0.0276) 101:(0.6152) \n",
      "OVERSPECIFIC \t\tProto:71 001:(0.0565) 002:(0.6393) 003:(0.1998) 023:(0.0161) 024:(0.0626) 025:(0.002) 045:(0.0433) 100:(0.801) 101:(0.8659) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 90it [00:02, 36.46it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 032+033\n",
      "\t Child: 033+031\n",
      "SHARED \t\tProto:19 031:(0.9981) 033:(0.9893) \n",
      "SHARED \t\tProto:1 031:(0.9928) 033:(0.9998) \n",
      "SHARED \t\tProto:3 031:(1.0) 033:(0.9961) \n",
      "SHARED \t\tProto:6 031:(0.9979) 033:(0.9982) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 270it [00:07, 35.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 045+101\n",
      "\t Child: 101+023\n",
      "OVERSPECIFIC \t\tProto:1 023:(0.693) 024:(0.9674) 025:(0.8513) 100:(0.004) 101:(0.004) \n",
      "OVERSPECIFIC \t\tProto:33 023:(0.9611) 024:(0.5683) 025:(0.9901) 100:(0.3083) 101:(0.0076) \n",
      "OVERSPECIFIC \t\tProto:35 023:(0.0742) 024:(0.9828) 025:(0.0041) 100:(0.9821) 101:(0.9897) \n",
      "OVERSPECIFIC \t\tProto:8 023:(0.858) 024:(0.9943) 025:(0.9897) 100:(0.0266) 101:(0.1589) \n",
      "OVERSPECIFIC \t\tProto:40 023:(0.9831) 024:(0.9955) 025:(0.9921) 100:(0.7705) 101:(0.2546) \n",
      "OVERSPECIFIC \t\tProto:44 023:(0.9983) 024:(0.9995) 025:(0.9996) 100:(0.002) 101:(0.0012) \n",
      "OVERSPECIFIC \t\tProto:46 023:(0.5892) 024:(0.1531) 025:(0.8785) 100:(0.1764) 101:(0.9774) \n",
      "OVERSPECIFIC \t\tProto:19 023:(0.9072) 024:(0.9955) 025:(0.4954) 100:(0.3348) 101:(0.0014) \n",
      "OVERSPECIFIC \t\tProto:52 023:(0.088) 024:(0.019) 025:(0.0283) 100:(0.9354) 101:(0.9812) \n",
      "OVERSPECIFIC \t\tProto:53 023:(0.7084) 024:(0.0273) 025:(0.1527) 100:(0.959) 101:(0.0132) \n",
      "OVERSPECIFIC \t\tProto:57 023:(0.0158) 024:(0.6905) 025:(0.0083) 100:(0.926) 101:(0.9175) \n",
      "OVERSPECIFIC \t\tProto:26 023:(0.4109) 024:(0.5681) 025:(0.6335) 100:(0.2762) 101:(0.4642) \n",
      "OVERSPECIFIC \t\tProto:59 023:(0.0052) 024:(0.0246) 025:(0.0064) 100:(0.9951) 101:(0.9992) \n",
      "OVERSPECIFIC \t\tProto:61 023:(0.5973) 024:(0.3303) 025:(0.6711) 100:(0.5679) 101:(0.4813) \n",
      "\t Child: 045+003\n",
      "SHARED \t\tProto:2 001:(0.9685) 002:(0.9743) 003:(0.9373) 045:(0.9517) \n",
      "OVERSPECIFIC \t\tProto:3 001:(0.5731) 002:(0.6035) 003:(0.7174) 045:(0.2595) \n",
      "OVERSPECIFIC \t\tProto:9 001:(0.4955) 002:(0.876) 003:(0.104) 045:(0.8747) \n",
      "OVERSPECIFIC \t\tProto:41 001:(0.9736) 002:(0.7787) 003:(0.8622) 045:(0.4196) \n",
      "OVERSPECIFIC \t\tProto:45 001:(0.9848) 002:(0.2565) 003:(0.9154) 045:(0.8466) \n",
      "SHARED \t\tProto:16 001:(0.9914) 002:(0.9982) 003:(0.9612) 045:(0.9745) \n",
      "SHARED \t\tProto:58 001:(0.9998) 002:(0.9983) 003:(0.9968) 045:(0.9314) \n",
      "OVERSPECIFIC \t\tProto:28 001:(0.9127) 002:(0.9222) 003:(0.9599) 045:(0.2055) \n",
      "SHARED \t\tProto:63 001:(0.6701) 002:(0.8241) 003:(0.9537) 045:(0.8679) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 120it [00:02, 40.83it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 045+003\n",
      "\t Child: 003+002\n",
      "SHARED \t\tProto:6 001:(0.8151) 002:(0.8045) 003:(0.7592) \n",
      "SHARED \t\tProto:8 001:(0.901) 002:(0.831) 003:(0.995) \n",
      "SHARED \t\tProto:9 001:(0.9851) 002:(0.9979) 003:(0.988) \n",
      "SHARED \t\tProto:11 001:(0.9955) 002:(0.9868) 003:(0.9836) \n",
      "SHARED \t\tProto:13 001:(0.9742) 002:(0.9523) 003:(0.8751) \n",
      "SHARED \t\tProto:17 001:(0.9984) 002:(0.9999) 003:(0.9919) \n",
      "OVERSPECIFIC \t\tProto:21 001:(0.9942) 002:(0.0968) 003:(0.9705) \n",
      "SHARED \t\tProto:23 001:(0.9219) 002:(0.9853) 003:(0.7604) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 150it [00:04, 36.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 101+023\n",
      "\t Child: 023+025\n",
      "SHARED \t\tProto:2 023:(0.988) 024:(0.9883) 025:(0.6582) \n",
      "SHARED \t\tProto:8 023:(0.8023) 024:(0.9999) 025:(0.9828) \n",
      "SHARED \t\tProto:9 023:(0.803) 024:(0.7869) 025:(0.773) \n",
      "SHARED \t\tProto:18 023:(0.996) 024:(0.9842) 025:(0.9956) \n",
      "SHARED \t\tProto:20 023:(0.8247) 024:(0.9982) 025:(0.8065) \n",
      "SHARED \t\tProto:24 023:(0.9171) 024:(0.9901) 025:(0.9944) \n",
      "SHARED \t\tProto:25 023:(0.992) 024:(0.9809) 025:(0.9993) \n",
      "SHARED \t\tProto:30 023:(0.8427) 024:(0.9735) 025:(0.8279) \n",
      "SHARED \t\tProto:31 023:(0.8521) 024:(0.9856) 025:(0.941) \n",
      "\t Child: 101+100\n",
      "SHARED \t\tProto:3 100:(0.9985) 101:(0.9951) \n",
      "SHARED \t\tProto:6 100:(0.9984) 101:(0.9991) \n",
      "SHARED \t\tProto:7 100:(0.9962) 101:(0.9986) \n",
      "SHARED \t\tProto:14 100:(0.9981) 101:(0.9989) \n",
      "SHARED \t\tProto:26 100:(0.9941) 101:(0.9772) \n",
      "SHARED \t\tProto:28 100:(0.9937) 101:(0.9847) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 90it [00:02, 36.98it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 003+002\n",
      "\t Child: 002+001\n",
      "SHARED \t\tProto:0 001:(0.9997) 002:(0.9986) \n",
      "SHARED \t\tProto:12 001:(0.9945) 002:(0.9844) \n",
      "SHARED \t\tProto:13 001:(0.9911) 002:(0.9926) \n",
      "SHARED \t\tProto:17 001:(0.9999) 002:(0.9968) \n",
      "SHARED \t\tProto:18 001:(0.9724) 002:(0.9732) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 90it [00:02, 38.08it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 023+025\n",
      "\t Child: 025+024\n",
      "SHARED \t\tProto:3 024:(0.9715) 025:(0.983) \n",
      "SHARED \t\tProto:6 024:(0.9997) 025:(0.9402) \n",
      "SHARED \t\tProto:7 024:(0.9959) 025:(0.9742) \n",
      "SHARED \t\tProto:9 024:(0.9898) 025:(0.9649) \n",
      "SHARED \t\tProto:13 024:(0.9978) 025:(0.9899) \n",
      "SHARED \t\tProto:14 024:(0.9993) 025:(0.9713) \n",
      "Done!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Proto activations on leaf descendents - topk images (prunes irrelevant protos based on threshold)\n",
    "\n",
    "from util.data import ModifiedLabelLoader\n",
    "from collections import defaultdict\n",
    "import heapq\n",
    "import pdb\n",
    "from util.vis_pipnet import get_img_coordinates\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image, ImageDraw as D\n",
    "import torchvision\n",
    "\n",
    "topk = 10\n",
    "save_images = True\n",
    "THRESHOLD = 0.5\n",
    "\n",
    "def get_heap():\n",
    "    list_ = []\n",
    "    heapq.heapify(list_)\n",
    "    return list_\n",
    "\n",
    "patchsize, skip = get_patch_size(args)\n",
    "\n",
    "for node in root.nodes_with_children():\n",
    "#     if node.name == 'root':\n",
    "#         continue\n",
    "    non_leaf_children_names = [child.name for child in node.children if not child.is_leaf()]\n",
    "    if len(non_leaf_children_names) == 0: # if all the children are leaf nodes then skip this node\n",
    "        continue\n",
    "\n",
    "    name2label = projectloader.dataset.class_to_idx\n",
    "    label2name = {label:name for name, label in name2label.items()}\n",
    "    modifiedLabelLoader = ModifiedLabelLoader(projectloader, node)\n",
    "    coarse_label2name = modifiedLabelLoader.modifiedlabel2name\n",
    "    node_label_to_children = {label: name for name, label in node.children_to_labels.items()}\n",
    "    \n",
    "    imgs = modifiedLabelLoader.filtered_imgs\n",
    "\n",
    "    img_iter = tqdm(enumerate(modifiedLabelLoader),\n",
    "                    total=len(modifiedLabelLoader),\n",
    "                    mininterval=50.,\n",
    "                    desc='Collecting topk',\n",
    "                    ncols=0)\n",
    "\n",
    "    classification_weights = getattr(net.module, '_'+node.name+'_classification').weight\n",
    "    \n",
    "    # maps proto_number -> grand_child_name (or descendant leaf name) -> list of top-k activations\n",
    "    proto_mean_activations = defaultdict(lambda: defaultdict(get_heap))\n",
    "\n",
    "    # maps class names to the prototypes that belong to that\n",
    "    class_and_prototypes = defaultdict(set)\n",
    "\n",
    "    for i, (xs, orig_y, ys) in img_iter:\n",
    "        if coarse_label2name[ys.item()] not in non_leaf_children_names:\n",
    "            continue\n",
    "\n",
    "        xs, ys = xs.to(device), ys.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            softmaxes, pooled, _ = net(xs, inference=False)\n",
    "            pooled = pooled[node.name].squeeze(0) \n",
    "            softmaxes = softmaxes[node.name]#.squeeze(0)\n",
    "\n",
    "            for p in range(pooled.shape[0]): # pooled.shape -> [768] (== num of prototypes)\n",
    "                c_weight = torch.max(classification_weights[:,p]) # classification_weights[:,p].shape -> [200] (== num of classes)\n",
    "                relevant_proto_classes = torch.nonzero(classification_weights[:, p] > 1e-3)\n",
    "                relevant_proto_class_names = [node_label_to_children[class_idx.item()] for class_idx in relevant_proto_classes]\n",
    "                \n",
    "                # Take the max per prototype.                             \n",
    "                max_per_prototype, max_idx_per_prototype = torch.max(softmaxes, dim=0)\n",
    "                max_per_prototype_h, max_idx_per_prototype_h = torch.max(max_per_prototype, dim=1)\n",
    "                max_per_prototype_w, max_idx_per_prototype_w = torch.max(max_per_prototype_h, dim=1) #shape (num_prototypes)\n",
    "                \n",
    "                h_idx = max_idx_per_prototype_h[p, max_idx_per_prototype_w[p]]\n",
    "                w_idx = max_idx_per_prototype_w[p]\n",
    "\n",
    "                if len(relevant_proto_class_names) == 0:\n",
    "                    continue\n",
    "                \n",
    "                if (len(relevant_proto_class_names) == 1) and (relevant_proto_class_names[0] not in non_leaf_children_names):\n",
    "                    continue\n",
    "                    \n",
    "                h_coor_min, h_coor_max, w_coor_min, w_coor_max = get_img_coordinates(args.image_size, softmaxes.shape, patchsize, skip, h_idx, w_idx)\n",
    "                \n",
    "                if (coarse_label2name[ys.item()] in relevant_proto_class_names):\n",
    "                    child_node = root.get_node(coarse_label2name[ys.item()])\n",
    "                    leaf_descendent = label2name[orig_y.item()][4:7]\n",
    "                    img_to_open = imgs[i][0] # it is a tuple of (path to image, lable)\n",
    "                    if topk and (len(proto_mean_activations[p][leaf_descendent]) > topk):\n",
    "                        heapq.heappushpop(proto_mean_activations[p][leaf_descendent], (pooled[p].item(), img_to_open, (h_coor_min, h_coor_max, w_coor_min, w_coor_max)))\n",
    "                    else:\n",
    "                        heapq.heappush(proto_mean_activations[p][leaf_descendent], (pooled[p].item(), img_to_open, (h_coor_min, h_coor_max, w_coor_min, w_coor_max)))\n",
    "\n",
    "                class_and_prototypes[', '.join(relevant_proto_class_names)].add(p)\n",
    "\n",
    "    \n",
    "    print('Node', node.name)\n",
    "    # NOTE: Prototypes are assumed to be exclusive to only one child class, may not always be the case\n",
    "    for child_classname in class_and_prototypes:\n",
    "        \n",
    "        print('\\t'*1, 'Child:', child_classname)\n",
    "        for p in class_and_prototypes[child_classname]:\n",
    "            topk_mean_activations = []\n",
    "            logstr = '\\t'*2 + f'Proto:{p} '\n",
    "            for leaf_descendent in proto_mean_activations[p]:\n",
    "                mean_activation = round(np.mean([activation for activation, *_ in proto_mean_activations[p][leaf_descendent]]), 4)\n",
    "                topk_mean_activations.append(mean_activation)\n",
    "                num_images = len(proto_mean_activations[p][leaf_descendent])\n",
    "                logstr += f'{leaf_descendent}:({mean_activation}) '\n",
    "            \n",
    "            relevant_proto = np.all(np.array(topk_mean_activations) > THRESHOLD)\n",
    "            \n",
    "            # pruning - setting weight of overspecific prototypes to zero\n",
    "            if not relevant_proto:\n",
    "                classification_layer = getattr(net.module, '_'+node.name+'_classification')\n",
    "                torch.nn.init.zeros_(classification_layer.weight[:,p])\n",
    "            \n",
    "            \n",
    "            print(('SHARED ' if relevant_proto else 'OVERSPECIFIC ') + logstr)\n",
    "            \n",
    "print('Done!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77065fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({'model_state_dict': net.state_dict(),\\\n",
    "            'optimizer_net_state_dict': optimizer_net.state_dict(),\\\n",
    "            'optimizer_classifier_state_dict': optimizer_classifier.state_dict()},\\\n",
    "           os.path.join(os.path.join(args.log_dir, 'checkpoints'),\\\n",
    "            f'net_overspecific_pruned_thresh={THRESHOLD}_{str(epoch)}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c319387c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num classes (k) =  18 ['cub_001_Black_footed_Albatross', 'cub_002_Laysan_Albatross', 'cub_003_Sooty_Albatross', 'cub_004_Groove_billed_Ani', 'cub_023_Brandt_Cormorant'] etc.\n",
      "Classes:  {'cub_001_Black_footed_Albatross': 0, 'cub_002_Laysan_Albatross': 1, 'cub_003_Sooty_Albatross': 2, 'cub_004_Groove_billed_Ani': 3, 'cub_023_Brandt_Cormorant': 4, 'cub_024_Red_faced_Cormorant': 5, 'cub_025_Pelagic_Cormorant': 6, 'cub_031_Black_billed_Cuckoo': 7, 'cub_032_Mangrove_Cuckoo': 8, 'cub_033_Yellow_billed_Cuckoo': 9, 'cub_045_Northern_Fulmar': 10, 'cub_050_Eared_Grebe': 11, 'cub_051_Horned_Grebe': 12, 'cub_052_Pied_billed_Grebe': 13, 'cub_053_Western_Grebe': 14, 'cub_086_Pacific_Loon': 15, 'cub_100_Brown_Pelican': 16, 'cub_101_White_Pelican': 17}\n",
      "Number of prototypes:  20\n",
      "----------Prototypes per descendant: 4----------\n",
      "Assigned 136 protos to node root\n",
      "Assigned 24 protos to node 052+053\n",
      "Assigned 104 protos to node 004+086\n",
      "Assigned 16 protos to node 053+050\n",
      "Assigned 24 protos to node 004+032\n",
      "Assigned 72 protos to node 086+045\n",
      "Assigned 8 protos to node 050+051\n",
      "Assigned 16 protos to node 032+033\n",
      "Assigned 64 protos to node 045+101\n",
      "Assigned 8 protos to node 033+031\n",
      "Assigned 24 protos to node 045+003\n",
      "Assigned 32 protos to node 101+023\n",
      "Assigned 16 protos to node 003+002\n",
      "Assigned 8 protos to node 101+100\n",
      "Assigned 16 protos to node 023+025\n",
      "Assigned 8 protos to node 002+001\n",
      "Assigned 8 protos to node 025+024\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    device_ids = [torch.cuda.current_device()]\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    device_ids = []\n",
    "\n",
    "args_file = open(os.path.join(run_path, 'metadata', 'args.pickle'), 'rb')\n",
    "args = pickle.load(args_file)\n",
    "\n",
    "# Obtain the dataset and dataloaders\n",
    "trainloader, trainloader_pretraining, trainloader_normal, trainloader_normal_augment, projectloader, testloader, test_projectloader, classes = get_dataloaders(args, device)\n",
    "if len(classes)<=20:\n",
    "    if args.validation_size == 0.:\n",
    "        print(\"Classes: \", testloader.dataset.class_to_idx, flush=True)\n",
    "    else:\n",
    "        print(\"Classes: \", str(classes), flush=True)\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------------------------------\n",
    "# Create a convolutional network based on arguments and add 1x1 conv layer\n",
    "feature_net, add_on_layers, pool_layer, classification_layers, num_prototypes = get_network(len(classes), args, root=root)\n",
    "   \n",
    "# Create a PIP-Net\n",
    "pruned_net = PIPNet(num_classes=len(classes),\n",
    "                    num_prototypes=num_prototypes,\n",
    "                    feature_net = feature_net,\n",
    "                    args = args,\n",
    "                    add_on_layers = add_on_layers,\n",
    "                    pool_layer = pool_layer,\n",
    "                    classification_layers = classification_layers,\n",
    "                    num_parent_nodes = len(root.nodes_with_children()),\n",
    "                    root = root\n",
    "                    )\n",
    "pruned_net = pruned_net.to(device=device)\n",
    "pruned_net = nn.DataParallel(pruned_net, device_ids = device_ids)    \n",
    "pruned_net.load_state_dict(checkpoint['model_state_dict'],strict=True)\n",
    "pruned_net.eval()\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------------------------------\n",
    "# Create a convolutional network based on arguments and add 1x1 conv layer\n",
    "# feature_net, add_on_layers, pool_layer, classification_layers, num_prototypes = get_network(len(classes), args, root=root)\n",
    "   \n",
    "# # Create a PIP-Net\n",
    "# replaced_net = PIPNet(num_classes=len(classes),\n",
    "#                     num_prototypes=num_prototypes,\n",
    "#                     feature_net = feature_net,\n",
    "#                     args = args,\n",
    "#                     add_on_layers = add_on_layers,\n",
    "#                     pool_layer = pool_layer,\n",
    "#                     classification_layers = classification_layers,\n",
    "#                     num_parent_nodes = len(root.nodes_with_children()),\n",
    "#                     root = root\n",
    "#                     )\n",
    "# replaced_net = replaced_net.to(device=device)\n",
    "# replaced_net = nn.DataParallel(replaced_net, device_ids = device_ids)    \n",
    "# replaced_net.load_state_dict(checkpoint['model_state_dict'],strict=True)\n",
    "# replaced_net.eval()\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------------------------------\n",
    "criterion = nn.NLLLoss(reduction='mean').to(device)\n",
    "\n",
    "args.wshape = 26\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5ed860",
   "metadata": {},
   "source": [
    "# Replace proto with parent protos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "58e9219e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pruned_checkpoint = torch.load(os.path.join(os.path.join(args.log_dir, 'checkpoints'),\\\n",
    "            f'net_overspecific_pruned_thresh={THRESHOLD}_{str(epoch)}'), map_location=device)\n",
    "\n",
    "import copy\n",
    "\n",
    "pruned_net.load_state_dict(pruned_checkpoint['model_state_dict'],strict=True)\n",
    "# replaced_net.load_state_dict(pruned_checkpoint['model_state_dict'],strict=True)\n",
    "replaced_net = copy.deepcopy(pruned_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3a77916b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "053+050\n",
      "050+051\n",
      "032+033\n",
      "033+031\n",
      "045+003\n",
      "003+002\n",
      "101+100\n",
      "023+025\n",
      "002+001\n",
      "025+024\n"
     ]
    }
   ],
   "source": [
    "import pdb\n",
    "from collections import defaultdict\n",
    "acc_before_after = defaultdict(lambda : {'any_relevant_proto':False, 'replaced':False, 'acc_before_replacing':None, 'acc_after_replacing':None})\n",
    "\n",
    "# First iterate through all nodes and check which ones have relevant protos\n",
    "with torch.no_grad():\n",
    "    for node in root.nodes_with_children():\n",
    "        if node.name == 'root':\n",
    "            continue\n",
    "\n",
    "        protos = getattr(pruned_net.module, '_'+node.name+'_add_on').weight\n",
    "        mlp = getattr(pruned_net.module, '_'+node.name+'_classification').weight\n",
    "        for class_idx in node.children_to_labels.values():\n",
    "            relevant_proto_idxs = np.where((mlp[class_idx, :] > 1e-3).cpu())[0]\n",
    "            if len(relevant_proto_idxs) > 0:\n",
    "                acc_before_after[node.name]['any_relevant_proto'] = True\n",
    "                # even if one of the classes of the node has relevant protos that is enough to say the node has relevant protos\n",
    "                break\n",
    "\n",
    "with torch.no_grad():\n",
    "    for node in root.nodes_with_children():\n",
    "        if node.name == 'root':\n",
    "            continue\n",
    "        if not acc_before_after[node.name]['any_relevant_proto']:\n",
    "            continue\n",
    "\n",
    "        parent_node = node.parent\n",
    "\n",
    "        parent_protos = getattr(pruned_net.module, '_'+parent_node.name+'_add_on').weight\n",
    "        parent_mlp = getattr(pruned_net.module, '_'+parent_node.name+'_classification').weight\n",
    "        class_idx = parent_node.children_to_labels[node.name]\n",
    "        parent_relevant_proto_idxs = np.where((parent_mlp[class_idx, :] > 1e-3).cpu())[0]\n",
    "\n",
    "        if len(parent_relevant_proto_idxs) == 0:\n",
    "            continue\n",
    "        print(node.name)\n",
    "        parent_relevant_protos = parent_protos[parent_relevant_proto_idxs]\n",
    "\n",
    "        child_protos = getattr(pruned_net.module, '_'+node.name+'_add_on').weight\n",
    "        child_mlp = getattr(pruned_net.module, '_'+node.name+'_classification').weight\n",
    "\n",
    "        relevant_ps = set()\n",
    "        for c in range(child_mlp.shape[0]):\n",
    "            proto_weights = child_mlp[c,:]\n",
    "            for p in range(child_mlp.shape[1]):\n",
    "                if proto_weights[p]> 1e-3:\n",
    "                    relevant_ps.add(p)\n",
    "\n",
    "        child_relevant_proto_idxs = np.array(list(relevant_ps))\n",
    "\n",
    "        c_idx_to_closest_p_idx = {}\n",
    "        for c_idx in child_relevant_proto_idxs:\n",
    "            closest_dist = float('inf')\n",
    "            closest_p_idx = None\n",
    "            for p_idx in parent_relevant_proto_idxs:\n",
    "                inner_product = torch.sum(parent_protos[p_idx].squeeze() * child_protos[c_idx].squeeze()).item()\n",
    "                if inner_product < closest_dist:\n",
    "                    closest_dist = inner_product\n",
    "                    closest_p_idx = p_idx\n",
    "            c_idx_to_closest_p_idx[c_idx] = closest_p_idx\n",
    "            getattr(replaced_net.module, '_'+node.name+'_add_on').weight[c_idx, :, :, :] = getattr(pruned_net.module, '_'+parent_node.name+'_add_on').weight[closest_p_idx, :, :, :]\n",
    "            \n",
    "        acc_before_after[node.name]['replaced'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7bed7ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_with_relevant_protos = set([node_name for node_name, details in acc_before_after.items() \\\n",
    "                              if details['any_relevant_proto']])\n",
    "\n",
    "nodes_with_replaced_protos = set([node_name for node_name, details in acc_before_after.items() \\\n",
    "                              if details['replaced']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c229b764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'101+023', '032+033', '033+031', '045+101', '003+002', '101+100', '052+053', '004+032', '045+003', '023+025', '053+050', '025+024', '002+001', '050+051', '086+045'}\n",
      "{'032+033', '033+031', '003+002', '101+100', '045+003', '023+025', '053+050', '025+024', '002+001', '050+051'}\n"
     ]
    }
   ],
   "source": [
    "print(nodes_with_relevant_protos)\n",
    "print(nodes_with_replaced_protos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cc8332",
   "metadata": {},
   "source": [
    "# Custom test funtion that skips nodes with no relevant proto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3e3991dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "from collections import defaultdict\n",
    "from torchmetrics.functional import f1_score, recall, precision\n",
    "from torchvision.datasets.folder import ImageFolder\n",
    "from pipnet.train import uniform_loss, align_loss, orth_dist\n",
    "\n",
    "OOD_LABEL = -1\n",
    "\n",
    "def test_pruned_pipnet(net, nodes_with_relevant_protos, test_loader, optimizer_net, optimizer_classifier, scheduler_net, scheduler_classifier, criterion, epoch, nr_epochs, device, pretrain=False, finetune=False, progress_prefix: str = 'Test Epoch', wandb_logging=True, test_loader_OOD=None, kernel_orth=False, wandb_run=None, pretrain_epochs=0, log:Log=None):\n",
    "\n",
    "    root = net.module.root\n",
    "    dataset = test_loader.dataset\n",
    "    while type(dataset) != ImageFolder:\n",
    "        dataset = dataset.dataset\n",
    "    name2label = dataset.class_to_idx\n",
    "    label2name = {label:name for name, label in name2label.items()}\n",
    "    label2name[OOD_LABEL] = 'OOD'\n",
    "\n",
    "    wandb_log_subdir = 'test'\n",
    "\n",
    "    # node_accuracy = defaultdict(lambda: {'n_examples': 0, 'n_correct': 0, 'accuracy': None, 'preds': None, 'children': defaultdict(lambda: {'n_examples': 0, 'n_correct': 0})})\n",
    "    node_accuracy = {}\n",
    "    for node in root.nodes_with_children():\n",
    "        node_accuracy[node.name] = {'n_examples': 0, 'n_correct': 0, 'accuracy': None, 'f1': None, 'preds': torch.empty(0, node.num_children()).to(device), 'gts': torch.empty(0).to(device)}\n",
    "        node_accuracy[node.name]['children'] = defaultdict(lambda: {'n_examples': 0, 'n_correct': 0})\n",
    "\n",
    "    # Make sure the model is in eval mode\n",
    "    net.eval()\n",
    "    \n",
    "    # Store info about the procedure\n",
    "    test_info = dict()\n",
    "    total_loss = 0.\n",
    "    total_acc = 0.\n",
    "    class_loss_ep_mean = 0.\n",
    "    a_loss_pf_ep_mean = 0.\n",
    "    tanh_loss_ep_mean = 0.\n",
    "    OOD_loss_ep_mean = 0.\n",
    "    kernel_orth_loss_ep_mean = 0.\n",
    "    uni_loss_ep_mean = 0.\n",
    "\n",
    "    iters = len(test_loader)\n",
    "    # Show progress on progress bar. \n",
    "    test_iter = tqdm(enumerate(test_loader),\n",
    "                    total=len(test_loader),\n",
    "                    desc=progress_prefix+'%s'%epoch,\n",
    "                    mininterval=2.,\n",
    "                    ncols=0)\n",
    "    \n",
    "    test_OOD_iter = iter(test_loader_OOD) if test_loader_OOD else None\n",
    "    OOD_loss_required = True if test_loader_OOD else False\n",
    "\n",
    "    if pretrain:\n",
    "        align_pf_weight = (epoch/nr_epochs)*1.\n",
    "        unif_weight = 0.5 #ignored\n",
    "        t_weight = 5.\n",
    "        cl_weight = 0.\n",
    "        OOD_loss_weight = 0.\n",
    "        orth_weight = 0.5\n",
    "    else:\n",
    "        align_pf_weight = 5. \n",
    "        t_weight = 2.\n",
    "        unif_weight = 0.\n",
    "        cl_weight = 2.\n",
    "        OOD_loss_weight = 0.2\n",
    "        orth_weight = 0.5\n",
    "\n",
    "    # maps, node_name -> loss_name -> list_of_loss_values_corresponding_to_each_step\n",
    "    # node_wise_losses = defaultdict(lambda: defaultdict(list))\n",
    "    node_wise_losses = {}\n",
    "    for node in root.nodes_with_children():\n",
    "        node_wise_losses[node.name] = {}\n",
    "        node_wise_losses[node.name]['class_loss'] = []\n",
    "        node_wise_losses[node.name]['a_loss'] = []\n",
    "        node_wise_losses[node.name]['tanh_loss'] = []\n",
    "        node_wise_losses[node.name]['OOD_loss'] = []\n",
    "        node_wise_losses[node.name]['kernel_orth_loss'] = []\n",
    "        node_wise_losses[node.name]['uni_loss'] = []\n",
    "\n",
    "    \n",
    "    lrs_net = []\n",
    "    lrs_class = []\n",
    "    n_fine_correct = 0\n",
    "    n_samples = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Iterate through the data set to update leaves, prototypes and network\n",
    "        for i, (xs, ys) in test_iter:       \n",
    "            \n",
    "            xs, ys = xs.to(device), ys.to(device)\n",
    "\n",
    "            if test_OOD_iter:\n",
    "                xs_OOD, ys_OOD = next(test_OOD_iter)\n",
    "                ys_OOD.fill_(OOD_LABEL)\n",
    "                xs_OOD, ys_OOD = xs_OOD.to(device), ys_OOD.to(device)\n",
    "                xs = torch.cat([xs, xs, xs_OOD])\n",
    "                ys = torch.cat([ys, ys, ys_OOD])\n",
    "            else:\n",
    "                xs = torch.cat([xs, xs])\n",
    "                ys = torch.cat([ys, ys])\n",
    "            \n",
    "            # Perform a forward pass through the network\n",
    "            proto_features, pooled, out = net(xs)\n",
    "            \n",
    "            loss, class_loss_dict, a_loss_pf_dict, tanh_loss_dict, OOD_loss_dict, kernel_orth_loss_dict, uni_loss_dict, avg_class_loss, avg_a_loss_pf, avg_tanh_loss, avg_OOD_loss, avg_kernel_orth_loss, avg_uni_loss, acc = \\\n",
    "            calculate_loss_pruned_net(net, nodes_with_relevant_protos, proto_features, pooled, out, ys, align_pf_weight, t_weight, unif_weight, cl_weight, OOD_loss_weight, orth_weight, net.module._multiplier, pretrain, finetune, \\\n",
    "                           criterion, test_iter, print=True, EPS=1e-8, root=root, label2name=label2name, node_accuracy=node_accuracy, OOD_loss_required=OOD_loss_required, kernel_orth=kernel_orth)\n",
    "            \n",
    "            for node_name, loss_value in class_loss_dict.items():\n",
    "                if node_name not in nodes_with_relevant_protos:\n",
    "                    continue\n",
    "                node_wise_losses[node_name]['class_loss'].append(loss_value.item())\n",
    "            \n",
    "            for node_name, loss_value in a_loss_pf_dict.items():\n",
    "                if node_name not in nodes_with_relevant_protos:\n",
    "                    continue\n",
    "                node_wise_losses[node_name]['a_loss'].append(loss_value.item())\n",
    "\n",
    "            for node_name, loss_value in tanh_loss_dict.items():\n",
    "                if node_name not in nodes_with_relevant_protos:\n",
    "                    continue\n",
    "                node_wise_losses[node_name]['tanh_loss'].append(loss_value.item())\n",
    "\n",
    "            for node_name, loss_value in OOD_loss_dict.items():\n",
    "                if node_name not in nodes_with_relevant_protos:\n",
    "                    continue\n",
    "                node_wise_losses[node_name]['OOD_loss'].append(loss_value.item())\n",
    "\n",
    "            for node_name, loss_value in kernel_orth_loss_dict.items():\n",
    "                if node_name not in nodes_with_relevant_protos:\n",
    "                    continue\n",
    "                node_wise_losses[node_name]['kernel_orth_loss'].append(loss_value.item())\n",
    "\n",
    "            for node_name, loss_value in uni_loss_dict.items():\n",
    "                if node_name not in nodes_with_relevant_protos:\n",
    "                    continue\n",
    "                node_wise_losses[node_name]['uni_loss'].append(loss_value.item())\n",
    "\n",
    "            class_loss_ep_mean += avg_class_loss if avg_class_loss else 0.\n",
    "            a_loss_pf_ep_mean += avg_a_loss_pf if avg_a_loss_pf else 0.\n",
    "            tanh_loss_ep_mean += avg_tanh_loss if avg_tanh_loss else 0.\n",
    "            OOD_loss_ep_mean += avg_OOD_loss if avg_OOD_loss else 0.\n",
    "            kernel_orth_loss_ep_mean += avg_kernel_orth_loss if avg_kernel_orth_loss else 0.\n",
    "            uni_loss_ep_mean += avg_uni_loss if avg_uni_loss else 0.\n",
    "                \n",
    "            total_acc+=acc # DUMMY can be removed\n",
    "            total_loss+=loss.item()\n",
    "            \n",
    "            _, preds_joint = net.module.get_joint_distribution(out)\n",
    "            preds_joint = preds_joint[ys != OOD_LABEL]\n",
    "            _, fine_predicted = torch.max(preds_joint.data, 1)\n",
    "            target = ys[ys != OOD_LABEL]\n",
    "            fine_correct = fine_predicted == target\n",
    "            n_fine_correct += fine_correct.sum().item()\n",
    "            n_samples += target.size(0)\n",
    "\n",
    "    test_info['fine_accuracy'] = n_fine_correct/n_samples\n",
    "\n",
    "    test_info['accuracy'] = total_acc/float(i+1)\n",
    "    test_info['loss'] = total_loss/float(i+1)\n",
    "\n",
    "    class_loss_ep_mean /= float(i+1)\n",
    "    a_loss_pf_ep_mean /= float(i+1)\n",
    "    tanh_loss_ep_mean /= float(i+1)\n",
    "    OOD_loss_ep_mean /= float(i+1)\n",
    "    kernel_orth_loss_ep_mean /= float(i+1)\n",
    "    uni_loss_ep_mean /= float(i+1)\n",
    "\n",
    "    log_dict = {}\n",
    "    # if wandb_logging:\n",
    "    log_dict[wandb_log_subdir + \"/epoch loss\"] = test_info['loss']\n",
    "    log_dict[wandb_log_subdir + \"/fine_accuracy\"] = test_info['accuracy']\n",
    "    log_dict[wandb_log_subdir + \"/class_loss\"] = class_loss_ep_mean\n",
    "    log_dict[wandb_log_subdir + \"/a_loss_pf\"] = a_loss_pf_ep_mean\n",
    "    log_dict[wandb_log_subdir + \"/tanh_loss\"] = tanh_loss_ep_mean\n",
    "    log_dict[wandb_log_subdir + \"/OOD_loss\"] = OOD_loss_ep_mean\n",
    "    log_dict[wandb_log_subdir + \"/kernel_orth_loss\"] = kernel_orth_loss_ep_mean\n",
    "    log_dict[wandb_log_subdir + \"/uni_loss\"] = uni_loss_ep_mean\n",
    "    # wandb_run.log({wandb_log_subdir + \"/epoch loss\": train_info['loss']}, step=epoch)\n",
    "    # wandb_run.log({wandb_log_subdir + \"/epoch lrs_net\": train_info['lrs_net']})\n",
    "    # wandb_run.log({wandb_log_subdir + \"/epoch lrs_class\": train_info['lrs_class']})\n",
    "\n",
    "    for node_name in node_accuracy:\n",
    "        if node_name not in nodes_with_relevant_protos:\n",
    "            continue\n",
    "        node_accuracy[node_name]['accuracy'] = round((node_accuracy[node_name]['n_correct'] / node_accuracy[node_name]['n_examples']) * 100, 2)\n",
    "        node_accuracy[node_name]['f1'] = f1_score(node_accuracy[node_name][\"preds\"], node_accuracy[node_name][\"gts\"].to(torch.int), \\\n",
    "                                                    average='weighted', num_classes=net.module.root.get_node(node_name).num_children()).item()\n",
    "        node_accuracy[node_name]['f1'] = round(node_accuracy[node_name]['f1'] * 100, 2)\n",
    "        # if wandb_logging:\n",
    "        log_dict[wandb_log_subdir + f\"/node_wise/acc:{node_name}\"] = node_accuracy[node_name]['accuracy']\n",
    "        log_dict[wandb_log_subdir + f\"/node_wise/f1:{node_name}\"] = node_accuracy[node_name]['f1']\n",
    "    if wandb_logging:\n",
    "        wandb_run.log(log_dict, step=epoch if pretrain else (epoch+pretrain_epochs))\n",
    "\n",
    "    test_info['node_accuracy'] = node_accuracy\n",
    "    print('\\tFine accuracy:', round(test_info['fine_accuracy'], 2))\n",
    "    for node_name in node_accuracy:\n",
    "        if node_name not in nodes_with_relevant_protos:\n",
    "            continue\n",
    "        acc = node_accuracy[node_name][\"accuracy\"]\n",
    "        f1 = node_accuracy[node_name]['f1']\n",
    "        samples = node_accuracy[node_name][\"n_examples\"]\n",
    "        log_string = f'\\tNode name: {node_name}, acc: {acc}, f1:{f1}, samples: {samples}'\n",
    "        for child in net.module.root.get_node(node_name).children:\n",
    "            child_n_correct = node_accuracy[node_name]['children'][child.name]['n_correct']\n",
    "            child_n_examples = node_accuracy[node_name]['children'][child.name]['n_examples']\n",
    "            log_string += \", \" + f'{child.name}={child_n_correct}/{child_n_examples}={round(child_n_correct/child_n_examples, 2)}'\n",
    "        print(log_string)\n",
    "\n",
    "    # create a log csv file for each node to log the different loss values\n",
    "    if log:\n",
    "        log_sub_dir = 'node_wise_metrics_val'\n",
    "        os.makedirs(os.path.join(log.log_dir, log_sub_dir), exist_ok=True)\n",
    "        for node_name in node_wise_losses:\n",
    "            if node_name not in nodes_with_relevant_protos:\n",
    "                continue\n",
    "            loss_names = sorted(list(node_wise_losses[node_name].keys()))\n",
    "            try:\n",
    "                log.create_log(f'{log_sub_dir}/{node_name}_losses', 'epoch', *loss_names)\n",
    "            except Exception as e:\n",
    "                pass\n",
    "\n",
    "            epoch_losses = [] # contains mean over each step for each loss\n",
    "            for loss_name in loss_names:\n",
    "                if len(node_wise_losses[node_name][loss_name]) != 0:\n",
    "                    epoch_losses.append(np.mean(node_wise_losses[node_name][loss_name]))\n",
    "                else:\n",
    "                    epoch_losses.append('n.a')\n",
    "            log.log_values(f'{log_sub_dir}/{node_name}_losses', epoch if pretrain else (epoch+pretrain_epochs), *epoch_losses)\n",
    "    \n",
    "    return test_info, log_dict\n",
    "\n",
    "def calculate_loss_pruned_net(net, nodes_with_relevant_protos, proto_features, pooled, out, ys, align_pf_weight, t_weight, unif_weight, cl_weight, OOD_loss_weight, orth_weight, net_normalization_multiplier, pretrain, finetune, criterion, train_iter, print=True, EPS=1e-10, root=None, label2name=None, node_accuracy=None, OOD_loss_required=False, kernel_orth=False):\n",
    "    batch_names = [label2name[y.item()] for y in ys]\n",
    "    loss = 0\n",
    "    class_loss = {}\n",
    "    a_loss_pf = {}\n",
    "    tanh_loss = {}\n",
    "    OOD_loss = {}\n",
    "    kernel_orth_loss = {}\n",
    "    uni_loss = {}\n",
    "    for node in root.nodes_with_children():\n",
    "        if node.name not in nodes_with_relevant_protos:\n",
    "            continue\n",
    "        children_idx = torch.tensor([name in node.descendents for name in batch_names])\n",
    "        batch_names_coarsest = [node.closest_descendent_for(name).name for name in batch_names if name in node.descendents]\n",
    "        node_y = torch.tensor([node.children_to_labels[name] for name in batch_names_coarsest]).cuda()\n",
    "\n",
    "        if len(node_y) == 0:\n",
    "            continue\n",
    "\n",
    "        # ys = torch.cat([node_y,node_y])\n",
    "\n",
    "        # pooled1, pooled2 = pooled[node.name].chunk(2)\n",
    "        # pooled1 = pooled1[children_idx]\n",
    "        # pooled2 = pooled2[children_idx]\n",
    "\n",
    "        pooled1, pooled2 = pooled[node.name][children_idx].chunk(2)\n",
    "\n",
    "        # pf1, pf2 = proto_features[node.name].chunk(2)\n",
    "        # pf1 = pf1[children_idx]\n",
    "        # pf2 = pf2[children_idx]\n",
    "\n",
    "        pf1, pf2 = proto_features[node.name][children_idx].chunk(2)\n",
    "\n",
    "        # out[node.name] = out[node.name][torch.cat([children_idx,children_idx])] # since out will have 2*batch_size samples\n",
    "        # node_logits = out[node.name][torch.cat([children_idx,children_idx])] # since out will have 2*batch_size samples\n",
    "        node_logits = out[node.name][children_idx]\n",
    "\n",
    "        embv2 = pf2.flatten(start_dim=2).permute(0,2,1).flatten(end_dim=1)\n",
    "        embv1 = pf1.flatten(start_dim=2).permute(0,2,1).flatten(end_dim=1)\n",
    "        \n",
    "        a_loss_pf[node.name] = (align_loss(embv1, embv2.detach()) \\\n",
    "                                + align_loss(embv2, embv1.detach())) / 2.\n",
    "\n",
    "        tanh_loss[node.name] = -(torch.log(torch.tanh(torch.sum(pooled1,dim=0))+EPS).mean() \\\n",
    "                                 + torch.log(torch.tanh(torch.sum(pooled2,dim=0))+EPS).mean()) / 2.\n",
    "        \n",
    "        if kernel_orth:\n",
    "            prototype_kernels = getattr(net.module, '_'+node.name+'_add_on')\n",
    "            classification_layer = getattr(net.module, '_'+node.name+'_classification')\n",
    "            # using any below because its a relevant prototype if it has strong connection to any one of the class\n",
    "            relevant_prototype_kernels = prototype_kernels.weight[(classification_layer.weight > 0.001).any(dim=0)]\n",
    "            kernel_orth_loss[node.name] = orth_dist(relevant_prototype_kernels)\n",
    "\n",
    "        if not finetune:\n",
    "            # pretraining or general training\n",
    "            loss += align_pf_weight * a_loss_pf[node.name]\n",
    "            loss += t_weight * tanh_loss[node.name]\n",
    "            if kernel_orth:\n",
    "                loss += orth_weight * kernel_orth_loss[node.name]\n",
    "        \n",
    "        if not pretrain:\n",
    "            # finetuning or general training\n",
    "            softmax_inputs = torch.log1p(node_logits**net_normalization_multiplier)\n",
    "            class_loss[node.name] = criterion(F.log_softmax((softmax_inputs),dim=1),node_y) # * (len(node_y) / len(ys[ys != OOD_LABEL]))\n",
    "            loss += cl_weight * class_loss[node.name]\n",
    "\n",
    "            if OOD_loss_required:\n",
    "                not_children_idx = torch.tensor([name not in node.descendents for name in batch_names]) # includes OOD images as well as images belonging to other nodes\n",
    "                OOD_logits = out[node.name][not_children_idx] # [sum(not_children_idx), node.num_children()]\n",
    "                sigmoid_out = F.sigmoid(torch.log1p(OOD_logits**net_normalization_multiplier))\n",
    "                OOD_loss[node.name] = F.binary_cross_entropy(sigmoid_out, torch.zeros_like(OOD_logits))\n",
    "                loss += OOD_loss_weight * OOD_loss[node.name]\n",
    "        # Our tanh-loss optimizes for uniformity and was sufficient for our experiments. However, if pretraining of the prototypes is not working well for your dataset, you may try to add another uniformity loss from https://www.tongzhouwang.info/hypersphere/ Just uncomment the following three lines\n",
    "        # else:\n",
    "        #     uni_loss[node.name] = (uniform_loss(F.normalize(pooled1+EPS,dim=1)) + uniform_loss(F.normalize(pooled2+EPS,dim=1)))/2.\n",
    "        #     loss += unif_weight * uni_loss[node.name]\n",
    "\n",
    "        # For debugging purpose\n",
    "        node_accuracy[node.name]['n_examples'] += node_y.shape[0]\n",
    "        _, node_coarsest_predicted = torch.max(node_logits.data, 1)\n",
    "        node_accuracy[node.name]['n_correct'] += (node_y == node_coarsest_predicted).sum().item()\n",
    "        for child in node.children:\n",
    "            node_accuracy[node.name]['children'][child.name]['n_examples'] += (node_y == node.children_to_labels[child.name]).sum().item()\n",
    "            node_accuracy[node.name]['children'][child.name]['n_correct'] += (node_coarsest_predicted[node_y == node.children_to_labels[child.name]] == node.children_to_labels[child.name]).sum().item()\n",
    "            node_accuracy[node.name]['preds'] = torch.cat((node_accuracy[node.name]['preds'], node_logits))\n",
    "            node_accuracy[node.name]['gts'] = torch.cat((node_accuracy[node.name]['gts'], node_y))\n",
    "\n",
    "    acc=0.\n",
    "    # if not pretrain:\n",
    "    #     ys_pred_max = torch.argmax(out, dim=1)\n",
    "    #     correct = torch.sum(torch.eq(ys_pred_max, ys))\n",
    "    #     acc = correct.item() / float(len(ys))\n",
    "    if print: \n",
    "        with torch.no_grad():\n",
    "            avg_a_loss_pf = np.mean([node_a_loss_pf.item() for node_name, node_a_loss_pf in a_loss_pf.items()])\n",
    "            avg_tanh_loss = np.mean([node_tanh_loss.item() for node_name, node_tanh_loss in tanh_loss.items()])\n",
    "\n",
    "            # optional loss, dict will be empty if not used, so setting the average to a placeholder vale\n",
    "            if len(kernel_orth_loss) > 0:\n",
    "                avg_kernel_orth_loss = np.mean([node_kernel_orth_loss.item() for node_name, node_kernel_orth_loss in kernel_orth_loss.items()])\n",
    "            else:\n",
    "                avg_kernel_orth_loss = -5 # placeholder value\n",
    "\n",
    "            # optional loss, dict will be empty if not used, so setting the average to a placeholder vale\n",
    "            if len(uni_loss) > 0:\n",
    "                avg_uni_loss = np.mean([node_uni_loss.item() for node_name, node_uni_loss in uni_loss.items()])\n",
    "            else:\n",
    "                avg_uni_loss = -5\n",
    "            \n",
    "            avg_class_loss = None\n",
    "            avg_OOD_loss = None\n",
    "            if pretrain:\n",
    "                # # optional loss, dict will be empty if not used, so setting the average to a placeholder vale\n",
    "                # if len(uni_loss) > 0:\n",
    "                #     avg_uni_loss = np.mean([node_uni_loss.item() for node_name, node_uni_loss in uni_loss.items()])\n",
    "                # else:\n",
    "                #     avg_uni_loss = -5\n",
    "                train_iter.set_postfix_str(\n",
    "                f'L: {loss.item():.3f}, LA:{avg_a_loss_pf.item():.2f}, LT:{avg_tanh_loss.item():.3f}, L_ORTH:{avg_kernel_orth_loss:.3f}, L_UNI:{avg_uni_loss:.3f}',refresh=False)\n",
    "            else:\n",
    "                avg_class_loss = np.mean([node_class_loss.item() for node_name, node_class_loss in class_loss.items()])\n",
    "                avg_OOD_loss = np.mean([node_OOD_loss.item() for node_name, node_OOD_loss in OOD_loss.items()]) if OOD_loss_required else -5\n",
    "                if finetune:\n",
    "                    train_iter.set_postfix_str(\n",
    "                    f'L:{loss.item():.3f},LC:{avg_class_loss.item():.3f}, LA:{avg_a_loss_pf.item():.2f}, LT:{avg_tanh_loss.item():.3f}, L_OOD:{avg_OOD_loss:.3f}',refresh=False)\n",
    "                else:\n",
    "                    train_iter.set_postfix_str(\n",
    "                    f'L:{loss.item():.3f},LC:{avg_class_loss.item():.3f}, LA:{avg_a_loss_pf.item():.2f}, LT:{avg_tanh_loss.item():.3f}, L_OOD:{avg_OOD_loss:.3f}, L_ORTH:{avg_kernel_orth_loss:.3f}',refresh=False)            \n",
    "    return loss, class_loss, a_loss_pf, tanh_loss, OOD_loss, kernel_orth_loss, uni_loss, avg_class_loss, avg_a_loss_pf, avg_tanh_loss, avg_OOD_loss, avg_kernel_orth_loss, avg_uni_loss, acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbfa1239",
   "metadata": {},
   "source": [
    "# Test pruned / replaced net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e476041d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chosen network is convnext\n",
      "Num classes (k) =  172 ['cub_005_Crested_Auklet', 'cub_006_Least_Auklet', 'cub_007_Parakeet_Auklet', 'cub_008_Rhinoceros_Auklet', 'cub_009_Brewer_Blackbird'] etc.\n",
      "-------------------------Using OOD data-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Epochlast: 100% 8/8 [00:06<00:00,  1.29it/s, L:10.261,LC:0.213, LA:0.02, LT:0.012, L_OOD:0.734, L_ORTH:0.001]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tFine accuracy: 0.2\n",
      "\tNode name: 052+053, acc: 95.0, f1:95.14, samples: 240, cub_052_Pied_billed_Grebe=60/60=1.0, 053+050=168/180=0.93\n",
      "\tNode name: 053+050, acc: 97.78, f1:97.78, samples: 180, cub_053_Western_Grebe=58/60=0.97, 050+051=118/120=0.98\n",
      "\tNode name: 004+032, acc: 94.64, f1:94.78, samples: 224, cub_004_Groove_billed_Ani=60/60=1.0, 032+033=152/164=0.93\n",
      "\tNode name: 086+045, acc: 10.75, f1:2.09, samples: 558, cub_086_Pacific_Loon=60/60=1.0, 045+101=0/498=0.0\n",
      "\tNode name: 050+051, acc: 83.33, f1:83.33, samples: 120, cub_050_Eared_Grebe=50/60=0.83, cub_051_Horned_Grebe=50/60=0.83\n",
      "\tNode name: 032+033, acc: 92.68, f1:92.77, samples: 164, cub_032_Mangrove_Cuckoo=42/46=0.91, 033+031=110/118=0.93\n",
      "\tNode name: 045+101, acc: 47.39, f1:30.47, samples: 498, 045+003=236/236=1.0, 101+023=0/262=0.0\n",
      "\tNode name: 033+031, acc: 88.14, f1:88.13, samples: 118, cub_033_Yellow_billed_Cuckoo=50/58=0.86, cub_031_Black_billed_Cuckoo=54/60=0.9\n",
      "\tNode name: 045+003, acc: 96.61, f1:96.65, samples: 236, cub_045_Northern_Fulmar=58/60=0.97, 003+002=170/176=0.97\n",
      "\tNode name: 101+023, acc: 97.71, f1:97.71, samples: 262, 101+100=96/100=0.96, 023+025=160/162=0.99\n",
      "\tNode name: 003+002, acc: 90.91, f1:91.11, samples: 176, cub_003_Sooty_Albatross=54/56=0.96, 002+001=106/120=0.88\n",
      "\tNode name: 101+100, acc: 94.0, f1:93.97, samples: 100, cub_101_White_Pelican=36/40=0.9, cub_100_Brown_Pelican=58/60=0.97\n",
      "\tNode name: 023+025, acc: 83.95, f1:83.56, samples: 162, cub_023_Brandt_Cormorant=40/58=0.69, 025+024=96/104=0.92\n",
      "\tNode name: 002+001, acc: 93.33, f1:93.33, samples: 120, cub_002_Laysan_Albatross=56/60=0.93, cub_001_Black_footed_Albatross=56/60=0.93\n",
      "\tNode name: 025+024, acc: 92.31, f1:92.31, samples: 104, cub_025_Pelagic_Cormorant=56/60=0.93, cub_024_Red_faced_Cormorant=40/44=0.91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer_net, optimizer_classifier, params_to_freeze, params_to_train, params_backbone = get_optimizer_nn(net, args)\n",
    "scheduler_net = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_net, T_max=len(trainloader_pretraining)*args.epochs_pretrain, eta_min=args.lr_block/100., last_epoch=-1)\n",
    "\n",
    "if args.epochs<=30:\n",
    "    scheduler_classifier = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer_classifier, T_0=5, eta_min=0.001, T_mult=1, verbose=False)\n",
    "else:\n",
    "    scheduler_classifier = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer_classifier, T_0=10, eta_min=0.001, T_mult=1, verbose=False)\n",
    "finetune = False\n",
    "\n",
    "if args.OOD_dataset:\n",
    "    trainloader_OOD, trainloader_pretraining_OOD, trainloader_normal_OOD, trainloader_normal_augment_OOD, projectloader_OOD, testloader_OOD, test_projectloader_OOD, _ = get_dataloaders(args, device, OOD=True)\n",
    "    print('-'*25 + 'Using OOD data' + '-'*25)\n",
    "else:\n",
    "    trainloader_OOD = trainloader_pretraining_OOD = trainloader_normal_OOD = trainloader_normal_augment_OOD = projectloader_OOD = testloader_OOD = test_projectloader_OOD = None\n",
    "    print('-'*25 + 'Not using OOD data' + '-'*25)\n",
    "\n",
    "test_info, pruned_log_dict = test_pruned_pipnet(pruned_net, nodes_with_relevant_protos, testloader, optimizer_net, optimizer_classifier, \\\n",
    "                                  scheduler_net, scheduler_classifier, criterion, epoch, \\\n",
    "                                    args.epochs, device, pretrain=False, finetune=finetune, \\\n",
    "                                    test_loader_OOD=testloader_OOD, kernel_orth=args.kernel_orth == 'y', \\\n",
    "                                        wandb_run=None, pretrain_epochs=args.epochs_pretrain, log=None, wandb_logging=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "73f7dcf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chosen network is convnext\n",
      "Num classes (k) =  172 ['cub_005_Crested_Auklet', 'cub_006_Least_Auklet', 'cub_007_Parakeet_Auklet', 'cub_008_Rhinoceros_Auklet', 'cub_009_Brewer_Blackbird'] etc.\n",
      "-------------------------Using OOD data-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Epochlast: 100% 8/8 [00:06<00:00,  1.28it/s, L:35.395,LC:0.552, LA:0.02, LT:0.012, L_OOD:0.722, L_ORTH:1.957]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tFine accuracy: 0.14\n",
      "\tNode name: 052+053, acc: 95.0, f1:95.14, samples: 240, cub_052_Pied_billed_Grebe=60/60=1.0, 053+050=168/180=0.93\n",
      "\tNode name: 053+050, acc: 67.78, f1:61.5, samples: 180, cub_053_Western_Grebe=10/60=0.17, 050+051=112/120=0.93\n",
      "\tNode name: 004+032, acc: 94.64, f1:94.78, samples: 224, cub_004_Groove_billed_Ani=60/60=1.0, 032+033=152/164=0.93\n",
      "\tNode name: 086+045, acc: 10.75, f1:2.09, samples: 558, cub_086_Pacific_Loon=60/60=1.0, 045+101=0/498=0.0\n",
      "\tNode name: 050+051, acc: 56.67, f1:51.25, samples: 120, cub_050_Eared_Grebe=14/60=0.23, cub_051_Horned_Grebe=54/60=0.9\n",
      "\tNode name: 032+033, acc: 71.95, f1:62.29, samples: 164, cub_032_Mangrove_Cuckoo=2/46=0.04, 033+031=116/118=0.98\n",
      "\tNode name: 045+101, acc: 47.39, f1:30.47, samples: 498, 045+003=236/236=1.0, 101+023=0/262=0.0\n",
      "\tNode name: 033+031, acc: 49.15, f1:47.15, samples: 118, cub_033_Yellow_billed_Cuckoo=40/58=0.69, cub_031_Black_billed_Cuckoo=18/60=0.3\n",
      "\tNode name: 045+003, acc: 72.88, f1:64.26, samples: 236, cub_045_Northern_Fulmar=2/60=0.03, 003+002=170/176=0.97\n",
      "\tNode name: 101+023, acc: 97.71, f1:97.71, samples: 262, 101+100=96/100=0.96, 023+025=160/162=0.99\n",
      "\tNode name: 003+002, acc: 68.18, f1:57.23, samples: 176, cub_003_Sooty_Albatross=2/56=0.04, 002+001=118/120=0.98\n",
      "\tNode name: 101+100, acc: 62.0, f1:49.38, samples: 100, cub_101_White_Pelican=2/40=0.05, cub_100_Brown_Pelican=60/60=1.0\n",
      "\tNode name: 023+025, acc: 62.96, f1:56.2, samples: 162, cub_023_Brandt_Cormorant=8/58=0.14, 025+024=94/104=0.9\n",
      "\tNode name: 002+001, acc: 33.33, f1:30.56, samples: 120, cub_002_Laysan_Albatross=8/60=0.13, cub_001_Black_footed_Albatross=32/60=0.53\n",
      "\tNode name: 025+024, acc: 38.46, f1:37.31, samples: 104, cub_025_Pelagic_Cormorant=32/60=0.53, cub_024_Red_faced_Cormorant=8/44=0.18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer_net, optimizer_classifier, params_to_freeze, params_to_train, params_backbone = get_optimizer_nn(net, args)\n",
    "scheduler_net = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_net, T_max=len(trainloader_pretraining)*args.epochs_pretrain, eta_min=args.lr_block/100., last_epoch=-1)\n",
    "\n",
    "if args.epochs<=30:\n",
    "    scheduler_classifier = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer_classifier, T_0=5, eta_min=0.001, T_mult=1, verbose=False)\n",
    "else:\n",
    "    scheduler_classifier = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer_classifier, T_0=10, eta_min=0.001, T_mult=1, verbose=False)\n",
    "finetune = False\n",
    "\n",
    "if args.OOD_dataset:\n",
    "    trainloader_OOD, trainloader_pretraining_OOD, trainloader_normal_OOD, trainloader_normal_augment_OOD, projectloader_OOD, testloader_OOD, test_projectloader_OOD, _ = get_dataloaders(args, device, OOD=True)\n",
    "    print('-'*25 + 'Using OOD data' + '-'*25)\n",
    "else:\n",
    "    trainloader_OOD = trainloader_pretraining_OOD = trainloader_normal_OOD = trainloader_normal_augment_OOD = projectloader_OOD = testloader_OOD = test_projectloader_OOD = None\n",
    "    print('-'*25 + 'Not using OOD data' + '-'*25)\n",
    "\n",
    "test_info, replaced_log_dict = test_pruned_pipnet(replaced_net, nodes_with_relevant_protos, testloader, optimizer_net, optimizer_classifier, \\\n",
    "                                  scheduler_net, scheduler_classifier, criterion, epoch, \\\n",
    "                                    args.epochs, device, pretrain=False, finetune=finetune, \\\n",
    "                                    test_loader_OOD=testloader_OOD, kernel_orth=args.kernel_orth == 'y', \\\n",
    "                                        wandb_run=None, pretrain_epochs=args.epochs_pretrain, log=None, wandb_logging=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "58574e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "053+050 before: 97.78 after: 67.78\n",
      "050+051 before: 83.33 after: 56.67\n",
      "032+033 before: 92.68 after: 71.95\n",
      "033+031 before: 88.14 after: 49.15\n",
      "045+003 before: 96.61 after: 72.88\n",
      "003+002 before: 90.91 after: 68.18\n",
      "101+100 before: 94.0 after: 62.0\n",
      "023+025 before: 83.95 after: 62.96\n",
      "002+001 before: 93.33 after: 33.33\n",
      "025+024 before: 92.31 after: 38.46\n"
     ]
    }
   ],
   "source": [
    "replaced_log_dict\n",
    "for node in root.nodes_with_children():\n",
    "    if acc_before_after[node.name]['replaced']:\n",
    "        print(node.name, 'before:', pruned_log_dict[f'test/node_wise/acc:{node.name}'], 'after:', replaced_log_dict[f'test/node_wise/acc:{node.name}'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "86bc339b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "052+053 before: 95.0 after: 95.0\n",
      "004+032 before: 94.64 after: 94.64\n",
      "086+045 before: 10.75 after: 10.75\n",
      "045+101 before: 47.39 after: 47.39\n",
      "101+023 before: 97.71 after: 97.71\n"
     ]
    }
   ],
   "source": [
    "replaced_log_dict\n",
    "for node in root.nodes_with_children():\n",
    "    if (not acc_before_after[node.name]['replaced']) and acc_before_after[node.name]['any_relevant_proto']:\n",
    "        print(node.name, 'before:', pruned_log_dict[f'test/node_wise/acc:{node.name}'], 'after:', replaced_log_dict[f'test/node_wise/acc:{node.name}'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8ebdad6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({'model_state_dict': replaced_net.state_dict(),\\\n",
    "            'optimizer_net_state_dict': optimizer_net.state_dict(),\\\n",
    "            'optimizer_classifier_state_dict': optimizer_classifier.state_dict()},\\\n",
    "           os.path.join(os.path.join(args.log_dir, 'checkpoints'),\\\n",
    "            f'net_overspecific_pruned_replaced_thresh={THRESHOLD}_{str(epoch)}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb70908",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
