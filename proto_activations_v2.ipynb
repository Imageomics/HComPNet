{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65792bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "import sys, os\n",
    "import random\n",
    "import numpy as np\n",
    "from shutil import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "import shutil\n",
    "import pickle\n",
    "import random\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fab72b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_path = '/home/harishbabu/projects/PIPNet/runs/010-CUB-27-imgnet_OOD_cnext26_img=224_nprotos=20'\n",
    "# run_path = '/home/harishbabu/projects/PIPNet/runs/031-CUB-18-imgnet_cnext26_img=224_nprotos=20_orth-on-rel'\n",
    "# run_path = '/home/harishbabu/projects/PIPNet/runs/032-CUB-18-imgnet_cnext26_img=224_nprotos=20_orth-on-rel'\n",
    "\n",
    "# run_path = '/home/harishbabu/projects/PIPNet/runs/035-CUB-18-imgnet_OOD_cnext26_img=224_nprotos=20_orth-on-rel'\n",
    "\n",
    "# run_path = '/home/harishbabu/projects/PIPNet/runs/043-035_clone-CUB-18-imgnet_OOD_cnext26_img=224_nprotos=20_orth-on-rel'\n",
    "# run_path = \"/home/harishbabu/projects/PIPNet/runs/036-CUB-18-imgnet_OOD_cnext26_img=224_nprotos=20_orth-on-rel_uniformity\"\n",
    "# run_path = \"/home/harishbabu/projects/PIPNet/runs/041-035_clone-CUB-18-imgnet_OOD_cnext26_img=224_nprotos=20_orth-on-rel\"\n",
    "# run_path = \"/home/harishbabu/projects/PIPNet/runs/042-035_clone-CUB-18-imgnet_OOD_cnext26_img=224_nprotos=20_orth-on-rel\"\n",
    "\n",
    "# run_path = \"/home/harishbabu/projects/PIPNet/runs/044-CUB-18-imgnet_OOD_cnext26_img=224_nprotos=20-or-4per-desc_orth-on-rel\"\n",
    "\n",
    "# run_path = \"/home/harishbabu/projects/PIPNet/runs/046-CUB-18-imgnet_OOD_cnext26_img=224_nprotos=10per-desc_orth-on-rel\"\n",
    "# run_path = \"/home/harishbabu/projects/PIPNet/runs/047-CUB-18-imgnet_OOD_cnext26_img=224_nprotos=5per-desc_tanh-desc\"\n",
    "# run_path = \"/home/harishbabu/projects/PIPNet/runs/048-CUB-18-imgnet_OOD_cnext26_img=224_nprotos=5per-desc_tanh-desc_unit-sphere\"\n",
    "# run_path = \"/home/harishbabu/projects/PIPNet/runs/051-CUB-18-imgnet_cnext26_img=224_nprotos=4per-desc_tanh-desc_unit-sphere_AW=5-TW=2-UW=2-CW=2\"\n",
    "# run_path = \"/home/harishbabu/projects/PIPNet/runs/052-CUB-18-imgnet_OOD_cnext26_img=224_nprotos=4per-desc_tanh-desc_unit-sphere_AW=5-TW=2-UW=2-CW=2\"\n",
    "# run_path = \"/home/harishbabu/projects/PIPNet/runs/055-CUB-18_cnext26_img=224_nprotos=4per-desc_unit-sphere_no-softmax_AW=3-TW=2-UW=3-CW=2\"\n",
    "# run_path = \"/home/harishbabu/projects/PIPNet/runs/056-CUB-18-imgnet_cnext26_img=224_nprotos=4per-desc_unit-sphere_no-softmax_AW=3-TW=2-UW=3-CW=2\"\n",
    "# run_path = \"/home/harishbabu/projects/PIPNet/runs/057-CUB-18-imgnet_cnext26_img=224_nprotos=4per-desc_unit-sphere_no-meanpool_no-softmax_AW=3-TW=2-UW=3-CW=2\"\n",
    "# run_path = \"/home/harishbabu/projects/PIPNet/runs/058-CUB-18-imgnet_with-equalize-aug_cnext26_img=224_nprotos=4per-desc_unit-sphere_no-meanpool_no-softmax_AW=3-TW=2-UW=3-CW=2\"\n",
    "\n",
    "# with unit sphere\n",
    "# run_path = \"/home/harishbabu/projects/PIPNet/runs/059-CUB-18-imgnet_with-equalize-aug_cnext26_img=224_nprotos=4per-desc_unit-sphere_finetune=5_no-meanpool_no-softmax_AW=3-TW=2-UW=3-CW=2_batch=20\"\n",
    "\n",
    "# unit sphere with softmax\n",
    "# run_path = \"/home/harishbabu/projects/PIPNet/runs/065-CUB-18-imgnet_with-equalize-aug_cnext26_img=224_nprotos=4per-desc_unit-sphere_finetune=5_no-meanpool_with-softmax_AW=3-TW=2-UW=3-CW=2_batch=20\"\n",
    "\n",
    "# original hpipnet with 20 protos per node no KO, no OOD, no tanh-desc\n",
    "# run_path = \"/home/harishbabu/projects/PIPNet/runs/062-CUB-18-imgnet_with-equalize-aug_cnext26_img=224_nprotos=20_no-KO_no-OOD\"\n",
    "\n",
    "# original hpipnet with 20 protos per node no KO, no OOD, WITH tanh-desc\n",
    "# run_path = \"/home/harishbabu/projects/PIPNet/runs/063-CUB-18-imgnet_with-equalize-aug_cnext26_img=224_nprotos=20_no-KO_no-OOD_tanh-desc\"\n",
    "\n",
    "# with unit sphere but no AL+UNI\n",
    "# run_path = \"/home/harishbabu/projects/PIPNet/runs/066-CUB-18-imgnet_with-equalize-aug_cnext26_img=224_nprotos=4per-desc_unit-sphere_finetune=5_no-meanpool_no-softmax_no-align_no-uni_AW=3-TW=2-UW=3-CW=2_batch=20\"\n",
    "\n",
    "# with unit sphere, protopool, with softmax, no tanh-desc\n",
    "# run_path = \"/home/harishbabu/projects/PIPNet/runs/067-CUB-18-imgnet_with-equalize-aug_cnext26_img=224_nprotos=4per-desc_unit-sphere-protopool_finetune=5_no-meanpool_with-softmax_AW=3-TW=2-UW=3-CW=2_batch=20\"\n",
    "\n",
    "# with unit sphere, protopool, with softmax, no tanh-desc, INCORRECT\n",
    "# run_path = \"/home/harishbabu/projects/PIPNet/runs/067-incorrect-CUB-18-imgnet_with-equalize-aug_cnext26_img=224_nprotos=4per-desc_unit-sphere-protopool_finetune=5_no-meanpool_with-softmax_AW=3-TW=2-UW=3-CW=2_batch=20\"\n",
    "\n",
    "# with unit sphere, protopool, no softmax, no tanh-desc\n",
    "# run_path = \"/home/harishbabu/projects/PIPNet/runs/068-CUB-18-imgnet_with-equalize-aug_cnext26_img=224_nprotos=4per-desc_unit-sphere-protopool_finetune=5_no-meanpool_no-softmax_AW=3-TW=2-UW=3-CW=2_batch=20\"\n",
    "\n",
    "# 071 with bias\n",
    "# run_path = \"/home/harishbabu/projects/PIPNet/runs/071-CUB-18-imgnet_with-equalize-aug_cnext26_img=224_nprotos=4per-desc_unit-sphere-protopool_finetune=5_no-meanpool_with-softmax_with-addon-bias_AW=3-TW=2-UW=3-CW=2_batch=20\"\n",
    "\n",
    "# 072 gumbel softmax, tau=0.5\n",
    "# run_path = \"/home/harishbabu/projects/PIPNet/runs/072-CUB-18-imgnet_with-equalize-aug_cnext26_img=224_nprotos=4per-desc_unit-sphere-protopool_finetune=5_no-meanpool_with-gumbel-softmax_no-addon-bias_AW=3-TW=2-UW=3-CW=2_batch=20\"\n",
    "\n",
    "# 074 multiply_cs_softmax\n",
    "run_path = \"/home/harishbabu/projects/PIPNet/runs/074-CUB-18-imgnet_with-equalize-aug_cnext26_img=224_nprotos=4per-desc_unit-sphere-protopool_finetune=5_no-meanpool_with-softmax_multi-cs-softmax_no-addon-bias_AW=3-TW=2-UW=3-CW=2_batch=20\"\n",
    "\n",
    "try:\n",
    "    sys.path.remove('/home/harishbabu/projects/PIPNet')\n",
    "except:\n",
    "    pass\n",
    "sys.path.insert(0, os.path.join(run_path, 'source_clone'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db0659a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heatmaps showing where a prototype is found will not be generated because OpenCV is not installed.\n"
     ]
    }
   ],
   "source": [
    "from pipnet.pipnet import PIPNet, get_network\n",
    "from util.log import Log\n",
    "from util.args import get_args, save_args, get_optimizer_nn\n",
    "from util.data import get_dataloaders\n",
    "from util.func import init_weights_xavier\n",
    "from pipnet.train import train_pipnet, test_pipnet\n",
    "# from pipnet.test import eval_pipnet, get_thresholds, eval_ood\n",
    "from util.eval_cub_csv import eval_prototypes_cub_parts_csv, get_topk_cub, get_proto_patches_cub\n",
    "from util.vis_pipnet import visualize, visualize_topk\n",
    "from util.visualize_prediction import vis_pred, vis_pred_experiments\n",
    "from util.node import Node\n",
    "from util.phylo_utils import construct_phylo_tree, construct_discretized_phylo_tree\n",
    "from util.func import get_patch_size\n",
    "from util.data import ModifiedLabelLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c1cf961",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pdb\n",
    "\n",
    "def get_heatmap(latent_activation, input_image):\n",
    "    image_a = latent_activation.cpu().numpy()\n",
    "    image_a = (image_a - image_a.min()) / (image_a.max() - image_a.min())\n",
    "\n",
    "    input_image = (input_image - input_image.min()) / (input_image.max() - input_image.min())\n",
    "    image_b = input_image.permute(1, 2, 0).cpu().numpy()\n",
    "    \n",
    "    reshaped_image_a = np.array(Image.fromarray((image_a[0] * 255).astype('uint8')).resize((input_image.shape[-1], input_image.shape[-1])))\n",
    "    normalized_heatmap = (reshaped_image_a - np.min(reshaped_image_a)) / (np.max(reshaped_image_a) - np.min(reshaped_image_a))\n",
    "    \n",
    "    heatmap_colormap = plt.get_cmap('jet')\n",
    "    heatmap_colored = heatmap_colormap(normalized_heatmap)\n",
    "    \n",
    "    heatmap_colored_uint8 = (heatmap_colored[:, :, :3] * 255).astype(np.uint8)\n",
    "    image_a_heatmap_pillow = Image.fromarray(heatmap_colored_uint8)\n",
    "    image_b_pillow = Image.fromarray((image_b * 255).astype('uint8'))\n",
    "    \n",
    "    result_image = Image.blend(image_b_pillow, image_a_heatmap_pillow, alpha=0.3)\n",
    "    \n",
    "    return np.array(result_image)\n",
    "\n",
    "\n",
    "def get_heatmap_uninterpolated(latent_activation, input_image):\n",
    "    image_a = latent_activation.cpu().numpy()\n",
    "    image_a = (image_a - image_a.min()) / (image_a.max() - image_a.min())\n",
    "\n",
    "    input_image = (input_image - input_image.min()) / (input_image.max() - input_image.min())\n",
    "    image_b = input_image.permute(1, 2, 0).cpu().numpy()\n",
    "    \n",
    "    reshaped_image_a = np.array(Image.fromarray((image_a[0] * 255).astype('uint8')).resize((input_image.shape[-1], input_image.shape[-1]), \\\n",
    "                                                                                          resample=Image.NEAREST ))\n",
    "    normalized_heatmap = (reshaped_image_a - np.min(reshaped_image_a)) / (np.max(reshaped_image_a) - np.min(reshaped_image_a))\n",
    "    \n",
    "    heatmap_colormap = plt.get_cmap('jet')\n",
    "    heatmap_colored = heatmap_colormap(normalized_heatmap)\n",
    "    \n",
    "    heatmap_colored_uint8 = (heatmap_colored[:, :, :3] * 255).astype(np.uint8)\n",
    "    image_a_heatmap_pillow = Image.fromarray(heatmap_colored_uint8)\n",
    "    image_b_pillow = Image.fromarray((image_b * 255).astype('uint8'))\n",
    "    \n",
    "    result_image = Image.blend(image_b_pillow, image_a_heatmap_pillow, alpha=0.3)\n",
    "    \n",
    "    return np.array(result_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32a4405",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70fb98b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------- No discretization -------------------------\n"
     ]
    }
   ],
   "source": [
    "args_file = open(os.path.join(run_path, 'metadata', 'args.pickle'), 'rb')\n",
    "args = pickle.load(args_file)\n",
    "\n",
    "if args.phylo_config:\n",
    "    phylo_config = OmegaConf.load(args.phylo_config)\n",
    "\n",
    "if args.phylo_config:\n",
    "    # construct the phylo tree\n",
    "    if phylo_config.phyloDistances_string == 'None':\n",
    "        if '031' in run_path: # this run uses a different phylogeny file that had an extra root node which is a mistake\n",
    "            root = construct_phylo_tree('/home/harishbabu/data/phlyogenyCUB/18Species-with-extra-root-node/1_tree-consensus-Hacket-18Species-modified_cub-names_v1.phy')\n",
    "        else:\n",
    "            root = construct_phylo_tree(phylo_config.phylogeny_path)\n",
    "        print('-'*25 + ' No discretization ' + '-'*25)\n",
    "    else:\n",
    "        root = construct_discretized_phylo_tree(phylo_config.phylogeny_path, phylo_config.phyloDistances_string)\n",
    "        print('-'*25 + ' Discretized ' + '-'*25)\n",
    "else:\n",
    "    # construct the tree (original hierarchy as described in the paper)\n",
    "    root = Node(\"root\")\n",
    "    root.add_children(['animal','vehicle','everyday_object','weapon','scuba_diver'])\n",
    "    root.add_children_to('animal',['non_primate','primate'])\n",
    "    root.add_children_to('non_primate',['African_elephant','giant_panda','lion'])\n",
    "    root.add_children_to('primate',['capuchin','gibbon','orangutan'])\n",
    "    root.add_children_to('vehicle',['ambulance','pickup','sports_car'])\n",
    "    root.add_children_to('everyday_object',['laptop','sandal','wine_bottle'])\n",
    "    root.add_children_to('weapon',['assault_rifle','rifle'])\n",
    "    # flat root\n",
    "    # root.add_children(['scuba_diver','African_elephant','giant_panda','lion','capuchin','gibbon','orangutan','ambulance','pickup','sports_car','laptop','sandal','wine_bottle','assault_rifle','rifle'])\n",
    "root.assign_all_descendents()\n",
    "\n",
    "if ('num_protos_per_descendant' in args) and (args.num_protos_per_descendant > 0):\n",
    "    for node in root.nodes_with_children():\n",
    "        node.set_num_protos(args.num_protos_per_descendant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b22ac247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping 0 samples from trainloader\n",
      "Dropping 0 samples from trainloader_pretraining\n",
      "Dropping 0 samples from trainloader_normal\n",
      "Dropping 0 samples from trainloader_normal_augment\n",
      "Num classes (k) =  18 ['cub_001_Black_footed_Albatross', 'cub_002_Laysan_Albatross', 'cub_003_Sooty_Albatross', 'cub_004_Groove_billed_Ani', 'cub_023_Brandt_Cormorant'] etc.\n",
      "Classes:  {'cub_001_Black_footed_Albatross': 0, 'cub_002_Laysan_Albatross': 1, 'cub_003_Sooty_Albatross': 2, 'cub_004_Groove_billed_Ani': 3, 'cub_023_Brandt_Cormorant': 4, 'cub_024_Red_faced_Cormorant': 5, 'cub_025_Pelagic_Cormorant': 6, 'cub_031_Black_billed_Cuckoo': 7, 'cub_032_Mangrove_Cuckoo': 8, 'cub_033_Yellow_billed_Cuckoo': 9, 'cub_045_Northern_Fulmar': 10, 'cub_050_Eared_Grebe': 11, 'cub_051_Horned_Grebe': 12, 'cub_052_Pied_billed_Grebe': 13, 'cub_053_Western_Grebe': 14, 'cub_086_Pacific_Loon': 15, 'cub_100_Brown_Pelican': 16, 'cub_101_White_Pelican': 17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harishbabu/.conda/envs/hpnet1/lib/python3.8/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of prototypes:  20\n",
      "----------Prototypes per descendant: 4----------\n",
      "Assigned 136 protos to node root\n",
      "Assigned 24 protos to node 052+053\n",
      "Assigned 104 protos to node 004+086\n",
      "Assigned 16 protos to node 053+050\n",
      "Assigned 24 protos to node 004+032\n",
      "Assigned 72 protos to node 086+045\n",
      "Assigned 8 protos to node 050+051\n",
      "Assigned 16 protos to node 032+033\n",
      "Assigned 64 protos to node 045+101\n",
      "Assigned 8 protos to node 033+031\n",
      "Assigned 24 protos to node 045+003\n",
      "Assigned 32 protos to node 101+023\n",
      "Assigned 16 protos to node 003+002\n",
      "Assigned 8 protos to node 101+100\n",
      "Assigned 16 protos to node 023+025\n",
      "Assigned 8 protos to node 002+001\n",
      "Assigned 8 protos to node 025+024\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    device_ids = [torch.cuda.current_device()]\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    device_ids = []\n",
    "\n",
    "args_file = open(os.path.join(run_path, 'metadata', 'args.pickle'), 'rb')\n",
    "args = pickle.load(args_file)\n",
    "\n",
    "# ckpt_file_name = 'net_overspecific_pruned_replaced_thresh=0.5_last'\n",
    "ckpt_file_name = 'net_trained_last'\n",
    "# ckpt_file_name = 'net_trained_10'\n",
    "# ckpt_file_name = 'net_pretrained'\n",
    "epoch = ckpt_file_name.split('_')[-1]\n",
    "\n",
    "ckpt_path = os.path.join(run_path, 'checkpoints', ckpt_file_name)\n",
    "checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "\n",
    "if ckpt_file_name != 'net_trained_last':\n",
    "    print('\\n', (10*'-')+'WARNING: Not using the final trained model'+(10*'-'), '\\n')\n",
    "\n",
    "# Obtain the dataset and dataloaders\n",
    "trainloader, trainloader_pretraining, trainloader_normal, trainloader_normal_augment, projectloader, testloader, test_projectloader, classes = get_dataloaders(args, device)\n",
    "if len(classes)<=20:\n",
    "    if args.validation_size == 0.:\n",
    "        print(\"Classes: \", testloader.dataset.class_to_idx, flush=True)\n",
    "    else:\n",
    "        print(\"Classes: \", str(classes), flush=True)\n",
    "\n",
    "# Create a convolutional network based on arguments and add 1x1 conv layer\n",
    "feature_net, add_on_layers, pool_layer, classification_layers, num_prototypes = get_network(len(classes), args, root=root)\n",
    "   \n",
    "# Create a PIP-Net\n",
    "net = PIPNet(num_classes=len(classes),\n",
    "                    num_prototypes=num_prototypes,\n",
    "                    feature_net = feature_net,\n",
    "                    args = args,\n",
    "                    add_on_layers = add_on_layers,\n",
    "                    pool_layer = pool_layer,\n",
    "                    classification_layers = classification_layers,\n",
    "                    num_parent_nodes = len(root.nodes_with_children()),\n",
    "                    root = root\n",
    "                    )\n",
    "net = net.to(device=device)\n",
    "net = nn.DataParallel(net, device_ids = device_ids)    \n",
    "net.load_state_dict(checkpoint['model_state_dict'],strict=True)\n",
    "net.eval()\n",
    "criterion = nn.NLLLoss(reduction='mean').to(device)\n",
    "\n",
    "# Forward one batch through the backbone to get the latent output size\n",
    "# with torch.no_grad():\n",
    "#     xs1, _, _ = next(iter(trainloader))\n",
    "#     xs1 = xs1.to(device)\n",
    "#     proto_features, _, _ = net(xs1)\n",
    "#     wshape = proto_features['root'].shape[-1]\n",
    "#     args.wshape = wshape #needed for calculating image patch size\n",
    "#     print(\"Output shape: \", proto_features['root'].shape, flush=True)\n",
    "    \n",
    "args.wshape = 26"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6def86c",
   "metadata": {},
   "source": [
    "# Proto activations on leaf descendents - topk images using either NAIVE-HPIPNET or UNIT-SPACE-PROTOPOOL with HEATMAP w/o INTERPOLATION with SOFTMAX SCORE and COSINE SIMILARITY (for experiment 074)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2e0f269",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 511it [00:29, 17.09it/s]\n",
      "/tmp/ipykernel_96780/3535226083.py:35: DeprecationWarning: NEAREST is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.NEAREST or Dither.NONE instead.\n",
      "  resample=Image.NEAREST ))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node root\n",
      "\t Child: 004+086\n",
      "\t\tProto:0 001:(0.004) 002:(0.0024) 003:(0.0027) 004:(0.0021) 023:(0.0027) 024:(0.0023) 025:(0.0019) 031:(0.0037) 032:(0.0021) 033:(0.0034) 045:(0.0026) 086:(0.0028) 100:(0.0018) 101:(0.0018) \n",
      "\t\tProto:1 001:(0.0011) 002:(0.0011) 003:(0.0012) 004:(0.0027) 023:(0.0029) 024:(0.0033) 025:(0.0028) 031:(0.0023) 032:(0.0018) 033:(0.0013) 045:(0.0008) 086:(0.0006) 100:(0.0015) 101:(0.0008) \n",
      "\t\tProto:2 001:(0.4898) 002:(0.5064) 003:(0.5492) 004:(0.2951) 023:(0.0176) 024:(0.1426) 025:(0.0377) 031:(0.477) 032:(0.3838) 033:(0.4694) 045:(0.4916) 086:(0.0719) 100:(0.414) 101:(0.4485) \n",
      "\t\tProto:3 001:(0.0015) 002:(0.0015) 003:(0.0016) 004:(0.0017) 023:(0.0013) 024:(0.001) 025:(0.0012) 031:(0.0022) 032:(0.0022) 033:(0.002) 045:(0.002) 086:(0.0014) 100:(0.0013) 101:(0.0014) \n",
      "\t\tProto:4 001:(0.001) 002:(0.0011) 003:(0.001) 004:(0.0012) 023:(0.0011) 024:(0.0011) 025:(0.0011) 031:(0.0012) 032:(0.001) 033:(0.0011) 045:(0.001) 086:(0.0008) 100:(0.0009) 101:(0.0008) \n",
      "\t\tProto:128 001:(0.1672) 002:(0.3507) 003:(0.1627) 004:(0.1674) 023:(0.0151) 024:(0.204) 025:(0.0651) 031:(0.1917) 032:(0.1887) 033:(0.2656) 045:(0.0082) 086:(0.0014) 100:(0.479) 101:(0.4909) \n",
      "\t\tProto:6 001:(0.1684) 002:(0.2389) 003:(0.2568) 004:(0.269) 023:(0.2892) 024:(0.3602) 025:(0.2819) 031:(0.3038) 032:(0.2935) 033:(0.3694) 045:(0.1493) 086:(0.0102) 100:(0.3038) 101:(0.2323) \n",
      "\t\tProto:129 001:(0.0011) 002:(0.0014) 003:(0.0013) 004:(0.0013) 023:(0.0012) 024:(0.0012) 025:(0.0011) 031:(0.0012) 032:(0.0011) 033:(0.0012) 045:(0.0012) 086:(0.0011) 100:(0.0011) 101:(0.001) \n",
      "\t\tProto:130 001:(0.4694) 002:(0.4459) 003:(0.4574) 004:(0.4525) 023:(0.4344) 024:(0.338) 025:(0.4152) 031:(0.4555) 032:(0.3887) 033:(0.4221) 045:(0.3217) 086:(0.2499) 100:(0.2839) 101:(0.1221) \n",
      "\t\tProto:131 001:(0.3745) 002:(0.3899) 003:(0.4279) 004:(0.5709) 023:(0.3322) 024:(0.4059) 025:(0.3948) 031:(0.5083) 032:(0.4653) 033:(0.5326) 045:(0.3677) 086:(0.3033) 100:(0.3271) 101:(0.3311) \n",
      "\t\tProto:10 001:(0.0782) 002:(0.0298) 003:(0.0379) 004:(0.0614) 023:(0.3641) 024:(0.117) 025:(0.3899) 031:(0.0306) 032:(0.0311) 033:(0.0323) 045:(0.0235) 086:(0.4217) 100:(0.0839) 101:(0.0109) \n",
      "\t\tProto:132 001:(0.001) 002:(0.0015) 003:(0.0009) 004:(0.0032) 023:(0.001) 024:(0.0008) 025:(0.0009) 031:(0.0032) 032:(0.0036) 033:(0.0032) 045:(0.001) 086:(0.0009) 100:(0.0014) 101:(0.0009) \n",
      "\t\tProto:12 001:(0.0012) 002:(0.0012) 003:(0.0011) 004:(0.0011) 023:(0.0011) 024:(0.0011) 025:(0.0012) 031:(0.001) 032:(0.0011) 033:(0.0012) 045:(0.001) 086:(0.0009) 100:(0.0011) 101:(0.001) \n",
      "\t\tProto:13 001:(0.017) 002:(0.0114) 003:(0.0111) 004:(0.0016) 023:(0.0075) 024:(0.0019) 025:(0.0072) 031:(0.002) 032:(0.0035) 033:(0.0035) 045:(0.0177) 086:(0.0306) 100:(0.0042) 101:(0.0348) \n",
      "\t\tProto:14 001:(0.3714) 002:(0.3743) 003:(0.296) 004:(0.1505) 023:(0.0145) 024:(0.0583) 025:(0.0603) 031:(0.3558) 032:(0.3187) 033:(0.3942) 045:(0.2496) 086:(0.0238) 100:(0.3773) 101:(0.3636) \n",
      "\t\tProto:134 001:(0.0014) 002:(0.0011) 003:(0.0013) 004:(0.0015) 023:(0.0015) 024:(0.0015) 025:(0.0016) 031:(0.0017) 032:(0.0013) 033:(0.0014) 045:(0.0013) 086:(0.0007) 100:(0.0024) 101:(0.0015) \n",
      "\t\tProto:16 001:(0.2007) 002:(0.2728) 003:(0.2373) 004:(0.2645) 023:(0.3092) 024:(0.2526) 025:(0.3026) 031:(0.2501) 032:(0.2676) 033:(0.2451) 045:(0.2238) 086:(0.3782) 100:(0.3442) 101:(0.3267) \n",
      "\t\tProto:20 001:(0.3671) 002:(0.3364) 003:(0.3838) 004:(0.517) 023:(0.369) 024:(0.4061) 025:(0.4248) 031:(0.4112) 032:(0.4052) 033:(0.4257) 045:(0.2836) 086:(0.0323) 100:(0.4484) 101:(0.2931) \n",
      "\t\tProto:21 001:(0.0013) 002:(0.0013) 003:(0.0015) 004:(0.0011) 023:(0.0016) 024:(0.0012) 025:(0.0013) 031:(0.0011) 032:(0.0012) 033:(0.0011) 045:(0.0012) 086:(0.0015) 100:(0.0016) 101:(0.0011) \n",
      "\t\tProto:22 001:(0.4365) 002:(0.4508) 003:(0.4678) 004:(0.02) 023:(0.0036) 024:(0.3105) 025:(0.0038) 031:(0.2562) 032:(0.0292) 033:(0.2447) 045:(0.4566) 086:(0.0014) 100:(0.0071) 101:(0.0529) \n",
      "\t\tProto:23 001:(0.0012) 002:(0.001) 003:(0.0013) 004:(0.0013) 023:(0.0013) 024:(0.0013) 025:(0.0012) 031:(0.0012) 032:(0.0012) 033:(0.0011) 045:(0.0011) 086:(0.0007) 100:(0.0012) 101:(0.0007) \n",
      "\t\tProto:24 001:(0.3886) 002:(0.4172) 003:(0.2904) 004:(0.0555) 023:(0.2107) 024:(0.0375) 025:(0.0325) 031:(0.2828) 032:(0.18) 033:(0.1709) 045:(0.0911) 086:(0.1554) 100:(0.4585) 101:(0.4434) \n",
      "\t\tProto:26 001:(0.2721) 002:(0.2176) 003:(0.2395) 004:(0.4082) 023:(0.3786) 024:(0.3976) 025:(0.4362) 031:(0.2687) 032:(0.2907) 033:(0.3292) 045:(0.1486) 086:(0.0697) 100:(0.4238) 101:(0.21) \n",
      "\t\tProto:27 001:(0.3764) 002:(0.3697) 003:(0.3911) 004:(0.0324) 023:(0.4094) 024:(0.3102) 025:(0.2561) 031:(0.3297) 032:(0.1867) 033:(0.1166) 045:(0.2738) 086:(0.4034) 100:(0.1169) 101:(0.0169) \n",
      "\t\tProto:28 001:(0.0012) 002:(0.0011) 003:(0.0008) 004:(0.0013) 023:(0.0012) 024:(0.0011) 025:(0.0011) 031:(0.0012) 032:(0.0013) 033:(0.0012) 045:(0.0009) 086:(0.0009) 100:(0.0011) 101:(0.0008) \n",
      "\t\tProto:31 001:(0.001) 002:(0.0009) 003:(0.001) 004:(0.0008) 023:(0.0008) 024:(0.0009) 025:(0.001) 031:(0.0008) 032:(0.0008) 033:(0.0009) 045:(0.0009) 086:(0.0006) 100:(0.0008) 101:(0.0008) \n",
      "\t\tProto:32 001:(0.0022) 002:(0.0025) 003:(0.0027) 004:(0.0019) 023:(0.0018) 024:(0.0019) 025:(0.0018) 031:(0.0018) 032:(0.002) 033:(0.0018) 045:(0.002) 086:(0.0017) 100:(0.0023) 101:(0.002) \n",
      "\t\tProto:34 001:(0.0451) 002:(0.0136) 003:(0.0733) 004:(0.0062) 023:(0.2901) 024:(0.1524) 025:(0.2547) 031:(0.0019) 032:(0.0023) 033:(0.0018) 045:(0.004) 086:(0.2295) 100:(0.0035) 101:(0.0026) \n",
      "\t\tProto:35 001:(0.3484) 002:(0.3876) 003:(0.2432) 004:(0.0262) 023:(0.0208) 024:(0.04) 025:(0.0054) 031:(0.1088) 032:(0.0134) 033:(0.081) 045:(0.2678) 086:(0.1888) 100:(0.0123) 101:(0.1539) \n",
      "\t\tProto:36 001:(0.2072) 002:(0.3291) 003:(0.2917) 004:(0.0108) 023:(0.1234) 024:(0.0436) 025:(0.1557) 031:(0.317) 032:(0.1587) 033:(0.324) 045:(0.2183) 086:(0.3966) 100:(0.0259) 101:(0.2399) \n",
      "\t\tProto:37 001:(0.4536) 002:(0.4037) 003:(0.0167) 004:(0.0041) 023:(0.0027) 024:(0.0333) 025:(0.0024) 031:(0.0243) 032:(0.0038) 033:(0.0061) 045:(0.0121) 086:(0.0702) 100:(0.0149) 101:(0.0067) \n",
      "\t\tProto:41 001:(0.0012) 002:(0.0015) 003:(0.0013) 004:(0.002) 023:(0.0022) 024:(0.0022) 025:(0.0022) 031:(0.002) 032:(0.0019) 033:(0.0018) 045:(0.0011) 086:(0.0005) 100:(0.0016) 101:(0.0007) \n",
      "\t\tProto:42 001:(0.0015) 002:(0.0013) 003:(0.0012) 004:(0.0014) 023:(0.0012) 024:(0.0013) 025:(0.0015) 031:(0.0015) 032:(0.0015) 033:(0.0016) 045:(0.0013) 086:(0.0014) 100:(0.001) 101:(0.0009) \n",
      "\t\tProto:45 001:(0.3369) 002:(0.3501) 003:(0.2632) 004:(0.0133) 023:(0.0029) 024:(0.0312) 025:(0.0095) 031:(0.0778) 032:(0.0439) 033:(0.0676) 045:(0.2752) 086:(0.0015) 100:(0.3436) 101:(0.3391) \n",
      "\t\tProto:48 001:(0.0011) 002:(0.001) 003:(0.001) 004:(0.0009) 023:(0.0009) 024:(0.001) 025:(0.001) 031:(0.0008) 032:(0.001) 033:(0.0009) 045:(0.0011) 086:(0.0011) 100:(0.001) 101:(0.0008) \n",
      "\t\tProto:49 001:(0.3181) 002:(0.3205) 003:(0.3268) 004:(0.4038) 023:(0.3671) 024:(0.3894) 025:(0.4071) 031:(0.3132) 032:(0.2782) 033:(0.3655) 045:(0.2549) 086:(0.2354) 100:(0.3789) 101:(0.2744) \n",
      "\t\tProto:52 001:(0.0014) 002:(0.0021) 003:(0.0018) 004:(0.0014) 023:(0.0012) 024:(0.0017) 025:(0.0009) 031:(0.0021) 032:(0.0017) 033:(0.0018) 045:(0.0011) 086:(0.0006) 100:(0.0029) 101:(0.0021) \n",
      "\t\tProto:54 001:(0.4219) 002:(0.4232) 003:(0.367) 004:(0.426) 023:(0.3433) 024:(0.3204) 025:(0.465) 031:(0.3772) 032:(0.3551) 033:(0.4) 045:(0.3412) 086:(0.2566) 100:(0.4007) 101:(0.2874) \n",
      "\t\tProto:56 001:(0.3402) 002:(0.2344) 003:(0.3527) 004:(0.0215) 023:(0.3765) 024:(0.2149) 025:(0.2365) 031:(0.1466) 032:(0.0369) 033:(0.064) 045:(0.2894) 086:(0.4343) 100:(0.0175) 101:(0.0332) \n",
      "\t\tProto:57 001:(0.0039) 002:(0.0036) 003:(0.0033) 004:(0.0012) 023:(0.002) 024:(0.001) 025:(0.0015) 031:(0.0015) 032:(0.0013) 033:(0.0016) 045:(0.0042) 086:(0.0019) 100:(0.0025) 101:(0.002) \n",
      "\t\tProto:58 001:(0.2896) 002:(0.2535) 003:(0.2754) 004:(0.327) 023:(0.2302) 024:(0.2086) 025:(0.2155) 031:(0.336) 032:(0.274) 033:(0.3273) 045:(0.2252) 086:(0.1001) 100:(0.212) 101:(0.1377) \n",
      "\t\tProto:59 001:(0.1741) 002:(0.1936) 003:(0.1807) 004:(0.2084) 023:(0.2944) 024:(0.2337) 025:(0.22) 031:(0.2859) 032:(0.1973) 033:(0.2208) 045:(0.1496) 086:(0.1267) 100:(0.0352) 101:(0.0363) \n",
      "\t\tProto:62 001:(0.0012) 002:(0.001) 003:(0.0014) 004:(0.0008) 023:(0.0008) 024:(0.001) 025:(0.0009) 031:(0.0008) 032:(0.0008) 033:(0.0009) 045:(0.0013) 086:(0.0008) 100:(0.0008) 101:(0.0007) \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tProto:63 001:(0.0067) 002:(0.0076) 003:(0.0075) 004:(0.0075) 023:(0.0072) 024:(0.0072) 025:(0.0082) 031:(0.0061) 032:(0.0085) 033:(0.0106) 045:(0.0031) 086:(0.0022) 100:(0.0127) 101:(0.0033) \n",
      "\t\tProto:64 001:(0.0458) 002:(0.0234) 003:(0.0158) 004:(0.2555) 023:(0.3144) 024:(0.4249) 025:(0.2936) 031:(0.0068) 032:(0.0571) 033:(0.0623) 045:(0.0051) 086:(0.003) 100:(0.0455) 101:(0.0019) \n",
      "\t\tProto:65 001:(0.0011) 002:(0.001) 003:(0.0012) 004:(0.0009) 023:(0.0009) 024:(0.0009) 025:(0.0009) 031:(0.001) 032:(0.0017) 033:(0.001) 045:(0.0011) 086:(0.001) 100:(0.0009) 101:(0.0013) \n",
      "\t\tProto:66 001:(0.1329) 002:(0.1116) 003:(0.1028) 004:(0.2048) 023:(0.0442) 024:(0.0754) 025:(0.1517) 031:(0.2463) 032:(0.2512) 033:(0.2514) 045:(0.0588) 086:(0.0052) 100:(0.3072) 101:(0.3644) \n",
      "\t\tProto:69 001:(0.3264) 002:(0.2926) 003:(0.3696) 004:(0.4716) 023:(0.2299) 024:(0.36) 025:(0.3494) 031:(0.4692) 032:(0.4064) 033:(0.488) 045:(0.2729) 086:(0.0372) 100:(0.3018) 101:(0.1936) \n",
      "\t\tProto:70 001:(0.0009) 002:(0.0008) 003:(0.001) 004:(0.0013) 023:(0.0011) 024:(0.0013) 025:(0.0014) 031:(0.001) 032:(0.0011) 033:(0.0012) 045:(0.0009) 086:(0.0006) 100:(0.001) 101:(0.0007) \n",
      "\t\tProto:71 001:(0.0011) 002:(0.0009) 003:(0.001) 004:(0.001) 023:(0.0009) 024:(0.0009) 025:(0.001) 031:(0.0009) 032:(0.0009) 033:(0.001) 045:(0.001) 086:(0.0008) 100:(0.0009) 101:(0.0009) \n",
      "\t\tProto:74 001:(0.3298) 002:(0.3493) 003:(0.416) 004:(0.4617) 023:(0.2648) 024:(0.3731) 025:(0.3983) 031:(0.4534) 032:(0.3758) 033:(0.4309) 045:(0.3783) 086:(0.0094) 100:(0.2014) 101:(0.19) \n",
      "\t\tProto:76 001:(0.001) 002:(0.001) 003:(0.001) 004:(0.0009) 023:(0.0011) 024:(0.0013) 025:(0.0012) 031:(0.001) 032:(0.0009) 033:(0.0013) 045:(0.0009) 086:(0.0007) 100:(0.0011) 101:(0.0007) \n",
      "\t\tProto:77 001:(0.3845) 002:(0.4136) 003:(0.4344) 004:(0.5695) 023:(0.4164) 024:(0.4417) 025:(0.4625) 031:(0.2936) 032:(0.2159) 033:(0.2299) 045:(0.386) 086:(0.372) 100:(0.3894) 101:(0.3147) \n",
      "\t\tProto:79 001:(0.0016) 002:(0.0016) 003:(0.0016) 004:(0.0018) 023:(0.0017) 024:(0.0018) 025:(0.0019) 031:(0.0017) 032:(0.0016) 033:(0.0018) 045:(0.0016) 086:(0.0009) 100:(0.0014) 101:(0.001) \n",
      "\t\tProto:81 001:(0.1855) 002:(0.1356) 003:(0.2427) 004:(0.0243) 023:(0.3135) 024:(0.1773) 025:(0.2687) 031:(0.1969) 032:(0.0834) 033:(0.0551) 045:(0.1213) 086:(0.3516) 100:(0.233) 101:(0.1257) \n",
      "\t\tProto:83 001:(0.3239) 002:(0.3213) 003:(0.0107) 004:(0.0026) 023:(0.0019) 024:(0.1603) 025:(0.0057) 031:(0.0037) 032:(0.003) 033:(0.0079) 045:(0.026) 086:(0.0045) 100:(0.0027) 101:(0.0047) \n",
      "\t\tProto:86 001:(0.2173) 002:(0.2357) 003:(0.3079) 004:(0.1027) 023:(0.2944) 024:(0.2048) 025:(0.0559) 031:(0.1608) 032:(0.1985) 033:(0.0441) 045:(0.236) 086:(0.168) 100:(0.0694) 101:(0.0422) \n",
      "\t\tProto:87 001:(0.0008) 002:(0.0009) 003:(0.0009) 004:(0.0009) 023:(0.001) 024:(0.001) 025:(0.001) 031:(0.0008) 032:(0.0008) 033:(0.0009) 045:(0.0009) 086:(0.0009) 100:(0.001) 101:(0.0008) \n",
      "\t\tProto:92 001:(0.4394) 002:(0.4489) 003:(0.4316) 004:(0.4315) 023:(0.2511) 024:(0.3616) 025:(0.4072) 031:(0.4337) 032:(0.4256) 033:(0.4442) 045:(0.3404) 086:(0.3165) 100:(0.3863) 101:(0.2981) \n",
      "\t\tProto:93 001:(0.2615) 002:(0.2018) 003:(0.043) 004:(0.2566) 023:(0.3096) 024:(0.2513) 025:(0.3364) 031:(0.3943) 032:(0.3248) 033:(0.429) 045:(0.1878) 086:(0.3947) 100:(0.0118) 101:(0.0052) \n",
      "\t\tProto:95 001:(0.0027) 002:(0.0024) 003:(0.0022) 004:(0.0029) 023:(0.0031) 024:(0.0026) 025:(0.0028) 031:(0.003) 032:(0.0024) 033:(0.0024) 045:(0.0019) 086:(0.0019) 100:(0.0027) 101:(0.0014) \n",
      "\t\tProto:96 001:(0.0016) 002:(0.0018) 003:(0.0018) 004:(0.0019) 023:(0.0018) 024:(0.0018) 025:(0.0018) 031:(0.0017) 032:(0.0016) 033:(0.0015) 045:(0.0015) 086:(0.001) 100:(0.0013) 101:(0.001) \n",
      "\t\tProto:103 001:(0.2595) 002:(0.2125) 003:(0.0831) 004:(0.0418) 023:(0.1922) 024:(0.1722) 025:(0.1527) 031:(0.1657) 032:(0.1957) 033:(0.2071) 045:(0.146) 086:(0.3911) 100:(0.022) 101:(0.1596) \n",
      "\t\tProto:104 001:(0.0016) 002:(0.0015) 003:(0.0016) 004:(0.0019) 023:(0.0014) 024:(0.002) 025:(0.0021) 031:(0.0019) 032:(0.0018) 033:(0.0021) 045:(0.0015) 086:(0.001) 100:(0.0018) 101:(0.0013) \n",
      "\t\tProto:105 001:(0.0009) 002:(0.0009) 003:(0.0008) 004:(0.0008) 023:(0.0007) 024:(0.0008) 025:(0.0008) 031:(0.0008) 032:(0.0008) 033:(0.0008) 045:(0.001) 086:(0.0005) 100:(0.0007) 101:(0.0006) \n",
      "\t\tProto:107 001:(0.0009) 002:(0.0008) 003:(0.0011) 004:(0.0007) 023:(0.0011) 024:(0.001) 025:(0.0008) 031:(0.0008) 032:(0.0007) 033:(0.0009) 045:(0.0008) 086:(0.0008) 100:(0.0007) 101:(0.0005) \n",
      "\t\tProto:108 001:(0.0011) 002:(0.0011) 003:(0.0011) 004:(0.0011) 023:(0.001) 024:(0.0011) 025:(0.0012) 031:(0.001) 032:(0.001) 033:(0.001) 045:(0.001) 086:(0.0011) 100:(0.0012) 101:(0.0009) \n",
      "\t\tProto:111 001:(0.0013) 002:(0.0012) 003:(0.0013) 004:(0.0014) 023:(0.0013) 024:(0.0014) 025:(0.0014) 031:(0.0016) 032:(0.0015) 033:(0.0017) 045:(0.0015) 086:(0.0009) 100:(0.0015) 101:(0.001) \n",
      "\t\tProto:112 001:(0.0026) 002:(0.0021) 003:(0.0032) 004:(0.0026) 023:(0.0017) 024:(0.0023) 025:(0.0027) 031:(0.0024) 032:(0.0022) 033:(0.0026) 045:(0.0021) 086:(0.0006) 100:(0.0028) 101:(0.0034) \n",
      "\t\tProto:114 001:(0.0009) 002:(0.0009) 003:(0.0009) 004:(0.0008) 023:(0.0008) 024:(0.0008) 025:(0.0009) 031:(0.0008) 032:(0.0008) 033:(0.0009) 045:(0.0009) 086:(0.0009) 100:(0.0009) 101:(0.0007) \n",
      "\t\tProto:121 001:(0.0012) 002:(0.0011) 003:(0.001) 004:(0.001) 023:(0.001) 024:(0.001) 025:(0.0008) 031:(0.0009) 032:(0.0008) 033:(0.0009) 045:(0.0011) 086:(0.0008) 100:(0.0009) 101:(0.0008) \n",
      "\t\tProto:123 001:(0.3025) 002:(0.287) 003:(0.3653) 004:(0.402) 023:(0.3424) 024:(0.3542) 025:(0.3733) 031:(0.3612) 032:(0.3469) 033:(0.3762) 045:(0.3173) 086:(0.0218) 100:(0.3223) 101:(0.1569) \n",
      "\t\tProto:124 001:(0.0013) 002:(0.0013) 003:(0.001) 004:(0.0016) 023:(0.0013) 024:(0.0013) 025:(0.0014) 031:(0.0017) 032:(0.0016) 033:(0.0016) 045:(0.0011) 086:(0.0012) 100:(0.001) 101:(0.0008) \n",
      "\t\tProto:125 001:(0.2716) 002:(0.2377) 003:(0.1164) 004:(0.086) 023:(0.2843) 024:(0.164) 025:(0.2825) 031:(0.2627) 032:(0.1902) 033:(0.2608) 045:(0.1919) 086:(0.4049) 100:(0.025) 101:(0.1868) \n",
      "\t\tProto:127 001:(0.0009) 002:(0.001) 003:(0.001) 004:(0.0011) 023:(0.0011) 024:(0.0012) 025:(0.0012) 031:(0.0011) 032:(0.0011) 033:(0.0012) 045:(0.001) 086:(0.0008) 100:(0.0013) 101:(0.0009) \n",
      "\t Child: 052+053\n",
      "\t\tProto:133 050:(0.3417) 051:(0.3592) 052:(0.3712) 053:(0.3385) \n",
      "\t\tProto:135 050:(0.4324) 051:(0.4482) 052:(0.4788) 053:(0.3091) \n",
      "\t\tProto:113 050:(0.5154) 051:(0.5251) 052:(0.5416) 053:(0.4235) \n",
      "\t\tProto:51 050:(0.3929) 051:(0.3884) 052:(0.4527) 053:(0.3056) \n",
      "\t\tProto:84 050:(0.5235) 051:(0.5339) 052:(0.5495) 053:(0.4654) \n",
      "\t\tProto:116 050:(0.4158) 051:(0.3733) 052:(0.0117) 053:(0.4108) \n",
      "\t\tProto:120 050:(0.4649) 051:(0.4684) 052:(0.4818) 053:(0.3827) \n",
      "\t\tProto:25 050:(0.3523) 051:(0.3531) 052:(0.3702) 053:(0.3549) \n",
      "\t\tProto:122 050:(0.3464) 051:(0.3116) 052:(0.3602) 053:(0.2847) \n",
      "\t\tProto:61 050:(0.3315) 051:(0.3412) 052:(0.3122) 053:(0.3522) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 120it [00:03, 37.95it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 052+053\n",
      "\t Child: 053+050\n",
      "\t\tProto:2 050:(0.5218) 051:(0.4555) 053:(0.4458) \n",
      "\t\tProto:6 050:(0.4848) 051:(0.4191) 053:(0.1902) \n",
      "\t\tProto:10 050:(0.0162) 051:(0.0312) 053:(0.4592) \n",
      "\t\tProto:15 050:(0.5941) 051:(0.5771) 053:(0.659) \n",
      "\t\tProto:16 050:(0.4263) 051:(0.3663) 053:(0.4272) \n",
      "\t\tProto:18 050:(0.3311) 051:(0.4146) 053:(0.4237) \n",
      "\t\tProto:22 050:(0.5118) 051:(0.5005) 053:(0.5007) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 391it [00:16, 24.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 004+086\n",
      "\t Child: 086+045\n",
      "\t\tProto:5 001:(0.3103) 002:(0.2379) 003:(0.3405) 023:(0.2663) 024:(0.2549) 025:(0.2414) 045:(0.2739) 086:(0.2774) 100:(0.1571) 101:(0.213) \n",
      "\t\tProto:7 001:(0.2097) 002:(0.2853) 003:(0.2992) 023:(0.4347) 024:(0.3932) 025:(0.4108) 045:(0.1839) 086:(0.437) 100:(0.4141) 101:(0.3915) \n",
      "\t\tProto:8 001:(0.337) 002:(0.1852) 003:(0.1847) 023:(0.4057) 024:(0.0902) 025:(0.2938) 045:(0.354) 086:(0.481) 100:(0.3505) 101:(0.3434) \n",
      "\t\tProto:9 001:(0.0524) 002:(0.0081) 003:(0.0089) 023:(0.0303) 024:(0.0043) 025:(0.0193) 045:(0.0297) 086:(0.045) 100:(0.024) 101:(0.0081) \n",
      "\t\tProto:10 001:(0.4254) 002:(0.4574) 003:(0.4603) 023:(0.279) 024:(0.3487) 025:(0.2203) 045:(0.1063) 086:(0.2892) 100:(0.3702) 101:(0.3137) \n",
      "\t\tProto:11 001:(0.4343) 002:(0.4391) 003:(0.3597) 023:(0.3223) 024:(0.2702) 025:(0.1594) 045:(0.0297) 086:(0.2858) 100:(0.3975) 101:(0.3432) \n",
      "\t\tProto:20 001:(0.0035) 002:(0.0019) 003:(0.0019) 023:(0.0038) 024:(0.0017) 025:(0.002) 045:(0.0037) 086:(0.0045) 100:(0.0022) 101:(0.0037) \n",
      "\t\tProto:23 001:(0.0014) 002:(0.0014) 003:(0.0015) 023:(0.0014) 024:(0.0015) 025:(0.0013) 045:(0.0016) 086:(0.0015) 100:(0.0014) 101:(0.0015) \n",
      "\t\tProto:25 001:(0.0012) 002:(0.002) 003:(0.0015) 023:(0.0018) 024:(0.0008) 025:(0.0014) 045:(0.0022) 086:(0.0023) 100:(0.0031) 101:(0.0018) \n",
      "\t\tProto:26 001:(0.4515) 002:(0.4269) 003:(0.3254) 023:(0.3499) 024:(0.2595) 025:(0.3475) 045:(0.3919) 086:(0.4476) 100:(0.4154) 101:(0.4258) \n",
      "\t\tProto:27 001:(0.3366) 002:(0.2173) 003:(0.1624) 023:(0.2453) 024:(0.0531) 025:(0.2091) 045:(0.2875) 086:(0.4079) 100:(0.379) 101:(0.2878) \n",
      "\t\tProto:28 001:(0.2771) 002:(0.1001) 003:(0.2072) 023:(0.1529) 024:(0.0532) 025:(0.0527) 045:(0.2882) 086:(0.2065) 100:(0.0968) 101:(0.1947) \n",
      "\t\tProto:33 001:(0.316) 002:(0.3755) 003:(0.3061) 023:(0.2085) 024:(0.2667) 025:(0.0781) 045:(0.3258) 086:(0.0815) 100:(0.2919) 101:(0.3423) \n",
      "\t\tProto:37 001:(0.0016) 002:(0.0013) 003:(0.0014) 023:(0.0016) 024:(0.0013) 025:(0.0012) 045:(0.0016) 086:(0.0018) 100:(0.0014) 101:(0.0015) \n",
      "\t\tProto:43 001:(0.0154) 002:(0.0326) 003:(0.0174) 023:(0.0285) 024:(0.0117) 025:(0.0251) 045:(0.0275) 086:(0.0463) 100:(0.0278) 101:(0.0554) \n",
      "\t\tProto:48 001:(0.3628) 002:(0.2786) 003:(0.2651) 023:(0.3266) 024:(0.0795) 025:(0.1635) 045:(0.3364) 086:(0.3866) 100:(0.2298) 101:(0.3443) \n",
      "\t\tProto:50 001:(0.0008) 002:(0.0012) 003:(0.0011) 023:(0.001) 024:(0.0009) 025:(0.0008) 045:(0.0014) 086:(0.0012) 100:(0.0009) 101:(0.001) \n",
      "\t\tProto:51 001:(0.0036) 002:(0.0027) 003:(0.0028) 023:(0.0031) 024:(0.0025) 025:(0.0027) 045:(0.0027) 086:(0.0032) 100:(0.0025) 101:(0.0019) \n",
      "\t\tProto:53 001:(0.5188) 002:(0.5424) 003:(0.453) 023:(0.5056) 024:(0.1801) 025:(0.2588) 045:(0.5086) 086:(0.5891) 100:(0.3032) 101:(0.4118) \n",
      "\t\tProto:54 001:(0.0014) 002:(0.0021) 003:(0.0017) 023:(0.0016) 024:(0.0017) 025:(0.0012) 045:(0.0018) 086:(0.0023) 100:(0.0015) 101:(0.0014) \n",
      "\t\tProto:55 001:(0.4037) 002:(0.4098) 003:(0.4067) 023:(0.4399) 024:(0.3523) 025:(0.4395) 045:(0.4343) 086:(0.4386) 100:(0.4185) 101:(0.4449) \n",
      "\t\tProto:56 001:(0.0013) 002:(0.0014) 003:(0.0016) 023:(0.0013) 024:(0.0013) 025:(0.0012) 045:(0.0015) 086:(0.0015) 100:(0.002) 101:(0.0016) \n",
      "\t\tProto:61 001:(0.4087) 002:(0.343) 003:(0.389) 023:(0.4331) 024:(0.3725) 025:(0.4974) 045:(0.3374) 086:(0.4132) 100:(0.4066) 101:(0.3512) \n",
      "\t\tProto:62 001:(0.0012) 002:(0.001) 003:(0.001) 023:(0.0011) 024:(0.0009) 025:(0.0009) 045:(0.0009) 086:(0.0011) 100:(0.0011) 101:(0.0008) \n",
      "\t\tProto:64 001:(0.4645) 002:(0.4317) 003:(0.4379) 023:(0.4493) 024:(0.3885) 025:(0.4544) 045:(0.4258) 086:(0.4403) 100:(0.3667) 101:(0.4556) \n",
      "\t\tProto:67 001:(0.5356) 002:(0.4771) 003:(0.5406) 023:(0.4922) 024:(0.3692) 025:(0.4408) 045:(0.487) 086:(0.4997) 100:(0.4584) 101:(0.469) \n",
      "\t\tProto:68 001:(0.1082) 002:(0.4727) 003:(0.3223) 023:(0.0034) 024:(0.002) 025:(0.0019) 045:(0.4226) 086:(0.0215) 100:(0.2644) 101:(0.2744) \n",
      "\t\tProto:73 001:(0.001) 002:(0.0011) 003:(0.0009) 023:(0.0012) 024:(0.0009) 025:(0.0009) 045:(0.0013) 086:(0.0012) 100:(0.001) 101:(0.0012) \n",
      "\t\tProto:76 001:(0.0093) 002:(0.0226) 003:(0.0059) 023:(0.0114) 024:(0.1108) 025:(0.0046) 045:(0.0114) 086:(0.0095) 100:(0.0909) 101:(0.1742) \n",
      "\t\tProto:78 001:(0.2163) 002:(0.0936) 003:(0.1235) 023:(0.418) 024:(0.0111) 025:(0.1315) 045:(0.0649) 086:(0.4567) 100:(0.0168) 101:(0.0206) \n",
      "\t\tProto:82 001:(0.0013) 002:(0.001) 003:(0.0012) 023:(0.001) 024:(0.0011) 025:(0.0012) 045:(0.0013) 086:(0.0011) 100:(0.0011) 101:(0.001) \n",
      "\t\tProto:83 001:(0.0018) 002:(0.0013) 003:(0.0011) 023:(0.0029) 024:(0.0011) 025:(0.0014) 045:(0.0017) 086:(0.003) 100:(0.0021) 101:(0.0013) \n",
      "\t\tProto:85 001:(0.3722) 002:(0.1989) 003:(0.3839) 023:(0.1002) 024:(0.2223) 025:(0.237) 045:(0.3576) 086:(0.0255) 100:(0.0689) 101:(0.0156) \n",
      "\t\tProto:87 001:(0.0018) 002:(0.0012) 003:(0.0014) 023:(0.0015) 024:(0.0011) 025:(0.0009) 045:(0.0012) 086:(0.0017) 100:(0.0011) 101:(0.0012) \n",
      "\t\tProto:90 001:(0.0014) 002:(0.0013) 003:(0.0015) 023:(0.0026) 024:(0.0015) 025:(0.0015) 045:(0.0016) 086:(0.0019) 100:(0.0017) 101:(0.0017) \n",
      "\t\tProto:92 001:(0.3232) 002:(0.1231) 003:(0.283) 023:(0.3845) 024:(0.3522) 025:(0.4612) 045:(0.2459) 086:(0.4147) 100:(0.4373) 101:(0.0724) \n",
      "\t\tProto:93 001:(0.0016) 002:(0.0019) 003:(0.0023) 023:(0.0024) 024:(0.0022) 025:(0.0014) 045:(0.0016) 086:(0.0017) 100:(0.0024) 101:(0.001) \n",
      "\t\tProto:94 001:(0.4868) 002:(0.3057) 003:(0.341) 023:(0.4154) 024:(0.3586) 025:(0.4112) 045:(0.3407) 086:(0.5329) 100:(0.3027) 101:(0.3416) \n",
      "\t\tProto:95 001:(0.366) 002:(0.312) 003:(0.4001) 023:(0.4104) 024:(0.2985) 025:(0.4108) 045:(0.2647) 086:(0.4675) 100:(0.3533) 101:(0.3335) \n",
      "\t\tProto:98 001:(0.0012) 002:(0.0009) 003:(0.001) 023:(0.001) 024:(0.001) 025:(0.0009) 045:(0.0011) 086:(0.0015) 100:(0.0011) 101:(0.0011) \n",
      "\t\tProto:99 001:(0.4507) 002:(0.3026) 003:(0.2304) 023:(0.4126) 024:(0.0816) 025:(0.2542) 045:(0.4444) 086:(0.4921) 100:(0.1638) 101:(0.3905) \n",
      "\t\tProto:103 001:(0.5015) 002:(0.472) 003:(0.5127) 023:(0.455) 024:(0.4112) 025:(0.4437) 045:(0.3713) 086:(0.3926) 100:(0.274) 101:(0.11) \n",
      "\t Child: 004+032\n",
      "\t\tProto:97 004:(0.4676) 031:(0.5405) 032:(0.5077) 033:(0.5278) \n",
      "\t\tProto:69 004:(0.2275) 031:(0.4302) 032:(0.3903) 033:(0.4314) \n",
      "\t\tProto:38 004:(0.2889) 031:(0.4086) 032:(0.3891) 033:(0.422) \n",
      "\t\tProto:79 004:(0.4418) 031:(0.4414) 032:(0.381) 033:(0.3852) \n",
      "\t\tProto:16 004:(0.3007) 031:(0.3984) 032:(0.3693) 033:(0.4105) \n",
      "\t\tProto:81 004:(0.3984) 031:(0.5011) 032:(0.4255) 033:(0.4934) \n",
      "\t\tProto:24 004:(0.331) 031:(0.5198) 032:(0.5037) 033:(0.5319) \n",
      "\t\tProto:88 004:(0.3677) 031:(0.4567) 032:(0.4736) 033:(0.4397) \n",
      "\t\tProto:29 004:(0.0108) 031:(0.4424) 032:(0.1403) 033:(0.1514) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 90it [00:02, 34.63it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 053+050\n",
      "\t Child: 050+051\n",
      "\t\tProto:3 050:(0.2528) 051:(0.2399) \n",
      "\t\tProto:4 050:(0.3866) 051:(0.4398) \n",
      "\t\tProto:6 050:(0.684) 051:(0.6864) \n",
      "\t\tProto:7 050:(0.7054) 051:(0.6844) \n",
      "\t\tProto:9 050:(0.4123) 051:(0.0389) \n",
      "\t\tProto:13 050:(0.5701) 051:(0.5694) \n",
      "\t\tProto:15 050:(0.6835) 051:(0.6722) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 112it [00:03, 35.78it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 004+032\n",
      "\t Child: 032+033\n",
      "\t\tProto:0 031:(0.0079) 032:(0.0074) 033:(0.0078) \n",
      "\t\tProto:4 031:(0.5934) 032:(0.5363) 033:(0.6321) \n",
      "\t\tProto:6 031:(0.522) 032:(0.5401) 033:(0.5575) \n",
      "\t\tProto:7 031:(0.0108) 032:(0.0082) 033:(0.0118) \n",
      "\t\tProto:8 031:(0.455) 032:(0.0152) 033:(0.1334) \n",
      "\t\tProto:10 031:(0.0067) 032:(0.0066) 033:(0.0068) \n",
      "\t\tProto:11 031:(0.2265) 032:(0.5406) 033:(0.4883) \n",
      "\t\tProto:12 031:(0.0102) 032:(0.0099) 033:(0.0103) \n",
      "\t\tProto:13 031:(0.0057) 032:(0.0053) 033:(0.0057) \n",
      "\t\tProto:15 031:(0.1949) 032:(0.0638) 033:(0.0163) \n",
      "\t\tProto:17 031:(0.008) 032:(0.0071) 033:(0.0075) \n",
      "\t\tProto:19 031:(0.5985) 032:(0.553) 033:(0.617) \n",
      "\t\tProto:20 031:(0.531) 032:(0.5075) 033:(0.5191) \n",
      "\t\tProto:21 031:(0.0117) 032:(0.0115) 033:(0.0126) \n",
      "\t\tProto:22 031:(0.4798) 032:(0.2544) 033:(0.2222) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 279it [00:09, 30.94it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 086+045\n",
      "\t Child: 045+101\n",
      "\t\tProto:0 001:(0.0013) 002:(0.0013) 003:(0.0015) 023:(0.0016) 024:(0.0014) 025:(0.0014) 045:(0.0012) 100:(0.0013) 101:(0.0009) \n",
      "\t\tProto:1 001:(0.431) 002:(0.2989) 003:(0.4169) 023:(0.3033) 024:(0.2996) 025:(0.2936) 045:(0.4258) 100:(0.2269) 101:(0.2135) \n",
      "\t\tProto:2 001:(0.4546) 002:(0.4525) 003:(0.4102) 023:(0.3019) 024:(0.34) 025:(0.3141) 045:(0.2854) 100:(0.1876) 101:(0.0194) \n",
      "\t\tProto:3 001:(0.4462) 002:(0.4044) 003:(0.4212) 023:(0.3874) 024:(0.3977) 025:(0.4523) 045:(0.3081) 100:(0.4226) 101:(0.2429) \n",
      "\t\tProto:4 001:(0.3121) 002:(0.3076) 003:(0.3747) 023:(0.3102) 024:(0.3416) 025:(0.4016) 045:(0.3165) 100:(0.3254) 101:(0.329) \n",
      "\t\tProto:5 001:(0.2772) 002:(0.3149) 003:(0.2564) 023:(0.0452) 024:(0.2114) 025:(0.0693) 045:(0.2684) 100:(0.3474) 101:(0.3631) \n",
      "\t\tProto:6 001:(0.4125) 002:(0.0026) 003:(0.0033) 023:(0.0036) 024:(0.0015) 025:(0.0017) 045:(0.0254) 100:(0.0017) 101:(0.0009) \n",
      "\t\tProto:8 001:(0.0013) 002:(0.0012) 003:(0.0011) 023:(0.0013) 024:(0.0014) 025:(0.0018) 045:(0.0013) 100:(0.0016) 101:(0.0009) \n",
      "\t\tProto:9 001:(0.0014) 002:(0.0015) 003:(0.0014) 023:(0.0015) 024:(0.0016) 025:(0.0017) 045:(0.0014) 100:(0.0017) 101:(0.0013) \n",
      "\t\tProto:18 001:(0.0017) 002:(0.0013) 003:(0.0014) 023:(0.0014) 024:(0.0014) 025:(0.0014) 045:(0.0016) 100:(0.0013) 101:(0.0009) \n",
      "\t\tProto:21 001:(0.3997) 002:(0.2248) 003:(0.3517) 023:(0.1849) 024:(0.2263) 025:(0.2053) 045:(0.3132) 100:(0.1173) 101:(0.0168) \n",
      "\t\tProto:22 001:(0.0018) 002:(0.0015) 003:(0.0025) 023:(0.0019) 024:(0.0031) 025:(0.0035) 045:(0.0019) 100:(0.0023) 101:(0.0013) \n",
      "\t\tProto:26 001:(0.3051) 002:(0.3863) 003:(0.3987) 023:(0.1227) 024:(0.2165) 025:(0.2599) 045:(0.3634) 100:(0.2869) 101:(0.306) \n",
      "\t\tProto:27 001:(0.4808) 002:(0.4689) 003:(0.5246) 023:(0.4688) 024:(0.4877) 025:(0.4886) 045:(0.3498) 100:(0.4117) 101:(0.2992) \n",
      "\t\tProto:35 001:(0.035) 002:(0.0092) 003:(0.0081) 023:(0.2754) 024:(0.3757) 025:(0.34) 045:(0.0032) 100:(0.0427) 101:(0.002) \n",
      "\t\tProto:37 001:(0.0076) 002:(0.0524) 003:(0.0062) 023:(0.0023) 024:(0.0121) 025:(0.0029) 045:(0.0095) 100:(0.0706) 101:(0.0369) \n",
      "\t\tProto:39 001:(0.3045) 002:(0.2644) 003:(0.1475) 023:(0.3225) 024:(0.3269) 025:(0.2919) 045:(0.1799) 100:(0.2023) 101:(0.2198) \n",
      "\t\tProto:44 001:(0.0014) 002:(0.0016) 003:(0.0014) 023:(0.0018) 024:(0.0019) 025:(0.0017) 045:(0.0013) 100:(0.0015) 101:(0.0012) \n",
      "\t\tProto:45 001:(0.3097) 002:(0.4766) 003:(0.2721) 023:(0.0229) 024:(0.3698) 025:(0.1361) 045:(0.0598) 100:(0.4954) 101:(0.515) \n",
      "\t\tProto:47 001:(0.4635) 002:(0.4494) 003:(0.4703) 023:(0.1895) 024:(0.2739) 025:(0.2891) 045:(0.1427) 100:(0.4256) 101:(0.3647) \n",
      "\t\tProto:51 001:(0.4435) 002:(0.3046) 003:(0.4309) 023:(0.3539) 024:(0.3605) 025:(0.3675) 045:(0.4086) 100:(0.3286) 101:(0.0635) \n",
      "\t\tProto:55 001:(0.2725) 002:(0.3901) 003:(0.2196) 023:(0.0821) 024:(0.294) 025:(0.2336) 045:(0.3562) 100:(0.2936) 101:(0.3604) \n",
      "\t\tProto:56 001:(0.3527) 002:(0.4054) 003:(0.1937) 023:(0.0055) 024:(0.0061) 025:(0.0061) 045:(0.3638) 100:(0.0067) 101:(0.1809) \n",
      "\t\tProto:60 001:(0.3402) 002:(0.2399) 003:(0.2424) 023:(0.4373) 024:(0.4586) 025:(0.5208) 045:(0.111) 100:(0.4791) 101:(0.0819) \n",
      "\t\tProto:65 001:(0.182) 002:(0.0751) 003:(0.0076) 023:(0.2578) 024:(0.3624) 025:(0.2578) 045:(0.0407) 100:(0.1208) 101:(0.1886) \n",
      "\t\tProto:69 001:(0.2995) 002:(0.2764) 003:(0.2812) 023:(0.2875) 024:(0.3159) 025:(0.3151) 045:(0.18) 100:(0.3133) 101:(0.3033) \n",
      "\t\tProto:70 001:(0.0024) 002:(0.0023) 003:(0.0017) 023:(0.0021) 024:(0.0023) 025:(0.0013) 045:(0.0025) 100:(0.0015) 101:(0.0009) \n",
      "\t\tProto:71 001:(0.0023) 002:(0.003) 003:(0.0019) 023:(0.0035) 024:(0.0047) 025:(0.0048) 045:(0.0016) 100:(0.004) 101:(0.0013) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 82it [00:02, 31.80it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 032+033\n",
      "\t Child: 033+031\n",
      "\t\tProto:1 031:(0.6211) 033:(0.6204) \n",
      "\t\tProto:3 031:(0.6945) 033:(0.6729) \n",
      "\t\tProto:7 031:(0.3736) 033:(0.3172) \n",
      "\t\tProto:9 031:(0.3463) 033:(0.2937) \n",
      "\t\tProto:11 031:(0.5184) 033:(0.4656) \n",
      "\t\tProto:12 031:(0.6617) 033:(0.6498) \n",
      "\t\tProto:13 031:(0.6314) 033:(0.5321) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 249it [00:08, 29.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 045+101\n",
      "\t Child: 101+023\n",
      "\t\tProto:3 023:(0.3701) 024:(0.3427) 025:(0.2778) 100:(0.5149) 101:(0.288) \n",
      "\t\tProto:37 023:(0.0183) 024:(0.0044) 025:(0.0097) 100:(0.0126) 101:(0.0061) \n",
      "\t\tProto:6 023:(0.4151) 024:(0.3909) 025:(0.4339) 100:(0.3824) 101:(0.0097) \n",
      "\t\tProto:7 023:(0.4544) 024:(0.4923) 025:(0.4803) 100:(0.4969) 101:(0.4496) \n",
      "\t\tProto:8 023:(0.0082) 024:(0.0123) 025:(0.0113) 100:(0.012) 101:(0.0079) \n",
      "\t\tProto:9 023:(0.3241) 024:(0.3232) 025:(0.3171) 100:(0.3304) 101:(0.4055) \n",
      "\t\tProto:41 023:(0.2953) 024:(0.2444) 025:(0.1369) 100:(0.377) 101:(0.3986) \n",
      "\t\tProto:59 023:(0.4559) 024:(0.4719) 025:(0.4924) 100:(0.3213) 101:(0.2944) \n",
      "\t\tProto:14 023:(0.2668) 024:(0.2894) 025:(0.3106) 100:(0.1134) 101:(0.0198) \n",
      "\t\tProto:48 023:(0.2118) 024:(0.1611) 025:(0.2557) 100:(0.278) 101:(0.1981) \n",
      "\t\tProto:18 023:(0.441) 024:(0.4066) 025:(0.4365) 100:(0.415) 101:(0.3706) \n",
      "\t\tProto:19 023:(0.384) 024:(0.3278) 025:(0.4315) 100:(0.4914) 101:(0.4532) \n",
      "\t\tProto:51 023:(0.2951) 024:(0.2314) 025:(0.3017) 100:(0.4541) 101:(0.4439) \n",
      "\t\tProto:22 023:(0.2939) 024:(0.3376) 025:(0.2944) 100:(0.4127) 101:(0.4024) \n",
      "\t\tProto:24 023:(0.3796) 024:(0.3659) 025:(0.3565) 100:(0.1866) 101:(0.2562) \n",
      "\t\tProto:27 023:(0.3968) 024:(0.398) 025:(0.4145) 100:(0.3301) 101:(0.3113) \n",
      "\t\tProto:62 023:(0.3879) 024:(0.4615) 025:(0.4402) 100:(0.4446) 101:(0.4134) \n",
      "\t Child: 045+003\n",
      "\t\tProto:33 001:(0.5297) 002:(0.4895) 003:(0.4899) 045:(0.4356) \n",
      "\t\tProto:34 001:(0.6096) 002:(0.594) 003:(0.6138) 045:(0.4985) \n",
      "\t\tProto:4 001:(0.4273) 002:(0.4914) 003:(0.4667) 045:(0.407) \n",
      "\t\tProto:39 001:(0.0103) 002:(0.0061) 003:(0.008) 045:(0.009) \n",
      "\t\tProto:44 001:(0.0082) 002:(0.0061) 003:(0.0093) 045:(0.0072) \n",
      "\t\tProto:49 001:(0.4157) 002:(0.4272) 003:(0.4228) 045:(0.2565) \n",
      "\t\tProto:20 001:(0.0058) 002:(0.0077) 003:(0.0089) 045:(0.0068) \n",
      "\t\tProto:25 001:(0.4413) 002:(0.4802) 003:(0.3998) 045:(0.4563) \n",
      "\t\tProto:57 001:(0.5689) 002:(0.5716) 003:(0.5561) 045:(0.6071) \n",
      "\t\tProto:61 001:(0.4926) 002:(0.3801) 003:(0.5279) 045:(0.5395) \n",
      "\t\tProto:30 001:(0.4652) 002:(0.4888) 003:(0.4717) 045:(0.5294) \n",
      "\t\tProto:31 001:(0.4681) 002:(0.4482) 003:(0.388) 045:(0.1132) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 118it [00:03, 35.82it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 045+003\n",
      "\t Child: 003+002\n",
      "\t\tProto:5 001:(0.5214) 002:(0.5277) 003:(0.4371) \n",
      "\t\tProto:6 001:(0.3393) 002:(0.2839) 003:(0.2456) \n",
      "\t\tProto:7 001:(0.5713) 002:(0.4519) 003:(0.1698) \n",
      "\t\tProto:9 001:(0.4411) 002:(0.658) 003:(0.5329) \n",
      "\t\tProto:10 001:(0.5803) 002:(0.6444) 003:(0.6084) \n",
      "\t\tProto:12 001:(0.4978) 002:(0.0133) 003:(0.0109) \n",
      "\t\tProto:15 001:(0.4334) 002:(0.4403) 003:(0.45) \n",
      "\t\tProto:17 001:(0.5505) 002:(0.5216) 003:(0.5846) \n",
      "\t\tProto:19 001:(0.623) 002:(0.6028) 003:(0.567) \n",
      "\t\tProto:21 001:(0.4637) 002:(0.4713) 003:(0.4801) \n",
      "\t\tProto:22 001:(0.4282) 002:(0.4525) 003:(0.2733) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 131it [00:04, 30.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 101+023\n",
      "\t Child: 023+025\n",
      "\t\tProto:0 023:(0.5316) 024:(0.5492) 025:(0.5698) \n",
      "\t\tProto:3 023:(0.5084) 024:(0.4597) 025:(0.5161) \n",
      "\t\tProto:4 023:(0.4423) 024:(0.4673) 025:(0.4698) \n",
      "\t\tProto:6 023:(0.3737) 024:(0.3824) 025:(0.3238) \n",
      "\t\tProto:12 023:(0.3687) 024:(0.3835) 025:(0.4526) \n",
      "\t\tProto:13 023:(0.037) 024:(0.3079) 025:(0.0497) \n",
      "\t\tProto:14 023:(0.42) 024:(0.3654) 025:(0.4376) \n",
      "\t\tProto:17 023:(0.5212) 024:(0.452) 025:(0.5035) \n",
      "\t\tProto:20 023:(0.4371) 024:(0.3043) 025:(0.3064) \n",
      "\t\tProto:22 023:(0.3068) 024:(0.3184) 025:(0.2965) \n",
      "\t\tProto:27 023:(0.3807) 024:(0.3448) 025:(0.4086) \n",
      "\t\tProto:31 023:(0.483) 024:(0.4578) 025:(0.4719) \n",
      "\t Child: 101+100\n",
      "\t\tProto:1 100:(0.5787) 101:(0.5605) \n",
      "\t\tProto:2 100:(0.5724) 101:(0.553) \n",
      "\t\tProto:9 100:(0.4134) 101:(0.1247) \n",
      "\t\tProto:16 100:(0.6195) 101:(0.5887) \n",
      "\t\tProto:21 100:(0.3627) 101:(0.4418) \n",
      "\t\tProto:25 100:(0.6201) 101:(0.6747) \n",
      "\t\tProto:30 100:(0.6134) 101:(0.6072) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 88it [00:02, 33.81it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 003+002\n",
      "\t Child: 002+001\n",
      "\t\tProto:0 001:(0.7191) 002:(0.7768) \n",
      "\t\tProto:4 001:(0.4682) 002:(0.4824) \n",
      "\t\tProto:5 001:(0.6094) 002:(0.3669) \n",
      "\t\tProto:7 001:(0.5762) 002:(0.5224) \n",
      "\t\tProto:8 001:(0.2077) 002:(0.1579) \n",
      "\t\tProto:10 001:(0.5885) 002:(0.5717) \n",
      "\t\tProto:11 001:(0.4383) 002:(0.3043) \n",
      "\t\tProto:13 001:(0.7415) 002:(0.659) \n",
      "\t\tProto:14 001:(0.4285) 002:(0.3636) \n",
      "\t\tProto:15 001:(0.4155) 002:(0.2445) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 81it [00:02, 34.42it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 023+025\n",
      "\t Child: 025+024\n",
      "\t\tProto:0 024:(0.5298) 025:(0.5154) \n",
      "\t\tProto:3 024:(0.139) 025:(0.1895) \n",
      "\t\tProto:4 024:(0.4726) 025:(0.4431) \n",
      "\t\tProto:5 024:(0.6065) 025:(0.4046) \n",
      "\t\tProto:6 024:(0.6194) 025:(0.5335) \n",
      "\t\tProto:9 024:(0.5334) 025:(0.5387) \n",
      "\t\tProto:11 024:(0.4281) 025:(0.4038) \n",
      "Done !!!\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "import pdb\n",
    "\n",
    "from pipnet.pipnet import functional_UnitConv2D\n",
    "\n",
    "# specifically written for for experiment 074\n",
    "def custom_forward(net, xs, inference):\n",
    "    features = net.module._net(xs) \n",
    "    proto_features = {}\n",
    "    # inner product between normalized unit vectors (cosine similarity)\n",
    "    proto_features_inner_product = {}\n",
    "    pooled = {}\n",
    "    pooled_inner_product = {}\n",
    "    out = {}\n",
    "    for node in net.module.root.nodes_with_children():\n",
    "        proto_features[node.name] = getattr(net.module, '_'+node.name+'_add_on')(features)\n",
    "        proto_features[node.name] = net.module._softmax(proto_features[node.name])\n",
    "        \n",
    "        # getting inner product with kernel and input as unit vectors\n",
    "        add_on = getattr(net.module, '_'+node.name+'_add_on')\n",
    "        normalized_weight = F.normalize(add_on.weight.data, p=2, dim=(1, 2, 3)) # Normalize the kernels to unit vectors\n",
    "        normalized_input = F.normalize(features, p=2, dim=1) # Normalize the input to unit vectors\n",
    "        proto_features_inner_product[node.name] = F.conv2d(normalized_input, normalized_weight, bias=None)\n",
    "\n",
    "        pooled[node.name] = net.module._pool(proto_features[node.name])\n",
    "        if inference:\n",
    "            pooled[node.name] = torch.where(pooled[node.name] < 0.1, 0., pooled[node.name])  #during inference, ignore all prototypes that have 0.1 similarity or lower\n",
    "        out[node.name] = getattr(net.module, '_'+node.name+'_classification')(pooled[node.name]) #shape (bs*2, num_classes)\n",
    "        \n",
    "        # finding correspoding value to pooled in inner product\n",
    "#         pdb.set_trace()\n",
    "        output, indices = F.max_pool2d(proto_features[node.name], kernel_size=(26, 26), return_indices=True)# these are logits\n",
    "        tensor_flattened = proto_features_inner_product[node.name].view(proto_features_inner_product[node.name].shape[0],\\\n",
    "                                                                        proto_features_inner_product[node.name].shape[1], -1)\n",
    "        indices_flattened = indices.view(proto_features_inner_product[node.name].shape[0],\\\n",
    "                                         proto_features_inner_product[node.name].shape[1], -1)\n",
    "        corresponding_values_in_proto_features_inner_product = torch.gather(tensor_flattened, 2, indices_flattened)\n",
    "        corresponding_values_in_proto_features_inner_product = corresponding_values_in_proto_features_inner_product.view(proto_features_inner_product[node.name].shape[0],\\\n",
    "                                         proto_features_inner_product[node.name].shape[1], 1, 1)\n",
    "        pooled_inner_product[node.name] = corresponding_values_in_proto_features_inner_product\n",
    "\n",
    "    return features, proto_features, pooled, pooled_inner_product, out\n",
    "\n",
    "def findCorrespondingToMax(base, target):\n",
    "    output, indices = F.max_pool2d(base, kernel_size=(26, 26), return_indices=True)# these are logits\n",
    "    tensor_flattened = target.view(target.shape[0], target.shape[1], -1)\n",
    "    indices_flattened = indices.view(target.shape[0], target.shape[1], -1)\n",
    "    corresponding_values_in_target = torch.gather(tensor_flattened, 2, indices_flattened)\n",
    "    corresponding_values_in_target = corresponding_values_in_target.view(target.shape[0],\\\n",
    "                                     target.shape[1], 1, 1)\n",
    "    pooled_target = corresponding_values_in_target\n",
    "    return pooled_target\n",
    "\n",
    "def customForwardWithCSandSoftmax(net, xs,  inference=False):\n",
    "    features = net.module._net(xs) \n",
    "    proto_features = {}\n",
    "    proto_features_cs = {}\n",
    "    proto_features_softmaxed = {}\n",
    "    pooled = {}\n",
    "    pooled_cs = {}\n",
    "    pooled_softmaxed = {}\n",
    "    out = {}\n",
    "    for node in net.module.root.nodes_with_children():\n",
    "        proto_features[node.name] = getattr(net.module, '_'+node.name+'_add_on')(features)\n",
    "\n",
    "        if net.module.args.softmax == 'y':\n",
    "            proto_features_softmaxed[node.name] = net.module._softmax(proto_features[node.name])\n",
    "            proto_features[node.name] = proto_features[node.name] # will be overwritten if args.multiply_cs_softmax == 'y'\n",
    "        elif net.module.args.gumbel_softmax == 'y':\n",
    "            proto_features_softmaxed[node.name] = net.module._gumbel_softmax(proto_features[node.name])\n",
    "            proto_features[node.name] = proto_features[node.name] # will be overwritten if args.multiply_cs_softmax == 'y'\n",
    "\n",
    "        if net.module.args.multiply_cs_softmax == 'y':\n",
    "            prototypes = getattr(net.module, '_'+node.name+'_add_on')\n",
    "            proto_features_cs[node.name] = functional_UnitConv2D(features, prototypes.weight, prototypes.bias)\n",
    "            proto_features[node.name] = proto_features_cs[node.name] * proto_features_softmaxed[node.name]\n",
    "\n",
    "        pooled[node.name] = net.module._pool(proto_features[node.name])\n",
    "\n",
    "        pooled_cs[node.name] = findCorrespondingToMax(base=proto_features[node.name], \\\n",
    "                                                     target=proto_features_cs[node.name])\n",
    "\n",
    "        pooled_softmaxed[node.name] = findCorrespondingToMax(base=proto_features[node.name], \\\n",
    "                                                     target=proto_features_softmaxed[node.name])\n",
    "\n",
    "        if inference:\n",
    "            pooled[node.name] = torch.where(pooled[node.name] < 0.1, 0., pooled[node.name])  #during inference, ignore all prototypes that have 0.1 similarity or lower\n",
    "        out[node.name] = getattr(net.module, '_'+node.name+'_classification')(pooled[node.name]) #shape (bs*2, num_classes) # these are logits\n",
    "\n",
    "    return features, proto_features, pooled, pooled_cs, pooled_softmaxed, out\n",
    "\n",
    "\n",
    "# Proto activations on leaf descendents - topk images\n",
    "\n",
    "from util.data import ModifiedLabelLoader\n",
    "from collections import defaultdict\n",
    "import heapq\n",
    "import pdb\n",
    "from util.vis_pipnet import get_img_coordinates\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import ImageFont, Image, ImageDraw as D\n",
    "import torchvision\n",
    "\n",
    "topk = 6\n",
    "save_images = True\n",
    "font = ImageFont.truetype(\"arial.ttf\", 50)\n",
    "\n",
    "def get_heap():\n",
    "    list_ = []\n",
    "    heapq.heapify(list_)\n",
    "    return list_\n",
    "\n",
    "patchsize, skip = get_patch_size(args)\n",
    "\n",
    "for node in root.nodes_with_children():\n",
    "#     if node.name == 'root':\n",
    "#         continue\n",
    "    non_leaf_children_names = [child.name for child in node.children if not child.is_leaf()]\n",
    "    if len(non_leaf_children_names) == 0: # if all the children are leaf nodes then skip this node\n",
    "        continue\n",
    "\n",
    "    name2label = testloader.dataset.class_to_idx\n",
    "    label2name = {label:name for name, label in name2label.items()}\n",
    "    modifiedLabelLoader = ModifiedLabelLoader(testloader, node)\n",
    "    coarse_label2name = modifiedLabelLoader.modifiedlabel2name\n",
    "    node_label_to_children = {label: name for name, label in node.children_to_labels.items()}\n",
    "    \n",
    "    imgs = modifiedLabelLoader.filtered_imgs\n",
    "\n",
    "    img_iter = tqdm(enumerate(modifiedLabelLoader),\n",
    "                    total=len(modifiedLabelLoader),\n",
    "                    mininterval=50.,\n",
    "                    desc='Collecting topk',\n",
    "                    ncols=0)\n",
    "\n",
    "    classification_weights = getattr(net.module, '_'+node.name+'_classification').weight\n",
    "    \n",
    "    # maps proto_number -> grand_child_name (or descendant leaf name) -> list of top-k activations\n",
    "    proto_mean_activations = defaultdict(lambda: defaultdict(get_heap))\n",
    "\n",
    "    # maps class names to the prototypes that belong to that\n",
    "    class_and_prototypes = defaultdict(set)\n",
    "\n",
    "    for i, (xs, orig_y, ys) in img_iter:\n",
    "        if coarse_label2name[ys.item()] not in non_leaf_children_names:\n",
    "            continue\n",
    "\n",
    "        xs, ys = xs.to(device), ys.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model_output = customForwardWithCSandSoftmax(net, xs, inference=False)\n",
    "            _, softmaxes, pooled, pooled_cs, pooled_softmaxed, _ = model_output\n",
    "#             if len(model_output) == 4:\n",
    "#                 softmaxes, pooled, pooled_ip, _ = model_output\n",
    "#             elif len(model_output) == 5:\n",
    "#                 _, softmaxes, pooled, pooled_ip, _ = model_output\n",
    "            pooled = pooled[node.name].squeeze(0) \n",
    "#             pdb.set_trace()\n",
    "            pooled_cs = pooled_cs[node.name].squeeze()\n",
    "            pooled_softmaxed = pooled_softmaxed[node.name].squeeze()\n",
    "            softmaxes = softmaxes[node.name]#.squeeze(0)\n",
    "\n",
    "            for p in range(pooled.shape[0]): # pooled.shape -> [768] (== num of prototypes)\n",
    "                c_weight = torch.max(classification_weights[:,p]) # classification_weights[:,p].shape -> [200] (== num of classes)\n",
    "                relevant_proto_classes = torch.nonzero(classification_weights[:, p] > 1e-3)\n",
    "                relevant_proto_class_names = [node_label_to_children[class_idx.item()] for class_idx in relevant_proto_classes]\n",
    "                \n",
    "                # Take the max per prototype.                             \n",
    "                max_per_prototype, max_idx_per_prototype = torch.max(softmaxes, dim=0)\n",
    "                max_per_prototype_h, max_idx_per_prototype_h = torch.max(max_per_prototype, dim=1)\n",
    "                max_per_prototype_w, max_idx_per_prototype_w = torch.max(max_per_prototype_h, dim=1) #shape (num_prototypes)\n",
    "                \n",
    "                h_idx = max_idx_per_prototype_h[p, max_idx_per_prototype_w[p]]\n",
    "                w_idx = max_idx_per_prototype_w[p]\n",
    "\n",
    "                if len(relevant_proto_class_names) == 0:\n",
    "                    continue\n",
    "                \n",
    "                if (len(relevant_proto_class_names) == 1) and (relevant_proto_class_names[0] not in non_leaf_children_names):\n",
    "                    continue\n",
    "                \n",
    "                h_coor_min, h_coor_max, w_coor_min, w_coor_max = get_img_coordinates(args.image_size, softmaxes.shape, patchsize, skip, h_idx, w_idx)\n",
    "                latent_activation = softmaxes[:, p, :, :]\n",
    "                if (coarse_label2name[ys.item()] in relevant_proto_class_names):\n",
    "                    child_node = root.get_node(coarse_label2name[ys.item()])\n",
    "                    leaf_descendent = label2name[orig_y.item()][4:7]\n",
    "                    img_to_open = imgs[i][0] # it is a tuple of (path to image, lable)\n",
    "                    if topk and (len(proto_mean_activations[p][leaf_descendent]) >= topk):\n",
    "                        heapq.heappushpop(proto_mean_activations[p][leaf_descendent],\\\n",
    "                                          (pooled[p].item(), pooled_cs[p].item(), pooled_softmaxed[p].item(), img_to_open,\\\n",
    "                                           (h_coor_min, h_coor_max, w_coor_min, w_coor_max), latent_activation))\n",
    "                    else:\n",
    "                        heapq.heappush(proto_mean_activations[p][leaf_descendent],\\\n",
    "                                       (pooled[p].item(), pooled_cs[p].item(), pooled_softmaxed[p].item(), img_to_open,\\\n",
    "                                        (h_coor_min, h_coor_max, w_coor_min, w_coor_max), latent_activation))\n",
    "\n",
    "                class_and_prototypes[', '.join(relevant_proto_class_names)].add(p)\n",
    "\n",
    "    \n",
    "    print('Node', node.name)\n",
    "    for child_classname in class_and_prototypes:\n",
    "        \n",
    "        print('\\t'*1, 'Child:', child_classname)\n",
    "        for p in class_and_prototypes[child_classname]:\n",
    "            \n",
    "            logstr = '\\t'*2 + f'Proto:{p} '\n",
    "            for leaf_descendent in proto_mean_activations[p]:\n",
    "                mean_activation = round(np.mean([activation for activation, *_ in proto_mean_activations[p][leaf_descendent]]), 4)\n",
    "                num_images = len(proto_mean_activations[p][leaf_descendent])\n",
    "                logstr += f'{leaf_descendent}:({mean_activation}) '\n",
    "            print(logstr)\n",
    "            \n",
    "            if save_images:\n",
    "                patches = []\n",
    "                right_descriptions = []\n",
    "                text_region_width = 3 # 3x the width of a patch\n",
    "                for leaf_descendent, heap in proto_mean_activations[p].items():\n",
    "                    heap = sorted(heap)[::-1]\n",
    "                    mean_activation = round(np.mean([activation for activation, *_ in proto_mean_activations[p][leaf_descendent]]), 2)\n",
    "                    mean_cosine_similarity = round(np.mean([activation_cs for _, activation_cs, *_ in proto_mean_activations[p][leaf_descendent]]), 2)\n",
    "                    mean_softmax = round(np.mean([activation_softmax for _, _, activation_softmax, *_ in proto_mean_activations[p][leaf_descendent]]), 2)\n",
    "                    \n",
    "                    for ele in heap:\n",
    "                        activation, activation_cs, activation_softmax, img_to_open, (h_coor_min, h_coor_max, w_coor_min, w_coor_max), latent_activation = ele\n",
    "                        image = transforms.Resize(size=(args.image_size, args.image_size))(Image.open(img_to_open))\n",
    "                        img_tensor = transforms.ToTensor()(image)#.unsqueeze_(0) #shape (1, 3, h, w)\n",
    "                        img_tensor_patch = img_tensor[:, h_coor_min:h_coor_max, w_coor_min:w_coor_max]\n",
    "                        overlayed_image_np = get_heatmap_uninterpolated(latent_activation, img_tensor)\n",
    "                        overlayed_image = torch.tensor(overlayed_image_np).permute(2, 0, 1).float() / 255.\n",
    "                        patches.append(overlayed_image)\n",
    "\n",
    "                    # description on the right hand side\n",
    "                    text = f'{mean_softmax}, {mean_cosine_similarity}, {mean_activation}, {leaf_descendent}'\n",
    "                    txtimage = Image.new(\"RGB\", (patches[0].shape[-2]*text_region_width,patches[0].shape[-1]), (0, 0, 0))\n",
    "                    draw = D.Draw(txtimage)\n",
    "                    draw.text((300, patches[0].shape[1]//2), text, anchor='mm', fill=\"white\", font=font)\n",
    "                    txttensor = transforms.ToTensor()(txtimage)#.unsqueeze_(0)\n",
    "                    right_descriptions.append(txttensor)\n",
    "\n",
    "                grid = torchvision.utils.make_grid(patches, nrow=topk, padding=1)\n",
    "                grid_right_descriptions = torchvision.utils.make_grid(right_descriptions, nrow=1, padding=1)\n",
    "\n",
    "                # merging right description with the grid of images\n",
    "                grid = torch.cat([grid, grid_right_descriptions], dim=-1)\n",
    "\n",
    "                # description on the top\n",
    "                text = f'Node:{node.name}, p{p}, Child:{child_classname}'\n",
    "                txtimage = Image.new(\"RGB\", (grid.shape[-1], 224), (0, 0, 0))\n",
    "                draw = D.Draw(txtimage)\n",
    "                draw.text((200, patches[0].shape[1]//2), text, anchor='mm', fill=\"white\", font=font)\n",
    "                txttensor = transforms.ToTensor()(txtimage)#.unsqueeze_(0)\n",
    "\n",
    "                # merging top description with the grid of images\n",
    "                grid = torch.cat([grid, txttensor], dim=1)\n",
    "\n",
    "                os.makedirs(os.path.join(run_path, f'descendent_specific_topk_heatmap_uninterp_cosine_and_softmax_testloader_ep={epoch}', node.name), exist_ok=True)\n",
    "                torchvision.utils.save_image(grid, os.path.join(run_path, f'descendent_specific_topk_heatmap_uninterp_cosine_and_softmax_testloader_ep={epoch}', node.name, f'{child_classname}-p{p}.png'))\n",
    "\n",
    "print('Done !!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1466b4f",
   "metadata": {},
   "source": [
    "# Proto activations on NON leaf descendents - topk images using either NAIVE-HPIPNET or UNIT-SPACE-PROTOPOOL with HEATMAP w/o INTERPOLATION with SOFTMAX SCORE and COSINE SIMILARITY (for experiment 074)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b986e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 511it [00:26, 19.44it/s]\n",
      "/tmp/ipykernel_22056/3535226083.py:35: DeprecationWarning: NEAREST is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.NEAREST or Dither.NONE instead.\n",
      "  resample=Image.NEAREST ))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node root\n",
      "\t Child: 004+086\n",
      "\t\tProto:0 050:(0.0014) 051:(0.0014) 052:(0.0012) 053:(0.0017) \n",
      "\t\tProto:1 050:(0.0007) 051:(0.0006) 052:(0.0006) 053:(0.0007) \n",
      "\t\tProto:2 050:(0.0557) 051:(0.0009) 052:(0.0016) 053:(0.0191) \n",
      "\t\tProto:3 050:(0.0013) 051:(0.0012) 052:(0.0008) 053:(0.0011) \n",
      "\t\tProto:4 050:(0.0007) 051:(0.0008) 052:(0.0009) 053:(0.0008) \n",
      "\t\tProto:128 050:(0.0014) 051:(0.0033) 052:(0.0008) 053:(0.0187) \n",
      "\t\tProto:6 050:(0.0299) 051:(0.011) 052:(0.0067) 053:(0.0027) \n",
      "\t\tProto:129 050:(0.0013) 051:(0.0009) 052:(0.001) 053:(0.001) \n",
      "\t\tProto:130 050:(0.0426) 051:(0.0252) 052:(0.0112) 053:(0.0244) \n",
      "\t\tProto:131 050:(0.0356) 051:(0.1034) 052:(0.0272) 053:(0.079) \n",
      "\t\tProto:10 050:(0.1116) 051:(0.0836) 052:(0.0688) 053:(0.0347) \n",
      "\t\tProto:132 050:(0.0011) 051:(0.0013) 052:(0.0006) 053:(0.001) \n",
      "\t\tProto:12 050:(0.0011) 051:(0.0011) 052:(0.001) 053:(0.0011) \n",
      "\t\tProto:13 050:(0.0031) 051:(0.0033) 052:(0.0025) 053:(0.0044) \n",
      "\t\tProto:14 050:(0.0105) 051:(0.0035) 052:(0.0016) 053:(0.003) \n",
      "\t\tProto:134 050:(0.0009) 051:(0.0008) 052:(0.0006) 053:(0.0006) \n",
      "\t\tProto:16 050:(0.0387) 051:(0.0389) 052:(0.0311) 053:(0.0671) \n",
      "\t\tProto:20 050:(0.0045) 051:(0.007) 052:(0.0268) 053:(0.0682) \n",
      "\t\tProto:21 050:(0.0011) 051:(0.0012) 052:(0.001) 053:(0.0013) \n",
      "\t\tProto:22 050:(0.016) 051:(0.0086) 052:(0.0005) 053:(0.0025) \n",
      "\t\tProto:23 050:(0.0009) 051:(0.0006) 052:(0.0005) 053:(0.0009) \n",
      "\t\tProto:24 050:(0.0031) 051:(0.0153) 052:(0.0064) 053:(0.012) \n",
      "\t\tProto:26 050:(0.0131) 051:(0.0044) 052:(0.0402) 053:(0.0474) \n",
      "\t\tProto:27 050:(0.0608) 051:(0.0104) 052:(0.0027) 053:(0.0063) \n",
      "\t\tProto:28 050:(0.0008) 051:(0.0009) 052:(0.0007) 053:(0.0008) \n",
      "\t\tProto:31 050:(0.0007) 051:(0.0007) 052:(0.0004) 053:(0.0008) \n",
      "\t\tProto:32 050:(0.0013) 051:(0.0015) 052:(0.0009) 053:(0.0016) \n",
      "\t\tProto:34 050:(0.0229) 051:(0.0173) 052:(0.0207) 053:(0.0043) \n",
      "\t\tProto:35 050:(0.0187) 051:(0.0024) 052:(0.0016) 053:(0.0286) \n",
      "\t\tProto:36 050:(0.0622) 051:(0.0372) 052:(0.0029) 053:(0.0252) \n",
      "\t\tProto:37 050:(0.0022) 051:(0.016) 052:(0.0009) 053:(0.003) \n",
      "\t\tProto:41 050:(0.0008) 051:(0.0005) 052:(0.0005) 053:(0.0008) \n",
      "\t\tProto:42 050:(0.0011) 051:(0.001) 052:(0.0007) 053:(0.0011) \n",
      "\t\tProto:45 050:(0.0029) 051:(0.0014) 052:(0.001) 053:(0.003) \n",
      "\t\tProto:48 050:(0.0009) 051:(0.0009) 052:(0.0008) 053:(0.0009) \n",
      "\t\tProto:49 050:(0.0244) 051:(0.0033) 052:(0.0178) 053:(0.0291) \n",
      "\t\tProto:52 050:(0.0008) 051:(0.0007) 052:(0.0005) 053:(0.0008) \n",
      "\t\tProto:54 050:(0.0123) 051:(0.0638) 052:(0.0556) 053:(0.0257) \n",
      "\t\tProto:56 050:(0.0799) 051:(0.0457) 052:(0.0259) 053:(0.021) \n",
      "\t\tProto:57 050:(0.0008) 051:(0.0011) 052:(0.0006) 053:(0.0008) \n",
      "\t\tProto:58 050:(0.0321) 051:(0.0289) 052:(0.0093) 053:(0.0492) \n",
      "\t\tProto:59 050:(0.0168) 051:(0.0225) 052:(0.0017) 053:(0.0105) \n",
      "\t\tProto:62 050:(0.0008) 051:(0.0007) 052:(0.0006) 053:(0.0007) \n",
      "\t\tProto:63 050:(0.0013) 051:(0.0007) 052:(0.0006) 053:(0.0008) \n",
      "\t\tProto:64 050:(0.0084) 051:(0.0014) 052:(0.0077) 053:(0.0014) \n",
      "\t\tProto:65 050:(0.0008) 051:(0.0009) 052:(0.0007) 053:(0.0009) \n",
      "\t\tProto:66 050:(0.0071) 051:(0.002) 052:(0.002) 053:(0.0029) \n",
      "\t\tProto:69 050:(0.0762) 051:(0.0976) 052:(0.0231) 053:(0.1087) \n",
      "\t\tProto:70 050:(0.0005) 051:(0.0005) 052:(0.0005) 053:(0.0007) \n",
      "\t\tProto:71 050:(0.0008) 051:(0.0007) 052:(0.0007) 053:(0.0008) \n",
      "\t\tProto:74 050:(0.0365) 051:(0.0239) 052:(0.0298) 053:(0.0023) \n",
      "\t\tProto:76 050:(0.0007) 051:(0.0008) 052:(0.0006) 053:(0.0007) \n",
      "\t\tProto:77 050:(0.0623) 051:(0.0871) 052:(0.0945) 053:(0.1048) \n",
      "\t\tProto:79 050:(0.001) 051:(0.0009) 052:(0.0009) 053:(0.001) \n",
      "\t\tProto:81 050:(0.0551) 051:(0.0508) 052:(0.071) 053:(0.0099) \n",
      "\t\tProto:83 050:(0.0028) 051:(0.0022) 052:(0.001) 053:(0.0056) \n",
      "\t\tProto:86 050:(0.0507) 051:(0.0121) 052:(0.0052) 053:(0.0201) \n",
      "\t\tProto:87 050:(0.0008) 051:(0.0008) 052:(0.0008) 053:(0.0008) \n",
      "\t\tProto:92 050:(0.0263) 051:(0.0905) 052:(0.1141) 053:(0.0045) \n",
      "\t\tProto:93 050:(0.0981) 051:(0.0731) 052:(0.0053) 053:(0.0421) \n",
      "\t\tProto:95 050:(0.0011) 051:(0.001) 052:(0.0006) 053:(0.0011) \n",
      "\t\tProto:96 050:(0.0009) 051:(0.0009) 052:(0.0007) 053:(0.0012) \n",
      "\t\tProto:103 050:(0.0434) 051:(0.0207) 052:(0.057) 053:(0.0262) \n",
      "\t\tProto:104 050:(0.0008) 051:(0.001) 052:(0.0006) 053:(0.0009) \n",
      "\t\tProto:105 050:(0.0006) 051:(0.0006) 052:(0.0004) 053:(0.0006) \n",
      "\t\tProto:107 050:(0.0006) 051:(0.0005) 052:(0.0004) 053:(0.0006) \n",
      "\t\tProto:108 050:(0.001) 051:(0.0011) 052:(0.0009) 053:(0.0009) \n",
      "\t\tProto:111 050:(0.001) 051:(0.001) 052:(0.001) 053:(0.0012) \n",
      "\t\tProto:112 050:(0.0008) 051:(0.0006) 052:(0.0004) 053:(0.0006) \n",
      "\t\tProto:114 050:(0.0007) 051:(0.0007) 052:(0.0006) 053:(0.0008) \n",
      "\t\tProto:121 050:(0.0007) 051:(0.0007) 052:(0.0006) 053:(0.0009) \n",
      "\t\tProto:123 050:(0.0085) 051:(0.0091) 052:(0.0175) 053:(0.0434) \n",
      "\t\tProto:124 050:(0.001) 051:(0.001) 052:(0.0007) 053:(0.001) \n",
      "\t\tProto:125 050:(0.0809) 051:(0.0941) 052:(0.0143) 053:(0.0039) \n",
      "\t\tProto:127 050:(0.0009) 051:(0.0009) 052:(0.0009) 053:(0.001) \n",
      "\t Child: 052+053\n",
      "\t\tProto:133 001:(0.0353) 002:(0.015) 003:(0.0071) 004:(0.0075) 023:(0.0977) 024:(0.0653) 025:(0.0391) 031:(0.0579) 032:(0.0401) 033:(0.0715) 045:(0.0113) 086:(0.0702) 100:(0.0506) 101:(0.0124) \n",
      "\t\tProto:135 001:(0.3041) 002:(0.0428) 003:(0.0974) 004:(0.0608) 023:(0.3279) 024:(0.177) 025:(0.2692) 031:(0.0295) 032:(0.074) 033:(0.0087) 045:(0.1131) 086:(0.2987) 100:(0.1741) 101:(0.0168) \n",
      "\t\tProto:113 001:(0.0083) 002:(0.0053) 003:(0.0048) 004:(0.0069) 023:(0.1433) 024:(0.0193) 025:(0.0323) 031:(0.0059) 032:(0.0667) 033:(0.0685) 045:(0.0401) 086:(0.1768) 100:(0.0217) 101:(0.0248) \n",
      "\t\tProto:51 001:(0.0575) 002:(0.0257) 003:(0.0458) 004:(0.0108) 023:(0.1606) 024:(0.0407) 025:(0.0376) 031:(0.0249) 032:(0.0331) 033:(0.1076) 045:(0.0737) 086:(0.1601) 100:(0.0178) 101:(0.029) \n",
      "\t\tProto:84 001:(0.0145) 002:(0.0048) 003:(0.0045) 004:(0.006) 023:(0.1166) 024:(0.0312) 025:(0.0212) 031:(0.0082) 032:(0.0171) 033:(0.0701) 045:(0.0128) 086:(0.2062) 100:(0.005) 101:(0.0616) \n",
      "\t\tProto:116 001:(0.0049) 002:(0.0101) 003:(0.0161) 004:(0.0039) 023:(0.0599) 024:(0.1106) 025:(0.2176) 031:(0.0566) 032:(0.0275) 033:(0.0068) 045:(0.0041) 086:(0.0041) 100:(0.0316) 101:(0.0024) \n",
      "\t\tProto:120 001:(0.0647) 002:(0.0264) 003:(0.0122) 004:(0.01) 023:(0.2059) 024:(0.02) 025:(0.0956) 031:(0.004) 032:(0.0987) 033:(0.0078) 045:(0.0852) 086:(0.1943) 100:(0.2612) 101:(0.073) \n",
      "\t\tProto:25 001:(0.0567) 002:(0.0606) 003:(0.0061) 004:(0.0143) 023:(0.1568) 024:(0.0269) 025:(0.0061) 031:(0.0716) 032:(0.0909) 033:(0.0779) 045:(0.065) 086:(0.089) 100:(0.1936) 101:(0.1214) \n",
      "\t\tProto:122 001:(0.0457) 002:(0.0383) 003:(0.0667) 004:(0.0052) 023:(0.0961) 024:(0.0799) 025:(0.1255) 031:(0.0889) 032:(0.0631) 033:(0.1387) 045:(0.0927) 086:(0.0444) 100:(0.034) 101:(0.0056) \n",
      "\t\tProto:61 001:(0.0701) 002:(0.037) 003:(0.0391) 004:(0.0205) 023:(0.0745) 024:(0.0402) 025:(0.1079) 031:(0.0063) 032:(0.0537) 033:(0.0082) 045:(0.0132) 086:(0.0691) 100:(0.0277) 101:(0.0037) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting topk: 120it [00:03, 35.00it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 052+053\n",
      "\t Child: 053+050\n",
      "\t\tProto:2 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "stack expects a non-empty TensorList",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 240\u001b[0m\n\u001b[1;32m    237\u001b[0m     txttensor \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mToTensor()(txtimage)\u001b[38;5;66;03m#.unsqueeze_(0)\u001b[39;00m\n\u001b[1;32m    238\u001b[0m     right_descriptions\u001b[38;5;241m.\u001b[39mappend(txttensor)\n\u001b[0;32m--> 240\u001b[0m grid \u001b[38;5;241m=\u001b[39m \u001b[43mtorchvision\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_grid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpatches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnrow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtopk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    241\u001b[0m grid_right_descriptions \u001b[38;5;241m=\u001b[39m torchvision\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mmake_grid(right_descriptions, nrow\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    243\u001b[0m \u001b[38;5;66;03m# merging right description with the grid of images\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/hpnet1/lib/python3.8/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/hpnet1/lib/python3.8/site-packages/torchvision/utils.py:67\u001b[0m, in \u001b[0;36mmake_grid\u001b[0;34m(tensor, nrow, padding, normalize, value_range, scale_each, pad_value, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# if list of tensors, convert to a 4D mini-batch Tensor\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensor, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m---> 67\u001b[0m     tensor \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tensor\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:  \u001b[38;5;66;03m# single image H x W\u001b[39;00m\n\u001b[1;32m     70\u001b[0m     tensor \u001b[38;5;241m=\u001b[39m tensor\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects a non-empty TensorList"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "import pdb\n",
    "\n",
    "from pipnet.pipnet import functional_UnitConv2D\n",
    "\n",
    "# specifically written for for experiment 074\n",
    "def custom_forward(net, xs, inference):\n",
    "    features = net.module._net(xs) \n",
    "    proto_features = {}\n",
    "    # inner product between normalized unit vectors (cosine similarity)\n",
    "    proto_features_inner_product = {}\n",
    "    pooled = {}\n",
    "    pooled_inner_product = {}\n",
    "    out = {}\n",
    "    for node in net.module.root.nodes_with_children():\n",
    "        proto_features[node.name] = getattr(net.module, '_'+node.name+'_add_on')(features)\n",
    "        proto_features[node.name] = net.module._softmax(proto_features[node.name])\n",
    "        \n",
    "        # getting inner product with kernel and input as unit vectors\n",
    "        add_on = getattr(net.module, '_'+node.name+'_add_on')\n",
    "        normalized_weight = F.normalize(add_on.weight.data, p=2, dim=(1, 2, 3)) # Normalize the kernels to unit vectors\n",
    "        normalized_input = F.normalize(features, p=2, dim=1) # Normalize the input to unit vectors\n",
    "        proto_features_inner_product[node.name] = F.conv2d(normalized_input, normalized_weight, bias=None)\n",
    "\n",
    "        pooled[node.name] = net.module._pool(proto_features[node.name])\n",
    "        if inference:\n",
    "            pooled[node.name] = torch.where(pooled[node.name] < 0.1, 0., pooled[node.name])  #during inference, ignore all prototypes that have 0.1 similarity or lower\n",
    "        out[node.name] = getattr(net.module, '_'+node.name+'_classification')(pooled[node.name]) #shape (bs*2, num_classes)\n",
    "        \n",
    "        # finding correspoding value to pooled in inner product\n",
    "#         pdb.set_trace()\n",
    "        output, indices = F.max_pool2d(proto_features[node.name], kernel_size=(26, 26), return_indices=True)# these are logits\n",
    "        tensor_flattened = proto_features_inner_product[node.name].view(proto_features_inner_product[node.name].shape[0],\\\n",
    "                                                                        proto_features_inner_product[node.name].shape[1], -1)\n",
    "        indices_flattened = indices.view(proto_features_inner_product[node.name].shape[0],\\\n",
    "                                         proto_features_inner_product[node.name].shape[1], -1)\n",
    "        corresponding_values_in_proto_features_inner_product = torch.gather(tensor_flattened, 2, indices_flattened)\n",
    "        corresponding_values_in_proto_features_inner_product = corresponding_values_in_proto_features_inner_product.view(proto_features_inner_product[node.name].shape[0],\\\n",
    "                                         proto_features_inner_product[node.name].shape[1], 1, 1)\n",
    "        pooled_inner_product[node.name] = corresponding_values_in_proto_features_inner_product\n",
    "\n",
    "    return features, proto_features, pooled, pooled_inner_product, out\n",
    "\n",
    "def findCorrespondingToMax(base, target):\n",
    "    output, indices = F.max_pool2d(base, kernel_size=(26, 26), return_indices=True)# these are logits\n",
    "    tensor_flattened = target.view(target.shape[0], target.shape[1], -1)\n",
    "    indices_flattened = indices.view(target.shape[0], target.shape[1], -1)\n",
    "    corresponding_values_in_target = torch.gather(tensor_flattened, 2, indices_flattened)\n",
    "    corresponding_values_in_target = corresponding_values_in_target.view(target.shape[0],\\\n",
    "                                     target.shape[1], 1, 1)\n",
    "    pooled_target = corresponding_values_in_target\n",
    "    return pooled_target\n",
    "\n",
    "def customForwardWithCSandSoftmax(net, xs,  inference=False):\n",
    "    features = net.module._net(xs) \n",
    "    proto_features = {}\n",
    "    proto_features_cs = {}\n",
    "    proto_features_softmaxed = {}\n",
    "    pooled = {}\n",
    "    pooled_cs = {}\n",
    "    pooled_softmaxed = {}\n",
    "    out = {}\n",
    "    for node in net.module.root.nodes_with_children():\n",
    "        proto_features[node.name] = getattr(net.module, '_'+node.name+'_add_on')(features)\n",
    "\n",
    "        if net.module.args.softmax == 'y':\n",
    "            proto_features_softmaxed[node.name] = net.module._softmax(proto_features[node.name])\n",
    "            proto_features[node.name] = proto_features[node.name] # will be overwritten if args.multiply_cs_softmax == 'y'\n",
    "        elif net.module.args.gumbel_softmax == 'y':\n",
    "            proto_features_softmaxed[node.name] = net.module._gumbel_softmax(proto_features[node.name])\n",
    "            proto_features[node.name] = proto_features[node.name] # will be overwritten if args.multiply_cs_softmax == 'y'\n",
    "\n",
    "        if net.module.args.multiply_cs_softmax == 'y':\n",
    "            prototypes = getattr(net.module, '_'+node.name+'_add_on')\n",
    "            proto_features_cs[node.name] = functional_UnitConv2D(features, prototypes.weight, prototypes.bias)\n",
    "            proto_features[node.name] = proto_features_cs[node.name] * proto_features_softmaxed[node.name]\n",
    "\n",
    "        pooled[node.name] = net.module._pool(proto_features[node.name])\n",
    "\n",
    "        pooled_cs[node.name] = findCorrespondingToMax(base=proto_features[node.name], \\\n",
    "                                                     target=proto_features_cs[node.name])\n",
    "\n",
    "        pooled_softmaxed[node.name] = findCorrespondingToMax(base=proto_features[node.name], \\\n",
    "                                                     target=proto_features_softmaxed[node.name])\n",
    "\n",
    "        if inference:\n",
    "            pooled[node.name] = torch.where(pooled[node.name] < 0.1, 0., pooled[node.name])  #during inference, ignore all prototypes that have 0.1 similarity or lower\n",
    "        out[node.name] = getattr(net.module, '_'+node.name+'_classification')(pooled[node.name]) #shape (bs*2, num_classes) # these are logits\n",
    "\n",
    "    return features, proto_features, pooled, pooled_cs, pooled_softmaxed, out\n",
    "\n",
    "\n",
    "# Proto activations on leaf descendents - topk images\n",
    "\n",
    "from util.data import ModifiedLabelLoader\n",
    "from collections import defaultdict\n",
    "import heapq\n",
    "import pdb\n",
    "from util.vis_pipnet import get_img_coordinates\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import ImageFont, Image, ImageDraw as D\n",
    "import torchvision\n",
    "\n",
    "topk = 6\n",
    "save_images = True\n",
    "font = ImageFont.truetype(\"arial.ttf\", 50)\n",
    "\n",
    "def get_heap():\n",
    "    list_ = []\n",
    "    heapq.heapify(list_)\n",
    "    return list_\n",
    "\n",
    "patchsize, skip = get_patch_size(args)\n",
    "\n",
    "for node in root.nodes_with_children():\n",
    "#     if node.name == 'root':\n",
    "#         continue\n",
    "    non_leaf_children_names = [child.name for child in node.children if not child.is_leaf()]\n",
    "    if len(non_leaf_children_names) == 0: # if all the children are leaf nodes then skip this node\n",
    "        continue\n",
    "\n",
    "    name2label = testloader.dataset.class_to_idx\n",
    "    label2name = {label:name for name, label in name2label.items()}\n",
    "    modifiedLabelLoader = ModifiedLabelLoader(testloader, node)\n",
    "    coarse_label2name = modifiedLabelLoader.modifiedlabel2name\n",
    "    node_label_to_children = {label: name for name, label in node.children_to_labels.items()}\n",
    "    \n",
    "    imgs = modifiedLabelLoader.filtered_imgs\n",
    "\n",
    "    img_iter = tqdm(enumerate(modifiedLabelLoader),\n",
    "                    total=len(modifiedLabelLoader),\n",
    "                    mininterval=50.,\n",
    "                    desc='Collecting topk',\n",
    "                    ncols=0)\n",
    "\n",
    "    classification_weights = getattr(net.module, '_'+node.name+'_classification').weight\n",
    "    \n",
    "    # maps proto_number -> grand_child_name (or descendant leaf name) -> list of top-k activations\n",
    "    proto_mean_activations = defaultdict(lambda: defaultdict(get_heap))\n",
    "\n",
    "    # maps class names to the prototypes that belong to that\n",
    "    class_and_prototypes = defaultdict(set)\n",
    "\n",
    "    for i, (xs, orig_y, ys) in img_iter:\n",
    "        if coarse_label2name[ys.item()] not in non_leaf_children_names:\n",
    "            continue\n",
    "\n",
    "        xs, ys = xs.to(device), ys.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model_output = customForwardWithCSandSoftmax(net, xs, inference=False)\n",
    "            _, softmaxes, pooled, pooled_cs, pooled_softmaxed, _ = model_output\n",
    "#             if len(model_output) == 4:\n",
    "#                 softmaxes, pooled, pooled_ip, _ = model_output\n",
    "#             elif len(model_output) == 5:\n",
    "#                 _, softmaxes, pooled, pooled_ip, _ = model_output\n",
    "            pooled = pooled[node.name].squeeze(0) \n",
    "#             pdb.set_trace()\n",
    "            pooled_cs = pooled_cs[node.name].squeeze()\n",
    "            pooled_softmaxed = pooled_softmaxed[node.name].squeeze()\n",
    "            softmaxes = softmaxes[node.name]#.squeeze(0)\n",
    "\n",
    "            for p in range(pooled.shape[0]): # pooled.shape -> [768] (== num of prototypes)\n",
    "                c_weight = torch.max(classification_weights[:,p]) # classification_weights[:,p].shape -> [200] (== num of classes)\n",
    "                relevant_proto_classes = torch.nonzero(classification_weights[:, p] > 1e-3)\n",
    "                relevant_proto_class_names = [node_label_to_children[class_idx.item()] for class_idx in relevant_proto_classes]\n",
    "                \n",
    "                # Take the max per prototype.                             \n",
    "                max_per_prototype, max_idx_per_prototype = torch.max(softmaxes, dim=0)\n",
    "                max_per_prototype_h, max_idx_per_prototype_h = torch.max(max_per_prototype, dim=1)\n",
    "                max_per_prototype_w, max_idx_per_prototype_w = torch.max(max_per_prototype_h, dim=1) #shape (num_prototypes)\n",
    "                \n",
    "                h_idx = max_idx_per_prototype_h[p, max_idx_per_prototype_w[p]]\n",
    "                w_idx = max_idx_per_prototype_w[p]\n",
    "\n",
    "                if len(relevant_proto_class_names) == 0:\n",
    "                    continue\n",
    "                \n",
    "                if (len(relevant_proto_class_names) == 1) and (relevant_proto_class_names[0] not in non_leaf_children_names):\n",
    "                    continue\n",
    "                \n",
    "                h_coor_min, h_coor_max, w_coor_min, w_coor_max = get_img_coordinates(args.image_size, softmaxes.shape, patchsize, skip, h_idx, w_idx)\n",
    "                latent_activation = softmaxes[:, p, :, :]\n",
    "                if (coarse_label2name[ys.item()] not in relevant_proto_class_names):\n",
    "                    child_node = root.get_node(coarse_label2name[ys.item()])\n",
    "                    leaf_descendent = label2name[orig_y.item()][4:7]\n",
    "                    img_to_open = imgs[i][0] # it is a tuple of (path to image, lable)\n",
    "                    if topk and (len(proto_mean_activations[p][leaf_descendent]) >= topk):\n",
    "                        heapq.heappushpop(proto_mean_activations[p][leaf_descendent],\\\n",
    "                                          (pooled[p].item(), pooled_cs[p].item(), pooled_softmaxed[p].item(), img_to_open,\\\n",
    "                                           (h_coor_min, h_coor_max, w_coor_min, w_coor_max), latent_activation))\n",
    "                    else:\n",
    "                        heapq.heappush(proto_mean_activations[p][leaf_descendent],\\\n",
    "                                       (pooled[p].item(), pooled_cs[p].item(), pooled_softmaxed[p].item(), img_to_open,\\\n",
    "                                        (h_coor_min, h_coor_max, w_coor_min, w_coor_max), latent_activation))\n",
    "\n",
    "                class_and_prototypes[', '.join(relevant_proto_class_names)].add(p)\n",
    "\n",
    "    \n",
    "    print('Node', node.name)\n",
    "    for child_classname in class_and_prototypes:\n",
    "        \n",
    "        print('\\t'*1, 'Child:', child_classname)\n",
    "        for p in class_and_prototypes[child_classname]:\n",
    "            \n",
    "            logstr = '\\t'*2 + f'Proto:{p} '\n",
    "            for leaf_descendent in proto_mean_activations[p]:\n",
    "                mean_activation = round(np.mean([activation for activation, *_ in proto_mean_activations[p][leaf_descendent]]), 4)\n",
    "                num_images = len(proto_mean_activations[p][leaf_descendent])\n",
    "                logstr += f'{leaf_descendent}:({mean_activation}) '\n",
    "            print(logstr)\n",
    "            \n",
    "            if len(proto_mean_activations_non_descendants[p]) == 0:\n",
    "                continue\n",
    "            \n",
    "            if save_images:\n",
    "                patches = []\n",
    "                right_descriptions = []\n",
    "                text_region_width = 3 # 3x the width of a patch\n",
    "                for leaf_descendent, heap in proto_mean_activations[p].items():\n",
    "                    heap = sorted(heap)[::-1]\n",
    "                    mean_activation = round(np.mean([activation for activation, *_ in proto_mean_activations[p][leaf_descendent]]), 2)\n",
    "                    mean_cosine_similarity = round(np.mean([activation_cs for _, activation_cs, *_ in proto_mean_activations[p][leaf_descendent]]), 2)\n",
    "                    mean_softmax = round(np.mean([activation_softmax for _, _, activation_softmax, *_ in proto_mean_activations[p][leaf_descendent]]), 2)\n",
    "                    \n",
    "                    for ele in heap:\n",
    "                        activation, activation_cs, activation_softmax, img_to_open, (h_coor_min, h_coor_max, w_coor_min, w_coor_max), latent_activation = ele\n",
    "                        image = transforms.Resize(size=(args.image_size, args.image_size))(Image.open(img_to_open))\n",
    "                        img_tensor = transforms.ToTensor()(image)#.unsqueeze_(0) #shape (1, 3, h, w)\n",
    "                        img_tensor_patch = img_tensor[:, h_coor_min:h_coor_max, w_coor_min:w_coor_max]\n",
    "                        overlayed_image_np = get_heatmap_uninterpolated(latent_activation, img_tensor)\n",
    "                        overlayed_image = torch.tensor(overlayed_image_np).permute(2, 0, 1).float() / 255.\n",
    "                        patches.append(overlayed_image)\n",
    "\n",
    "                    # description on the right hand side\n",
    "                    text = f'{mean_softmax}, {mean_cosine_similarity}, {mean_activation}, {leaf_descendent}'\n",
    "                    txtimage = Image.new(\"RGB\", (patches[0].shape[-2]*text_region_width,patches[0].shape[-1]), (0, 0, 0))\n",
    "                    draw = D.Draw(txtimage)\n",
    "                    draw.text((300, patches[0].shape[1]//2), text, anchor='mm', fill=\"white\", font=font)\n",
    "                    txttensor = transforms.ToTensor()(txtimage)#.unsqueeze_(0)\n",
    "                    right_descriptions.append(txttensor)\n",
    "\n",
    "                grid = torchvision.utils.make_grid(patches, nrow=topk, padding=1)\n",
    "                grid_right_descriptions = torchvision.utils.make_grid(right_descriptions, nrow=1, padding=1)\n",
    "\n",
    "                # merging right description with the grid of images\n",
    "                grid = torch.cat([grid, grid_right_descriptions], dim=-1)\n",
    "\n",
    "                # description on the top\n",
    "                text = f'Node:{node.name}, p{p}, Child:{child_classname}'\n",
    "                txtimage = Image.new(\"RGB\", (grid.shape[-1], 224), (0, 0, 0))\n",
    "                draw = D.Draw(txtimage)\n",
    "                draw.text((200, patches[0].shape[1]//2), text, anchor='mm', fill=\"white\", font=font)\n",
    "                txttensor = transforms.ToTensor()(txtimage)#.unsqueeze_(0)\n",
    "\n",
    "                # merging top description with the grid of images\n",
    "                grid = torch.cat([grid, txttensor], dim=1)\n",
    "\n",
    "                os.makedirs(os.path.join(run_path, f'non_descendent_specific_topk_heatmap_uninterp_cosine_and_softmax_testloader_ep={epoch}', node.name), exist_ok=True)\n",
    "                torchvision.utils.save_image(grid, os.path.join(run_path, f'non_descendent_specific_topk_heatmap_uninterp_cosine_and_softmax_testloader_ep={epoch}', node.name, f'{child_classname}-p{p}.png'))\n",
    "\n",
    "print('Done !!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08576a0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
