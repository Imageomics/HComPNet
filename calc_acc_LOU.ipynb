{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3473e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import sys, os\n",
    "import random\n",
    "import numpy as np\n",
    "from shutil import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "import shutil\n",
    "import pickle\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "from torchvision.datasets.folder import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "# from skimage.filters import threshold_local, gaussian\n",
    "\n",
    "from datetime import datetime\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ecb88ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_path = \"runs/178-PruningNaiveHPIPNetMaskL1=0.5MaskTrainExtra=15epsEps=60TanhDesc=0.05MinCont=0.1_cnext26_CUB-190-imgnet-224_WeightedCE_with-equalize-aug_img=224_nprotos=20\"\n",
    "# ckpt_file_name = 'net_trained_last'\n",
    "\n",
    "# run_path = \"runs/179-PruningNaiveHPIPNetMaskL1=0.5MaskTrainExtra=15epsEps=60TanhDesc=0.05MinCont=0.1_cnext26_CUB-190-224_WeightedCE_with-equalize-aug_img=224_nprotos=20\"\n",
    "\n",
    "# run_path = \"runs/180-PruningNaiveHPIPNetMaskL1=0.5MaskTrainExtra=15epsEps=60TanhDesc=2.0MinCont=0.1_cnext26_CUB-190-imgnet-224_WeightedCE_with-equalize-aug_img=224_nprotos=20\"\n",
    "\n",
    "# run_path = \"runs/181-PruningNaiveHPIPNetMaskL1=0.5MaskTrainExtra=15epsEps=60TanhDesc=2.0MinCont=0.1_cnext26_CUB-190-224_WeightedCE_with-equalize-aug_img=224_nprotos=20\"\n",
    "\n",
    "# run_path = \"runs/182-PruningNaiveHPIPNetMaskL1=0.5MaskTrainExtra=15epsEps=60TanhDesc=0.1MinCont=0.1_cnext26_CUB-190-imgnet-224_WeightedCE_with-equalize-aug_img=224_nprotos=20\"\n",
    "\n",
    "# run_path = \"runs/183-PruningBF=1.1NaiveHPIPNetMaskL1=0.5MaskTrainExtra=15epsEps=60TanhDesc=0.05MinCont=0.1_cnext26_CUB-190-imgnet-224_WeightedCE_with-equalize-aug_img=224_nprotos=20\"\n",
    "\n",
    "# run_path = \"runs/185-LOUSet1-PruningBF=1.1NaiveHPIPNetMaskL1=0.5MaskTrainExtra=15epsEps=60TanhDesc=0.05MinCont=0.1_cnext26_CUB-190-imgnet-224_WeightedCE_with-equalize-aug_img=224_nprotos=20\"\n",
    "\n",
    "# run_path = \"runs/191-LOUSet2-PruningBF=1.1NaiveHPIPNetMaskL1=0.5MaskTrainExtra=05epsEps=85Cl=4.0TanhDesc=0.05MinCont=0.5_cnext26_CUB-190-imgnet-224_WeightedCE_with-equalize-aug_img=224_nprotos=20\"\n",
    "\n",
    "# run_path = \"runs/192-PruningNaiveHPIPNetMaskL1=0.5MaskTrainExtra=05epsEps=85Cl=4.0NoTanhDescMinCont=0.5_cnext26_CUB-190-imgnet-224_WeightedCE_with-equalize-aug_img=224_nprotos=20\"\n",
    "\n",
    "# run_path = \"runs/193-LOUSet3-PruningBF=1.1NaiveHPIPNetMaskL1=0.5MaskTrainExtra=05epsEps=85Cl=4.0TanhDesc=0.05MinCont=0.5_cnext26_CUB-190-imgnet-224_WeightedCE_with-equalize-aug_img=224_nprotos=20\"\n",
    "# ckpt_file_name = 'net_trained_last'\n",
    "\n",
    "# run_path = \"runs/195-LOUSet3-PruningBF=1.1NaiveHPIPNetMaskL1=0.5MaskTrainExtra=05epsEps=85Cl=4.0TanhDesc=0.05MinCont=0.5_cnext26_CUB-190-imgnet-224_WeightedCE_with-equalize-aug_img=224_nprotos=10pcOr1pd\"\n",
    "# ckpt_file_name = 'net_trained_last'\n",
    "\n",
    "# run_path = \"runs/198-rerun-LOUSet1-PruningBF=1.1NaiveHPIPNetMaskL1=0.5MaskTrainExtra=05epsEps=85Cl=4.0TanhDesc=0.05MinCont=0.5_cnext26_CUB-190-imgnet-224_WeightedCE_with-equalize-aug_img=224_nprotos=10pcOr1pd\"\n",
    "# ckpt_file_name = 'net_trained_last'\n",
    "\n",
    "# run_path = \"runs/199-rerun-LOUSet3-PruningBF=1.1NaiveHPIPNetOODEnt=-0.2MaskL1=0.5MaskTrainExtra=05epsEps=85Cl=4.0TanhDesc=0.05MinCont=0.5_cnext26_CUB-190-imgnet-224_WeightedCE_with-equalize-aug_img=224_nprotos=10pc\"\n",
    "# ckpt_file_name = 'net_trained_90'\n",
    "\n",
    "# run_path = \"runs/197-LOUSet3-PruningBF=1.1NaiveHPIPNetOODEnt=-0.2MaskL1=0.5MaskTrainExtra=05epsEps=85Cl=4.0TanhDesc=0.05MinCont=0.5_cnext26_CUB-190-imgnet-224_WeightedCE_with-equalize-aug_img=224_nprotos=10pcOr1pd\"\n",
    "# ckpt_file_name = 'net_trained_90'\n",
    "\n",
    "# run_path = \"runs/200-rerun-LOUSet1-PruningBF=1.1NaiveHPIPNetOODEnt=-0.2MaskL1=0.5MaskTrainExtra=05epsEps=85Cl=4.0TanhDesc=0.05MinCont=0.5_cnext26_CUB-190-imgnet-224_WeightedCE_with-equalize-aug_img=224_nprotos=10pc\"\n",
    "# ckpt_file_name = 'net_trained_last'\n",
    "\n",
    "# run_path = \"runs/201-LOUSet1-PruningBF=1.1NaiveHPIPNetOODEnt=-0.2MaskL1=0.5MaskTrainExtra=05epsEps=85Cl=4.0TanhDesc=0.05MinCont=0.5_cnext26_CUB-190-imgnet-224_WeightedCE_with-equalize-aug_img=224_nprotos=10pcOr1pd\"\n",
    "# ckpt_file_name = 'net_trained_90'\n",
    "\n",
    "run_path = \"runs/205-LOUSet4-PruningBF=1.1NaiveHPIPNetMaskL1=0.5MaskTrainExtra=05epsEps=60Cl=2.0TanhDesc=0.05MinCont=0.1_cnext26_CUB-190-imgnet-224_WeightedCE_with-equalize-aug_img=224_nprotos=10pc\"\n",
    "ckpt_file_name = 'net_trained_last'\n",
    "\n",
    "# run_path = \"runs/206-LOUSet5-PruningBF=1.1NaiveHPIPNetMaskL1=0.5MaskTrainExtra=05epsEps=60Cl=2.0TanhDesc=0.05MinCont=0.1_cnext26_CUB-190-imgnet-224_WeightedCE_with-equalize-aug_img=224_nprotos=10pc\"\n",
    "# ckpt_file_name = 'net_trained_last'\n",
    "\n",
    "# run_path = \"runs/207-PruningBF=1.1NaiveHPIPNetMaskL1=0.5MaskTrainExtra=05epsEps=60Cl=2.0TanhDesc=0.05MinCont=0.1_cnext26_BUT-51-224_WeightedCE_with-equalize-aug_img=224_nprotos=10pc\"\n",
    "# ckpt_file_name = 'net_trained_last'\n",
    "\n",
    "# run_path = \"runs/208-PruningBF=1.1NaiveHPIPNetMaskL1=0.5MaskTrainExtra=05epsEps=60Cl=2.0TanhDesc=0.05MinCont=0.1_cnext26_FISH-38-224_WeightedCE_with-equalize-aug_img=224_nprotos=10pc\"\n",
    "# ckpt_file_name = 'net_trained_last'\n",
    "\n",
    "# run_path = \"runs/216-178like-cnext26_PruningBF=1.1NaiveHPIPNetMaskL1=0.5MaskTrainExtra=05epsEps=60Cl=2.0TanhDesc=1.0MinCont=0.1_CUB-190-imgnet-224_WeightedCE_with-equalize-aug_img=224_nprotos=10pc\"\n",
    "# ckpt_file_name = 'net_trained_last'\n",
    "\n",
    "# run_path = \"runs/215-178like-cnext26_PruningBF=1.1NaiveHPIPNetMaskL1=0.5MaskTrainExtra=05epsEps=60Cl=2.0TanhDesc=0.5MinCont=0.1_CUB-190-imgnet-224_WeightedCE_with-equalize-aug_img=224_nprotos=10pc\"\n",
    "# ckpt_file_name = 'net_trained_last'\n",
    "\n",
    "# run_path = \"runs/214-178like-cnext26_PruningBF=1.1NaiveHPIPNetMaskL1=0.5MaskTrainExtra=05epsEps=60Cl=2.0TanhDesc=0.1MinCont=0.1_CUB-190-imgnet-224_WeightedCE_with-equalize-aug_img=224_nprotos=10pc\"\n",
    "# ckpt_file_name = 'net_trained_last'\n",
    "\n",
    "# run_path = \"runs/213-178like-cnext26_PruningBF=1.1NaiveHPIPNetMaskL1=0.5MaskTrainExtra=05epsEps=60Cl=2.0TanhDesc=0.05MinCont=0.1_CUB-190-imgnet-224_WeightedCE_with-equalize-aug_img=224_nprotos=10pc\"\n",
    "# ckpt_file_name = 'net_trained_last'\n",
    "\n",
    "print(run_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d53cfe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipnet.pipnet import PIPNet, get_network\n",
    "from util.log import Log\n",
    "from util.args import get_args, save_args, get_optimizer_nn\n",
    "from util.data import get_dataloaders\n",
    "from util.func import init_weights_xavier\n",
    "from pipnet.train import train_pipnet, test_pipnet\n",
    "# from pipnet.test import eval_pipnet, get_thresholds, eval_ood\n",
    "from util.eval_cub_csv import eval_prototypes_cub_parts_csv, get_topk_cub, get_proto_patches_cub\n",
    "from util.vis_pipnet import visualize, visualize_topk\n",
    "from util.visualize_prediction import vis_pred, vis_pred_experiments\n",
    "from util.node import Node\n",
    "from util.phylo_utils import construct_phylo_tree, construct_discretized_phylo_tree\n",
    "from util.func import get_patch_size\n",
    "from util.data import ModifiedLabelLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e5271b-b798-43a6-86b2-8aef7c52a7ff",
   "metadata": {},
   "source": [
    "# Save leave_out_classes to a variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f0fbbc-f837-48cf-a5e2-3cc4a4be71ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "args_file = open(os.path.join(run_path, 'metadata', 'args.pickle'), 'rb')\n",
    "args = pickle.load(args_file)\n",
    "\n",
    "if ('leave_out_classes' in args) and (args.leave_out_classes != ''):\n",
    "        with open(args.leave_out_classes, 'r') as file:\n",
    "            leave_out_classes = [line.strip() for line in file]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5a1043-540f-448c-b94c-ee06ba6bd91f",
   "metadata": {},
   "source": [
    "# Define tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11d438b",
   "metadata": {},
   "outputs": [],
   "source": [
    "args_file = open(os.path.join(run_path, 'metadata', 'args.pickle'), 'rb')\n",
    "args = pickle.load(args_file)\n",
    "\n",
    "if args.phylo_config:\n",
    "    phylo_config = OmegaConf.load(args.phylo_config)\n",
    "\n",
    "if args.phylo_config:\n",
    "    # construct the phylo tree\n",
    "    if phylo_config.phyloDistances_string == 'None':\n",
    "        if '031' in run_path: # this run uses a different phylogeny file that had an extra root node which is a mistake\n",
    "            root = construct_phylo_tree('data/phlyogenyCUB/18Species-with-extra-root-node/1_tree-consensus-Hacket-18Species-modified_cub-names_v1.phy')\n",
    "        else:\n",
    "            root = construct_phylo_tree(phylo_config.phylogeny_path)\n",
    "        print('-'*25 + ' No discretization ' + '-'*25)\n",
    "    else:\n",
    "        root = construct_discretized_phylo_tree(phylo_config.phylogeny_path, phylo_config.phyloDistances_string)\n",
    "        print('-'*25 + ' Discretized ' + '-'*25)\n",
    "else:\n",
    "    # construct the tree (original hierarchy as described in the paper)\n",
    "    root = Node(\"root\")\n",
    "    root.add_children(['animal','vehicle','everyday_object','weapon','scuba_diver'])\n",
    "    root.add_children_to('animal',['non_primate','primate'])\n",
    "    root.add_children_to('non_primate',['African_elephant','giant_panda','lion'])\n",
    "    root.add_children_to('primate',['capuchin','gibbon','orangutan'])\n",
    "    root.add_children_to('vehicle',['ambulance','pickup','sports_car'])\n",
    "    root.add_children_to('everyday_object',['laptop','sandal','wine_bottle'])\n",
    "    root.add_children_to('weapon',['assault_rifle','rifle'])\n",
    "    # flat root\n",
    "    # root.add_children(['scuba_diver','African_elephant','giant_panda','lion','capuchin','gibbon','orangutan','ambulance','pickup','sports_car','laptop','sandal','wine_bottle','assault_rifle','rifle'])\n",
    "root.assign_all_descendents()\n",
    "\n",
    "exp_no = int(os.path.basename(run_path)[:3])\n",
    "\n",
    "if exp_no < 77:\n",
    "    if ('num_protos_per_descendant' in args) and (args.num_protos_per_descendant > 0):\n",
    "        for node in root.nodes_with_children():\n",
    "            node.set_num_protos(args.num_protos_per_descendant)\n",
    "if exp_no == 77:\n",
    "    # update num of protos per node based on num_protos_per_descendant\n",
    "    if args.num_features == 0 and args.num_protos_per_descendant == 0:\n",
    "        raise Exception('Either of num_features or num_protos_per_descendant must be greater than zero')\n",
    "    for node in root.nodes_with_children():\n",
    "        node.set_num_protos(num_protos_per_descendant=args.num_protos_per_descendant,\\\n",
    "                                                            min_protos=args.num_features)\n",
    "\n",
    "elif 'num_protos_per_child' in args:\n",
    "    if ('num_protos_per_descendant' in args):\n",
    "        # update num of protos per node based on num_protos_per_descendant\n",
    "        if args.num_features == 0 and args.num_protos_per_descendant == 0 and args.num_protos_per_child == 0:\n",
    "            raise Exception('Either of num_features or num_protos_per_descendant must be greater than zero')\n",
    "        for node in root.nodes_with_children():\n",
    "            node.set_num_protos(num_protos_per_descendant=args.num_protos_per_descendant,\\\n",
    "                                num_protos_per_child=args.num_protos_per_child,\\\n",
    "                                min_protos=args.num_features,\\\n",
    "                                split_protos=('protopool' in args) and (args.protopool == 'n'))\n",
    "else:\n",
    "    if ('num_protos_per_descendant' in args):\n",
    "        # update num of protos per node based on num_protos_per_descendant\n",
    "        if args.num_features == 0 and args.num_protos_per_descendant == 0:\n",
    "            raise Exception('Either of num_features or num_protos_per_descendant must be greater than zero')\n",
    "        for node in root.nodes_with_children():\n",
    "            node.set_num_protos(num_protos_per_descendant=args.num_protos_per_descendant,\\\n",
    "                                num_protos_per_child=0,\\\n",
    "                                min_protos=args.num_features,\\\n",
    "                                split_protos=('protopool' in args) and (args.protopool == 'n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751cc10c-e33b-472c-b904-36e94ab2c189",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3ed910",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    device_ids = [torch.cuda.current_device()]\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    device_ids = []\n",
    "\n",
    "# args_file = open(os.path.join(run_path, 'metadata', 'args.pickle'), 'rb')\n",
    "# args = pickle.load(args_file)\n",
    "\n",
    "# ckpt_file_name = 'net_overspecific_pruned_replaced_thresh=0.5_last'\n",
    "# ckpt_file_name = 'net_trained_last'\n",
    "# ckpt_file_name = 'net_trained_10'\n",
    "# ckpt_file_name = 'net_pretrained'\n",
    "epoch = ckpt_file_name.split('_')[-1]\n",
    "\n",
    "ckpt_path = os.path.join(run_path, 'checkpoints', ckpt_file_name)\n",
    "checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "\n",
    "if ckpt_file_name != 'net_trained_last':\n",
    "    print('\\n', (10*'-')+'WARNING: Not using the final trained model'+(10*'-'), '\\n')\n",
    "\n",
    "# Obtain the dataset and dataloaders\n",
    "temp = args.leave_out_classes\n",
    "args.leave_out_classes = '' # NOTE: because here we need to load the entire dataloader, and filter out the classes later\n",
    "trainloader, trainloader_pretraining, trainloader_normal, trainloader_normal_augment, projectloader, testloader, test_projectloader, classes = get_dataloaders(args, device)\n",
    "args.leave_out_classes = temp\n",
    "\n",
    "print(args.batch_size, trainloader.batch_size)\n",
    "\n",
    "if len(classes)<=20:\n",
    "    if args.validation_size == 0.:\n",
    "        print(\"Classes: \", testloader.dataset.class_to_idx, flush=True)\n",
    "    else:\n",
    "        print(\"Classes: \", str(classes), flush=True)\n",
    "\n",
    "# Create a convolutional network based on arguments and add 1x1 conv layer\n",
    "feature_net, add_on_layers, pool_layer, classification_layers = get_network(len(classes), args, root=root)\n",
    "   \n",
    "# Create a PIP-Net\n",
    "net = PIPNet(num_classes=len(classes),\n",
    "                    feature_net = feature_net,\n",
    "                    args = args,\n",
    "                    add_on_layers = add_on_layers,\n",
    "                    pool_layer = pool_layer,\n",
    "                    classification_layers = classification_layers,\n",
    "                    num_parent_nodes = len(root.nodes_with_children()),\n",
    "                    root = root\n",
    "                    )\n",
    "net = net.to(device=device)\n",
    "net = nn.DataParallel(net, device_ids = device_ids)    \n",
    "net.load_state_dict(checkpoint['model_state_dict'],strict=True)\n",
    "# print(net.eval())\n",
    "criterion = nn.NLLLoss(reduction='mean').to(device)\n",
    "\n",
    "# Forward one batch through the backbone to get the latent output size\n",
    "with torch.no_grad():\n",
    "    xs1, _, _ = next(iter(trainloader))\n",
    "    xs1 = xs1.to(device)\n",
    "    _, proto_features, _, _ = net(xs1)\n",
    "    wshape = proto_features['root'].shape[-1]\n",
    "    args.wshape = wshape #needed for calculating image patch size\n",
    "    print(\"Output shape: \", proto_features['root'].shape, flush=True)\n",
    "    \n",
    "print(args.wshape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9b5b65-35ae-441d-939f-76897b2dad35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6339634b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from util.log import Log\n",
    "os.environ['WANDB_DISABLED'] = 'true'\n",
    "wandb_run = wandb.init(project=\"pipnet\", name=os.path.basename(args.log_dir), config=vars(args), reinit=False)\n",
    "log = Log(args.log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e8df17",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_net, optimizer_classifier, params_to_freeze, params_to_train, params_backbone = get_optimizer_nn(net, args)            \n",
    "scheduler_net = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_net, T_max=len(trainloader)*args.epochs, eta_min=args.lr_net/100.)\n",
    "if args.epochs<=30:\n",
    "    scheduler_classifier = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer_classifier, T_0=5, eta_min=0.001, T_mult=1, verbose=False)\n",
    "else:\n",
    "    scheduler_classifier = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer_classifier, T_0=10, eta_min=0.001, T_mult=1, verbose=False)\n",
    "\n",
    "    \n",
    "if args.OOD_dataset:\n",
    "    trainloader_OOD, trainloader_pretraining_OOD, trainloader_normal_OOD, trainloader_normal_augment_OOD, projectloader_OOD, testloader_OOD, test_projectloader_OOD, _ = get_dataloaders(args, device, OOD=True)\n",
    "    print('-'*25 + 'Using OOD data' + '-'*25)\n",
    "else:\n",
    "    trainloader_OOD = trainloader_pretraining_OOD = trainloader_normal_OOD = trainloader_normal_augment_OOD = projectloader_OOD = testloader_OOD = test_projectloader_OOD = None\n",
    "    print('-'*25 + 'Not using OOD data' + '-'*25)\n",
    "    \n",
    "if ('focal_loss' in args) and (args.focal_loss == 'y'):\n",
    "    from util.custom_losses import WeightedCrossEntropyLoss, WeightedNLLLoss, FocalLossWrapper\n",
    "    criterion = FocalLossWrapper(device=device, alpha=1, gamma=args.focal_loss_gamma, reduction='mean').to(device)\n",
    "else:\n",
    "    from util.custom_losses import WeightedCrossEntropyLoss, WeightedNLLLoss, FocalLossWrapper\n",
    "    criterion = WeightedNLLLoss(device=device).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb43785a-cf12-4460-bae5-efe9cf9d332d",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84a48d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_heatmap(latent_activation, input_image):\n",
    "    image_a = latent_activation.cpu().numpy()\n",
    "    image_a = (image_a - image_a.min()) / (image_a.max() - image_a.min())\n",
    "\n",
    "    input_image = (input_image - input_image.min()) / (input_image.max() - input_image.min())\n",
    "    image_b = input_image.permute(1, 2, 0).cpu().numpy()\n",
    "    \n",
    "    reshaped_image_a = np.array(Image.fromarray((image_a[0] * 255).astype('uint8')).resize((input_image.shape[-1], input_image.shape[-1])))\n",
    "    normalized_heatmap = (reshaped_image_a - np.min(reshaped_image_a)) / (np.max(reshaped_image_a) - np.min(reshaped_image_a))\n",
    "    \n",
    "    heatmap_colormap = plt.get_cmap('jet')\n",
    "    heatmap_colored = heatmap_colormap(normalized_heatmap)\n",
    "    \n",
    "    heatmap_colored_uint8 = (heatmap_colored[:, :, :3] * 255).astype(np.uint8)\n",
    "    image_a_heatmap_pillow = Image.fromarray(heatmap_colored_uint8)\n",
    "    image_b_pillow = Image.fromarray((image_b * 255).astype('uint8'))\n",
    "    \n",
    "    result_image = Image.blend(image_b_pillow, image_a_heatmap_pillow, alpha=0.3)\n",
    "    \n",
    "    return np.array(result_image)\n",
    "\n",
    "\n",
    "def get_heatmap_uninterpolated(latent_activation, input_image):\n",
    "    image_a = latent_activation.cpu().numpy()\n",
    "    image_a = (image_a - image_a.min()) / (image_a.max() - image_a.min())\n",
    "\n",
    "    input_image = (input_image - input_image.min()) / (input_image.max() - input_image.min())\n",
    "    image_b = input_image.permute(1, 2, 0).cpu().numpy()\n",
    "    \n",
    "    reshaped_image_a = np.array(Image.fromarray((image_a[0] * 255).astype('uint8')).resize((input_image.shape[-1], input_image.shape[-1]), \\\n",
    "                                                                                          resample=Image.NEAREST ))\n",
    "    normalized_heatmap = (reshaped_image_a - np.min(reshaped_image_a)) / (np.max(reshaped_image_a) - np.min(reshaped_image_a))\n",
    "    \n",
    "    heatmap_colormap = plt.get_cmap('jet')\n",
    "    heatmap_colored = heatmap_colormap(normalized_heatmap)\n",
    "    \n",
    "    heatmap_colored_uint8 = (heatmap_colored[:, :, :3] * 255).astype(np.uint8)\n",
    "    image_a_heatmap_pillow = Image.fromarray(heatmap_colored_uint8)\n",
    "    image_b_pillow = Image.fromarray((image_b * 255).astype('uint8'))\n",
    "    \n",
    "    result_image = Image.blend(image_b_pillow, image_a_heatmap_pillow, alpha=0.3)\n",
    "    \n",
    "    return np.array(result_image)\n",
    "\n",
    "def get_bb_gaussian_threshold(latent_activation, sigma=1.0, percentile=97, extend_h=0, extend_w=0):\n",
    "    # latent_activation -> []\n",
    "    upscaled_similarity = get_upscaled_activation_uninterpolated(latent_activation, \\\n",
    "                                                                 image_size=(args.image_size, args.image_size))\n",
    "    upscaled_similarity = minmaxscale(upscaled_similarity)\n",
    "    upscaled_similarity = gaussian(upscaled_similarity, sigma=sigma)\n",
    "    upscaled_similarity = threshold_local(upscaled_similarity, block_size=15, method='mean')\n",
    "    h_min, h_max, w_min, w_max = find_top_percentile_bbox(upscaled_similarity ,percentile=97)\n",
    "    h_min = max(0, h_min-extend_h)\n",
    "    h_max = min(upscaled_similarity.shape[0], h_max+extend_h)\n",
    "    w_min = max(0, w_min-extend_w)\n",
    "    w_max = min(upscaled_similarity.shape[1], w_max+extend_w)\n",
    "    return h_min, h_max, w_min, w_max\n",
    "\n",
    "\n",
    "def minmaxscale(tensor):\n",
    "    return (tensor - tensor.min()) / (tensor.max() - tensor.min())\n",
    "\n",
    "from torch.utils.data import DataLoader, SequentialSampler\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def unshuffle_dataloader(dataloader):\n",
    "    if type(dataloader.dataset) == ImageFolder:\n",
    "        dataset = dataloader.dataset\n",
    "    else:\n",
    "        dataset = dataloader.dataset.dataset.dataset\n",
    "    new_dataloader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=dataloader.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=dataloader.num_workers,\n",
    "        pin_memory=dataloader.pin_memory,\n",
    "        drop_last=dataloader.drop_last,\n",
    "        timeout=dataloader.timeout,\n",
    "        worker_init_fn=dataloader.worker_init_fn,\n",
    "        multiprocessing_context=dataloader.multiprocessing_context,\n",
    "        generator=dataloader.generator,\n",
    "        prefetch_factor=dataloader.prefetch_factor,\n",
    "        persistent_workers=dataloader.persistent_workers\n",
    "    )\n",
    "    return new_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d7da75-8319-4629-8865-668f55c0a8f0",
   "metadata": {},
   "source": [
    "# Fine accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c293507-c348-4fec-9669-938143926a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'cl_weight' not in args:\n",
    "    args.cl_weight = 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1414fe0c-accf-4289-9b2f-9dac08326ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testloader\n",
    "\n",
    "test_info, log_dict = test_pipnet(net, testloader, optimizer_net, optimizer_classifier, \\\n",
    "                                    scheduler_net, scheduler_classifier, criterion, 0, \\\n",
    "                                        args.epochs, device, pretrain=False, finetune=False, \\\n",
    "                                        test_loader_OOD=testloader_OOD, kernel_orth=args.kernel_orth == 'y', \\\n",
    "                                            tanh_desc=args.tanh_desc == 'y', align=args.align == 'y', uni=args.uni == 'y', align_pf=args.align_pf == 'y',\\\n",
    "                                            minmaximize=args.minmaximize == 'y', wandb_run=wandb_run, pretrain_epochs=args.epochs_pretrain, log=log, \\\n",
    "                                          args=args, apply_overspecificity_mask=False, path_prob_softmax_tau=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179f3017-a4a7-418a-8a25-496de314ceca",
   "metadata": {},
   "outputs": [],
   "source": [
    "round(test_info['fine_accuracy'], 4)\n",
    "test_info['fine_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f88bcde-7e7b-4f26-8d70-4b3cac5c874d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor([5, 3]).pow(1/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10f6a5d-d5b6-41f3-9da2-9ca115fa5855",
   "metadata": {},
   "source": [
    "# Geometric mean accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bff115-cb1e-44af-b08e-8126a4e40d97",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "leafname_to_depth = {}\n",
    "leafname_to_idx = testloader.dataset.class_to_idx\n",
    "\n",
    "\n",
    "\n",
    "def get_node_depth(node):\n",
    "    depth = 0\n",
    "    while node.parent:\n",
    "        depth += 1\n",
    "        node = node.parent\n",
    "    return depth\n",
    "\n",
    "for leaf_name in root.leaf_descendents:\n",
    "    leaf_node = root.get_node(leaf_name)\n",
    "    leafname_to_depth[leaf_name] = get_node_depth(leaf_node)\n",
    "\n",
    "def get_path_prob_list(out_probs, data_idx, root, leaf_node):\n",
    "    node = leaf_node\n",
    "    path_prob_list = []\n",
    "    while node.parent:\n",
    "        parent = node.parent\n",
    "        child_class_idx = parent.children_to_labels[node.name]\n",
    "        path_prob_list.append(out_probs[parent.name][data_idx, child_class_idx])\n",
    "        node = parent\n",
    "    return path_prob_list\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (xs, ys) in tqdm(enumerate(testloader), total=len(testloader)):\n",
    "\n",
    "        features, proto_features, pooled, out = net(xs)\n",
    "        out_probs = {}\n",
    "        for node_name, output_logits in out.items():\n",
    "            out_probs[node_name] = torch.softmax(output_logits, dim=1)\n",
    "\n",
    "        batch_leaf_probs_list = []\n",
    "        for data_idx in range(xs.shape[0]):\n",
    "            leaf_probs = [0 for _ in range(len(leafname_to_idx))]\n",
    "            for leafname, class_idx in leafname_to_idx.items():\n",
    "                leaf_node = root.get_node(leafname)\n",
    "                path_prob_list = get_path_prob_list(out_probs, data_idx, root, leaf_node)\n",
    "                depth = leafname_to_depth[leafname]\n",
    "                leaf_probs[class_idx] = torch.tensor([prob**(1/depth) for prob in path_prob_list]).prod()\n",
    "            batch_leaf_probs_list.append(torch.tensor(leaf_probs))\n",
    "        batch_leaf_probs = torch.stack(batch_leaf_probs_list)\n",
    "\n",
    "        correct += (batch_leaf_probs.argmax(dim=1) == ys).sum()\n",
    "        total += ys.shape[0]\n",
    "\n",
    "print('Accuracy:', correct / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c792a6-0923-4dc6-8b39-b3e92d47d71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(testloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449a56f4-b599-437e-b216-bd9ff22b2a61",
   "metadata": {},
   "source": [
    "# Leave one out accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de75a068-908d-454a-b5a4-1b5b3e441cae",
   "metadata": {},
   "source": [
    "## Remove classes from dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fb1e40-0c5d-469c-ba78-126b1f762e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "leave_out_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caed7bac-9b1a-4f96-8945-75427c659692",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Sampler, SubsetRandomSampler\n",
    "\n",
    "def create_filtered_dataloader(dataloader, new_sampler, batch_size):\n",
    "    if type(dataloader.dataset) == ImageFolder:\n",
    "        dataset = dataloader.dataset\n",
    "    else:\n",
    "        dataset = dataloader.dataset.dataset.dataset\n",
    "    new_dataloader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        sampler=new_sampler,\n",
    "        num_workers=dataloader.num_workers,\n",
    "        pin_memory=dataloader.pin_memory,\n",
    "        drop_last=dataloader.drop_last,\n",
    "        timeout=dataloader.timeout,\n",
    "        worker_init_fn=dataloader.worker_init_fn,\n",
    "        multiprocessing_context=dataloader.multiprocessing_context,\n",
    "        generator=dataloader.generator,\n",
    "        prefetch_factor=dataloader.prefetch_factor,\n",
    "        persistent_workers=dataloader.persistent_workers\n",
    "    )\n",
    "    return new_dataloader\n",
    "\n",
    "leave_out_loader = testloader\n",
    "# leave_out_loader = test_projectloader\n",
    "unique_labels = set()\n",
    "total_samples = 0\n",
    "for xs, ys in testloader:\n",
    "    unique_labels.update(ys.tolist())\n",
    "    total_samples += xs.shape[0]\n",
    "print(\"testloader Unique Labels:\", unique_labels)\n",
    "print(\"testloader total_samples:\", total_samples)\n",
    "\n",
    "# leave_out_classes = args.leave_out_classes.split(',')[2]\n",
    "\n",
    "class_of_interest = 0\n",
    "classes_to_keep = [leave_out_classes[class_of_interest]]\n",
    "\n",
    "# classes_to_keep = [leave_out_classes[1], leave_out_classes[6], leave_out_classes[8]]\n",
    "# classes_to_keep = [leave_out_classes[0], leave_out_classes[1], leave_out_classes[2], leave_out_classes[3], leave_out_classes[6], leave_out_classes[7]]\n",
    "\n",
    "\n",
    "# classes_to_keep = leave_out_classes\n",
    "\n",
    "# leave_out_classes = ['cub_093_Clark_Nutcracker']\n",
    "\n",
    "print('classes_to_keep', classes_to_keep)\n",
    "idx_of_classes_to_keep = set()\n",
    "name2label = leave_out_loader.dataset.class_to_idx # param\n",
    "label2name = {label:name for name, label in name2label.items()}\n",
    "for label in label2name:\n",
    "    # NOTE: Keeping the left out classes here\n",
    "    if label2name[label] in classes_to_keep:\n",
    "        idx_of_classes_to_keep.add(label)\n",
    "\n",
    "target_indices = []\n",
    "for i in range(len(leave_out_loader.dataset)):\n",
    "    *_, label = leave_out_loader.dataset[i]\n",
    "    if label in idx_of_classes_to_keep:\n",
    "        target_indices.append(i)\n",
    "sampler = SubsetRandomSampler(target_indices)\n",
    "to_shuffle = False\n",
    "    \n",
    "leave_out_loader = create_filtered_dataloader(leave_out_loader, sampler, batch_size=1) # dataloader.batch_size\n",
    "unique_labels = set()\n",
    "total_samples = 0\n",
    "for xs, ys in leave_out_loader:\n",
    "    unique_labels.update(ys.tolist())\n",
    "    total_samples += xs.shape[0]\n",
    "print(\"leave_out_loader Unique Labels:\", unique_labels)\n",
    "print(\"leave_out_loader total_samples:\", total_samples)\n",
    "\n",
    "\n",
    "\n",
    "# print('Leave out classes', args.leave_out_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b4aae4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# leave_out_loader.dataset.class_to_idx\n",
    "leave_out_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79ee1fb-5bdf-44d1-ad64-bdeafa2b449f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'cl_weight' not in args:\n",
    "    args.cl_weight = 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d710c6-2358-4550-974a-7b0526ceb117",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Check each left out image path probability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915e59f4-b165-4779-8a34-5ce2d4b353f5",
   "metadata": {},
   "source": [
    "This is to see what is the probablity for an image of a left out class at each node along the path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fde7ba-258c-42b8-86e9-ce9edccf4443",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "leafname_to_depth = {}\n",
    "leafname_to_idx = testloader.dataset.class_to_idx\n",
    "\n",
    "name2label = leave_out_loader.dataset.class_to_idx\n",
    "label2name = {label:name for name, label in name2label.items()}\n",
    "\n",
    "def get_node_depth(node):\n",
    "    depth = 0\n",
    "    while node.parent:\n",
    "        depth += 1\n",
    "        node = node.parent\n",
    "    return depth\n",
    "\n",
    "for leaf_name in root.leaf_descendents:\n",
    "    leaf_node = root.get_node(leaf_name)\n",
    "    leafname_to_depth[leaf_name] = get_node_depth(leaf_node)\n",
    "\n",
    "def get_path_prob_list(out_probs, data_idx, root, leaf_node):\n",
    "    node = leaf_node\n",
    "    path_prob_list = []\n",
    "    while node.parent:\n",
    "        parent = node.parent\n",
    "        child_class_idx = parent.children_to_labels[node.name]\n",
    "        path_prob_list.append(out_probs[parent.name][data_idx, child_class_idx])\n",
    "        node = parent\n",
    "    return path_prob_list\n",
    "\n",
    "def get_path_node_and_prob_list(out, out_probs, data_idx, root, leaf_node):\n",
    "    node = leaf_node\n",
    "    path_node_and_prob_list = []\n",
    "    while node.parent:\n",
    "        parent = node.parent\n",
    "        child_class_idx = parent.children_to_labels[node.name]\n",
    "        path_node_and_prob_list.append((node, out_probs[parent.name][data_idx, child_class_idx], \\\n",
    "                                        out[parent.name][data_idx, child_class_idx]))\n",
    "        node = parent\n",
    "    return path_node_and_prob_list\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (xs, ys) in enumerate(leave_out_loader):\n",
    "\n",
    "        print('Ground truth:', label2name[ys.item()])\n",
    "\n",
    "        features, proto_features, pooled, out = net(xs)\n",
    "        out_probs = {}\n",
    "        for node_name, output_logits in out.items():\n",
    "            out_probs[node_name] = torch.softmax(output_logits, dim=1)\n",
    "\n",
    "        batch_leaf_probs_list = []\n",
    "        for data_idx in range(xs.shape[0]):\n",
    "            leaf_probs = [0 for _ in range(len(leafname_to_idx))]\n",
    "            leafname_to_path_node_and_prob_list = {}\n",
    "            for leafname, class_idx in leafname_to_idx.items():\n",
    "                leaf_node = root.get_node(leafname)\n",
    "                \n",
    "                path_node_and_prob_list = get_path_node_and_prob_list(out, out_probs, data_idx, root, leaf_node)\n",
    "                depth = leafname_to_depth[leafname]\n",
    "                leaf_probs[class_idx] = torch.tensor([prob for _, prob, _ in path_node_and_prob_list]).prod()\n",
    "\n",
    "                leafname_to_path_node_and_prob_list[leafname] = path_node_and_prob_list\n",
    "            batch_leaf_probs_list.append(torch.tensor(leaf_probs))\n",
    "        batch_leaf_probs = torch.stack(batch_leaf_probs_list)\n",
    "\n",
    "        predicted_label = np.argmax(leaf_probs)\n",
    "        print('Predicted:', label2name[predicted_label])\n",
    "\n",
    "        predicted_node = root.get_node(label2name[predicted_label])\n",
    "        ground_truth_parent_node = root.get_node(label2name[ys.item()]).parent\n",
    "\n",
    "        print('Node of certainty:', ground_truth_parent_node.name)\n",
    "        print('Node of certainty in predicted path:', ground_truth_parent_node.name in [node.name for node, _, _ in leafname_to_path_node_and_prob_list[predicted_node.name]])\n",
    "\n",
    "        print(\" <- \".join([f\"({node.name}|{prob:.2f}|{logit:.2f})\" for node, prob, logit in leafname_to_path_node_and_prob_list[predicted_node.name]]))\n",
    "\n",
    "        correct += (batch_leaf_probs.argmax(dim=1) == ys).sum()\n",
    "        total += ys.shape[0]\n",
    "\n",
    "        print('-'*50)\n",
    "\n",
    "print('Accuracy:', correct / total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225c9fc1-d569-4dd5-a7dc-400d38ae22b2",
   "metadata": {},
   "source": [
    "## Logit distribution for descendant and non-descendant species for each node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a125c908-34e3-44e3-b854-9880e86ed66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "leafname_to_depth = {}\n",
    "leafname_to_idx = testloader.dataset.class_to_idx\n",
    "idx_to_leafname = {v: k for k, v in leafname_to_idx.items()}\n",
    "\n",
    "\n",
    "def get_node_depth(node):\n",
    "    depth = 0\n",
    "    while node.parent:\n",
    "        depth += 1\n",
    "        node = node.parent\n",
    "    return depth\n",
    "\n",
    "for leaf_name in root.leaf_descendents:\n",
    "    leaf_node = root.get_node(leaf_name)\n",
    "    leafname_to_depth[leaf_name] = get_node_depth(leaf_node)\n",
    "\n",
    "def get_path_prob_list(out_probs, data_idx, root, leaf_node):\n",
    "    node = leaf_node\n",
    "    path_prob_list = []\n",
    "    while node.parent:\n",
    "        parent = node.parent\n",
    "        child_class_idx = parent.children_to_labels[node.name]\n",
    "        path_prob_list.append(out_probs[parent.name][data_idx, child_class_idx])\n",
    "        node = parent\n",
    "    return path_prob_list\n",
    "\n",
    "def findCorrespondingToMax(base, target):\n",
    "    output, indices = F.max_pool2d(base, kernel_size=(26, 26), return_indices=True)# these are logits\n",
    "    tensor_flattened = target.view(target.shape[0], target.shape[1], -1)\n",
    "    indices_flattened = indices.view(target.shape[0], target.shape[1], -1)\n",
    "    corresponding_values_in_target = torch.gather(tensor_flattened, 2, indices_flattened)\n",
    "    corresponding_values_in_target = corresponding_values_in_target.view(target.shape[0],\\\n",
    "                                     target.shape[1], 1, 1)\n",
    "    pooled_target = corresponding_values_in_target\n",
    "    return pooled_target\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# maps node_name to list of descendant_logits and non_descendant_logits\n",
    "logits = defaultdict(lambda: {'descendants': [], 'non_descendants': []})\n",
    "avg_proto_activations_softmaxed = defaultdict(lambda: {'descendants': [], 'non_descendants': []})\n",
    "avg_proto_activations = defaultdict(lambda: {'descendants': [], 'non_descendants': []})\n",
    "with torch.no_grad():\n",
    "    cl_weights_dict = {node.name: getattr(net.module, '_'+node.name+'_classification').weight for node in root.nodes_with_children()}\n",
    "    for i, (xs, ys) in tqdm(enumerate(testloader), total=len(testloader)):\n",
    "        features, proto_features, proto_features_softmaxed, pooled, out = net(xs, return_inner_product=True)\n",
    "        pooled_ip = {}\n",
    "        for node in root.nodes_with_children():\n",
    "            pooled_ip[node.name] = findCorrespondingToMax(proto_features_softmaxed[node.name], proto_features[node.name])\n",
    "        for data_idx in range(xs.shape[0]):\n",
    "            gt_label = ys[data_idx].item()\n",
    "            gt_leafname = idx_to_leafname[gt_label]\n",
    "            \n",
    "            for node in root.nodes_with_children():\n",
    "                logit_of_prediction = out[node.name][data_idx, :].max().item()\n",
    "                if gt_leafname in node.leaf_descendents:\n",
    "                    logits[node.name]['descendants'].append(logit_of_prediction)\n",
    "                else:\n",
    "                    logits[node.name]['non_descendants'].append(logit_of_prediction)\n",
    "                    \n",
    "            # for node in root.nodes_with_children():\n",
    "            #     predicted_child_class_idx = out[node.name][data_idx, :].argmax().item()\n",
    "            #     # classification_weights = getattr(net.module, '_'+node.name+'_classification').weight\n",
    "            #     classification_weights = cl_weights_dict[node.name]\n",
    "            #     relevant_proto_idx = torch.nonzero(classification_weights[predicted_child_class_idx, :] > 1e-3).squeeze(-1)\n",
    "            #     avg_proto_activation = pooled[node.name][data_idx, relevant_proto_idx].mean().item()\n",
    "            #     if gt_leafname in node.leaf_descendents:\n",
    "            #         avg_proto_activations_softmaxed[node.name]['descendants'].append(avg_proto_activation)\n",
    "            #     else:\n",
    "            #         avg_proto_activations_softmaxed[node.name]['non_descendants'].append(avg_proto_activation)\n",
    "\n",
    "            #     avg_proto_activation = pooled_ip[node.name][data_idx, relevant_proto_idx].mean().item()\n",
    "            #     if gt_leafname in node.leaf_descendents:\n",
    "            #         avg_proto_activations[node.name]['descendants'].append(avg_proto_activation)\n",
    "            #     else:\n",
    "            #         avg_proto_activations[node.name]['non_descendants'].append(avg_proto_activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cce6dc6-72b9-4974-b50f-6595ef329f13",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# np.printoptions(formatter={'float': lambda x: \"{0:0.2f}\".format(x)})\n",
    "# np.set_printoptions(precision=3)\n",
    "\n",
    "for node in root.nodes_with_children():\n",
    "    print(node.name)\n",
    "    print(np.mean(logits[node.name]['descendants']).round(2), np.std(logits[node.name]['descendants']).round(2),\n",
    "          np.min(logits[node.name]['descendants']).round(2), np.max(logits[node.name]['descendants']).round(2))\n",
    "    if node.name != 'root':\n",
    "        print(np.mean(logits[node.name]['non_descendants']).round(2), np.std(logits[node.name]['non_descendants']).round(2),\n",
    "              np.min(logits[node.name]['non_descendants']).round(2), np.max(logits[node.name]['non_descendants']).round(2))\n",
    "    print('-'*20)\n",
    "\n",
    "\n",
    "# for node in root.nodes_with_children():\n",
    "#     print(node.name)\n",
    "#     print(np.mean(avg_proto_activations_softmaxed[node.name]['descendants']).round(2), np.std(avg_proto_activations_softmaxed[node.name]['descendants']).round(2),\n",
    "#           np.min(avg_proto_activations_softmaxed[node.name]['descendants']).round(2), np.max(avg_proto_activations_softmaxed[node.name]['descendants']).round(2))\n",
    "#     if node.name != 'root':\n",
    "#         print(np.mean(avg_proto_activations_softmaxed[node.name]['non_descendants']).round(2), np.std(avg_proto_activations_softmaxed[node.name]['non_descendants']).round(2),\n",
    "#               np.min(avg_proto_activations_softmaxed[node.name]['non_descendants']).round(2), np.max(avg_proto_activations_softmaxed[node.name]['non_descendants']).round(2))\n",
    "#     print('-'*20)\n",
    "\n",
    "# for node in root.nodes_with_children():\n",
    "#     print(node.name)\n",
    "#     print(np.mean(avg_proto_activations_softmaxed[node.name]['descendants']), \\\n",
    "#           np.std(avg_proto_activations_softmaxed[node.name]['descendants']))\n",
    "#     if node.name != 'root':\n",
    "#         print(np.mean(avg_proto_activations_softmaxed[node.name]['non_descendants']), \\\n",
    "#               np.std(avg_proto_activations_softmaxed[node.name]['non_descendants']))\n",
    "#     print('-'*20)\n",
    "\n",
    "# for node in root.nodes_with_children():\n",
    "#     print(node.name)\n",
    "#     print(np.mean(avg_proto_activations[node.name]['descendants']).round(2), \\\n",
    "#           np.std(avg_proto_activations[node.name]['descendants']).round(2),\n",
    "#          np.min(avg_proto_activations[node.name]['descendants']).round(2),\n",
    "#          np.max(avg_proto_activations[node.name]['descendants']).round(2))\n",
    "#     if node.name != 'root':\n",
    "#         print(np.mean(avg_proto_activations[node.name]['non_descendants']).round(2), \\\n",
    "#               np.std(avg_proto_activations[node.name]['non_descendants']).round(2),\n",
    "#              np.min(avg_proto_activations[node.name]['non_descendants']).round(2),\n",
    "#              np.max(avg_proto_activations[node.name]['non_descendants']).round(2))\n",
    "#     print('-'*20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e494f64-ca50-4a1b-9739-73e5a3304486",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Plot and save histograms of desc and non-desc logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5e1c6d-6a78-4635-a0fb-76a8c3896cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# data = {\n",
    "#     'node1': {'descendants': [1, 2, 3], 'non_descendants': [4, 5]},\n",
    "#     'node2': {'descendants': [2, 3, 4], 'non_descendants': [1, 5]}\n",
    "# }\n",
    "\n",
    "data = logits\n",
    "\n",
    "# Directory to save images\n",
    "output_dir = os.path.join(run_path, 'analysis', 'desc_and_nondesc_logit_distributions')\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "bins = 50\n",
    "# Plotting and saving histograms\n",
    "for node_name, inner_dict in data.items():\n",
    "    plt.figure()\n",
    "    # Plotting the histogram for descendants\n",
    "    plt.hist(inner_dict['descendants'], bins=bins, color='blue', alpha=0.5, label='Descendants', density=True)\n",
    "    # Plotting the histogram for non_descendants\n",
    "    plt.hist(inner_dict['non_descendants'], bins=bins, color='red', alpha=0.5, label='Non Descendants', density=True)\n",
    "    plt.title(f'Distribution for {node_name}')\n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.legend()\n",
    "    # Save the figure\n",
    "    plt.savefig(os.path.join(output_dir, f'{node_name}_distribution.png'))\n",
    "    # plt.show()\n",
    "    plt.close()\n",
    "\n",
    "print(\"All histograms have been saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c177f1f7-fbdf-47ca-963e-63f36271561c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Plot and save gaussian distribution of desc and non-desc logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d45946-80f4-4faf-9669-4697946ace4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from scipy.stats import norm\n",
    "\n",
    "\n",
    "# data = {\n",
    "#     'node1': {'descendants': [1, 2, 3], 'non_descendants': [4, 5]},\n",
    "#     'node2': {'descendants': [2, 3, 4], 'non_descendants': [1, 5]}\n",
    "# }\n",
    "\n",
    "data = logits\n",
    "\n",
    "# Directory to save images\n",
    "output_dir = os.path.join(run_path, 'analysis', 'desc_and_nondesc_logit_distributions_gaussian')\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Plotting and saving Gaussian curves\n",
    "for node_name, inner_dict in data.items():\n",
    "    plt.figure()\n",
    "    # Calculate and plot for descendants\n",
    "    mean_desc = np.mean(inner_dict['descendants'])\n",
    "    std_desc = np.std(inner_dict['descendants'])\n",
    "    xmin_desc, xmax_desc = mean_desc - 3*std_desc, mean_desc + 3*std_desc\n",
    "    x_desc = np.linspace(xmin_desc, xmax_desc, 100)\n",
    "    y_desc = norm.pdf(x_desc, mean_desc, std_desc)\n",
    "    plt.plot(x_desc, y_desc, label='Descendants', color='blue')\n",
    "\n",
    "    # Calculate and plot for non_descendants\n",
    "    mean_non_desc = np.mean(inner_dict['non_descendants'])\n",
    "    std_desc_non_desc = np.std(inner_dict['non_descendants'])\n",
    "    xmin_non_desc, xmax_non_desc = mean_non_desc - 3*std_desc_non_desc, mean_non_desc + 3*std_desc_non_desc\n",
    "    x_non_desc = np.linspace(xmin_non_desc, xmax_non_desc, 100)\n",
    "    y_non_desc = norm.pdf(x_non_desc, mean_non_desc, std_desc_non_desc)\n",
    "    plt.plot(x_non_desc, y_non_desc, label='Non Descendants', color='red')\n",
    "\n",
    "    plt.title(f'Normal Distribution for {node_name}')\n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Probability Density')\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(output_dir, f'{node_name}_gaussian.png'))\n",
    "    plt.close()\n",
    "\n",
    "print(\"All Gaussian plots have been saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3825c1-fa23-44f0-a520-e6013537a2c1",
   "metadata": {},
   "source": [
    "## Bayesian classifier based on logits to classify desc and non-desc images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21c20fe-e527-464a-8fe1-015410b7fa0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "likelihood_nodewise = {}\n",
    "\n",
    "def classify_point(data_A, data_B, x):\n",
    "    # Calculate parameters for Distribution A\n",
    "    mean_A = np.mean(data_A)\n",
    "    std_A = np.std(data_A, ddof=1)  # ddof=1 for sample standard deviation\n",
    "    # Calculate parameters for Distribution B\n",
    "    mean_B = np.mean(data_B)\n",
    "    std_B = np.std(data_B, ddof=1)\n",
    "    # Calculate prior probabilities\n",
    "    prior_A = 0.5  # assuming equal priors\n",
    "    prior_B = 0.5\n",
    "    # Calculate likelihoods using the Gaussian PDF\n",
    "    likelihood_A = norm.pdf(x, mean_A, std_A)\n",
    "    likelihood_B = norm.pdf(x, mean_B, std_B)\n",
    "    # Calculate evidence\n",
    "    evidence = likelihood_A * prior_A + likelihood_B * prior_B\n",
    "    # Calculate posterior probabilities using Bayes' Theorem\n",
    "    posterior_A = (likelihood_A * prior_A) / evidence\n",
    "    posterior_B = (likelihood_B * prior_B) / evidence\n",
    "    return posterior_A, posterior_B\n",
    "\n",
    "# # Example usage:\n",
    "# data_A = [1, 2, 3, 4, 5]  # Data for distribution A\n",
    "# data_B = [6, 7, 8, 9, 10]  # Data for distribution B\n",
    "# prediction_point = 5  # New data point to classify\n",
    "\n",
    "# # Get the probabilities for the new data point\n",
    "# prob_desc, prob_nondesc = classify_point(data_desc, data_nondesc, prediction_point)\n",
    "# print(\"Probability of belonging to Distribution A:\", prob_A)\n",
    "# print(\"Probability of belonging to Distribution B:\", prob_B)\n",
    "\n",
    "total = 0\n",
    "correct = 0\n",
    "\n",
    "acc_nodewise = {node.name: {'total': 0, 'correct': 0, 'gt': [], 'pred': []} for node in root.nodes_with_children()}\n",
    "# 1 -> desc\n",
    "# 0 -> non-desc\n",
    "\n",
    "with torch.no_grad():\n",
    "    cl_weights_dict = {node.name: getattr(net.module, '_'+node.name+'_classification').weight for node in root.nodes_with_children()}\n",
    "    for i, (xs, ys) in tqdm(enumerate(testloader), total=len(testloader)):\n",
    "        features, proto_features, proto_features_softmaxed, pooled, out = net(xs, return_inner_product=True)\n",
    "\n",
    "        for node in root.nodes_with_children():\n",
    "            pooled_ip[node.name] = findCorrespondingToMax(proto_features_softmaxed[node.name], proto_features[node.name])\n",
    "        for data_idx in range(xs.shape[0]):\n",
    "            gt_label = ys[data_idx].item()\n",
    "            gt_leafname = idx_to_leafname[gt_label]\n",
    "            \n",
    "            for node in root.nodes_with_children():\n",
    "                logit_of_prediction = out[node.name][data_idx, :].max().item()\n",
    "                prob_desc, prob_nondesc = classify_point(logits[node.name]['descendants'], \\\n",
    "                                                         logits[node.name]['non_descendants'], logit_of_prediction)\n",
    "                if (gt_leafname in node.leaf_descendents) and (prob_desc > prob_nondesc):# and (prob_desc >= 0.55):\n",
    "                    acc_nodewise[node.name]['correct'] += 1\n",
    "                elif (gt_leafname not in node.leaf_descendents) and (prob_desc < prob_nondesc):# and (prob_nondesc >= 0.55):\n",
    "                    acc_nodewise[node.name]['correct'] += 1\n",
    "                acc_nodewise[node.name]['total'] += 1\n",
    "                \n",
    "                if (gt_leafname in node.leaf_descendents):\n",
    "                    acc_nodewise[node.name]['gt'].append(1)\n",
    "                else:\n",
    "                    acc_nodewise[node.name]['gt'].append(0)\n",
    "\n",
    "                if (prob_desc >= prob_nondesc):\n",
    "                    acc_nodewise[node.name]['pred'].append(1)\n",
    "                else:\n",
    "                    acc_nodewise[node.name]['pred'].append(0)\n",
    "\n",
    "\n",
    "print('Accuracy:', np.mean([acc['correct']/acc['total'] for _, acc in acc_nodewise.items()]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79c7a39-ce5d-4fea-8727-e9f39aec0957",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_desc = []\n",
    "acc_list = []\n",
    "for node_name, acc in acc_nodewise.items():\n",
    "    node = root.get_node(node_name)\n",
    "    num_desc.append(len(node.leaf_descendents))\n",
    "    acc_list.append(acc['correct']/acc['total'])\n",
    "\n",
    "def plot_xy(x, y):\n",
    "    plt.figure(figsize=(8, 6))  # Set the figure size\n",
    "    plt.scatter(x, y, color='blue')\n",
    "    # plt.plot(x, y, color='red', label='Line connecting points')\n",
    "    plt.title('Plot of X vs Y')\n",
    "    plt.xlabel('Accuracy')\n",
    "    plt.ylabel('Num descendants')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Call the function with the example data\n",
    "plot_xy(num_desc, acc_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00acfb04-a641-434c-b942-f6f96b6151dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert leave_out_loader.batch_size == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac38fb8b-b7b8-4834-a463-a8e375f29462",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_path_node_and_prob_list(out, out_probs, data_idx, root, leaf_node):\n",
    "    node = leaf_node\n",
    "    path_node_and_prob_list = []\n",
    "    while node.parent:\n",
    "        parent = node.parent\n",
    "        child_class_idx = parent.children_to_labels[node.name]\n",
    "        path_node_and_prob_list.append((node, out_probs[parent.name][data_idx, child_class_idx], \\\n",
    "                                        out[parent.name][data_idx, child_class_idx]))\n",
    "        node = parent\n",
    "    return path_node_and_prob_list\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    assert leave_out_loader.batch_size == 1\n",
    "    for i, (xs, ys) in enumerate(leave_out_loader):\n",
    "        print('Ground truth:', label2name[ys.item()])\n",
    "        features, proto_features, pooled, out = net(xs)\n",
    "        out_probs = {}\n",
    "        for node_name, output_logits in out.items():\n",
    "            out_probs[node_name] = torch.softmax(output_logits, dim=1)\n",
    "        batch_leaf_probs_list = []\n",
    "        desc_or_nondesc = defaultdict(list)\n",
    "        for data_idx in range(xs.shape[0]):\n",
    "            leaf_probs = [0 for _ in range(len(leafname_to_idx))]\n",
    "            leafname_to_path_node_and_prob_list = {}\n",
    "            for leafname, class_idx in leafname_to_idx.items():\n",
    "                leaf_node = root.get_node(leafname)\n",
    "                \n",
    "                path_node_and_prob_list = get_path_node_and_prob_list(out, out_probs, data_idx, root, leaf_node)\n",
    "\n",
    "                for node, _, logit_of_prediction in path_node_and_prob_list:\n",
    "                    prob_desc, prob_nondesc = classify_point(logits[node.name]['descendants'], \\\n",
    "                                                         logits[node.name]['non_descendants'], logit_of_prediction.item())\n",
    "                    desc_or_nondesc[leafname].append((node, prob_desc > prob_nondesc))\n",
    "\n",
    "                depth = leafname_to_depth[leafname]\n",
    "                leaf_probs[class_idx] = torch.tensor([prob for _, prob, _ in path_node_and_prob_list]).prod()\n",
    "\n",
    "                leafname_to_path_node_and_prob_list[leafname] = path_node_and_prob_list\n",
    "            batch_leaf_probs_list.append(torch.tensor(leaf_probs))\n",
    "            \n",
    "        batch_leaf_probs = torch.stack(batch_leaf_probs_list)\n",
    "        predicted_label = np.argmax(leaf_probs)\n",
    "        print('Predicted:', label2name[predicted_label])\n",
    "        predicted_node = root.get_node(label2name[predicted_label])\n",
    "        ground_truth_parent_node = root.get_node(label2name[ys.item()]).parent\n",
    "        print('Node of certainty:', ground_truth_parent_node.name)\n",
    "        print('Node of certainty in predicted path:', ground_truth_parent_node.name in [node.name for node, _, _ in leafname_to_path_node_and_prob_list[predicted_node.name]])\n",
    "        # print(\" <- \".join([f\"({node.name}|{prob:.2f}|{logit:.2f})\" for node, prob, logit in leafname_to_path_node_and_prob_list[predicted_node.name]]))\n",
    "        print(\" <- \".join([f\"({node.name}|{d_or_nd})\" for node, d_or_nd in desc_or_nondesc[predicted_node.name]]))\n",
    "        correct += (batch_leaf_probs.argmax(dim=1) == ys).sum()\n",
    "        total += ys.shape[0]\n",
    "        print('-'*50)\n",
    "\n",
    "print('Accuracy:', correct / total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d131c4ec-83d8-451a-b02c-42ed8841130a",
   "metadata": {},
   "source": [
    "## Leave out without mask pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e724d2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# testloader\n",
    "\n",
    "test_info, log_dict = test_pipnet(net, leave_out_loader, optimizer_net, optimizer_classifier, \\\n",
    "                                    scheduler_net, scheduler_classifier, criterion, 0, \\\n",
    "                                        args.epochs, device, pretrain=False, finetune=False, \\\n",
    "                                        test_loader_OOD=testloader_OOD, kernel_orth=args.kernel_orth == 'y', \\\n",
    "                                            tanh_desc=args.tanh_desc == 'y', align=args.align == 'y', uni=args.uni == 'y', align_pf=args.align_pf == 'y',\\\n",
    "                                            minmaximize=args.minmaximize == 'y', wandb_run=wandb_run, pretrain_epochs=args.epochs_pretrain, log=log, \\\n",
    "                                          args=args, apply_overspecificity_mask=False, leave_out_classes=leave_out_classes, path_prob_softmax_tau=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0562137d-d40c-453a-b950-87626d5abcf1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "round(test_info['fine_accuracy'], 2)\n",
    "leave_out_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea6aab4-9851-4c8c-a7ed-60b0ff34c3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test_info['node_accuracy'].values()\n",
    "\n",
    "# # print(leave_out_classes[class_of_interest])\n",
    "\n",
    "# # for key in test_info['node_accuracy']:\n",
    "# #     if leave_out_classes[class_of_interest] in modified_root.get_node(key).leaf_descendents:\n",
    "# #         print(key, test_info['node_accuracy'][key]['accuracy'])\n",
    "\n",
    "# print(leave_out_classes[class_of_interest])\n",
    "# leave_out_accuracy = 1\n",
    "# for key in test_info['node_accuracy']:\n",
    "#     if leave_out_classes[class_of_interest] in root.get_node(key).leaf_descendents:\n",
    "#         if not (leave_out_classes[class_of_interest] in [child.name for child in root.get_node(key).children]):\n",
    "#             leave_out_accuracy *= test_info['node_accuracy'][key]['accuracy'] / 100\n",
    "#         print(key, test_info['node_accuracy'][key]['accuracy'])\n",
    "\n",
    "# print(leave_out_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec387d2-56a5-4046-b426-2721ed40d729",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Sampler, SubsetRandomSampler\n",
    "\n",
    "def create_filtered_dataloader(dataloader, new_sampler, batch_size):\n",
    "    if type(dataloader.dataset) == ImageFolder:\n",
    "        dataset = dataloader.dataset\n",
    "    else:\n",
    "        dataset = dataloader.dataset.dataset.dataset\n",
    "    new_dataloader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        sampler=new_sampler,\n",
    "        num_workers=dataloader.num_workers,\n",
    "        pin_memory=dataloader.pin_memory,\n",
    "        drop_last=dataloader.drop_last,\n",
    "        timeout=dataloader.timeout,\n",
    "        worker_init_fn=dataloader.worker_init_fn,\n",
    "        multiprocessing_context=dataloader.multiprocessing_context,\n",
    "        generator=dataloader.generator,\n",
    "        prefetch_factor=dataloader.prefetch_factor,\n",
    "        persistent_workers=dataloader.persistent_workers\n",
    "    )\n",
    "    return new_dataloader\n",
    "\n",
    "leave_out_loader = testloader\n",
    "\n",
    "for left_out_class in leave_out_classes:\n",
    "\n",
    "    classes_to_keep = [left_out_class]\n",
    "    \n",
    "    print('classes_to_keep', classes_to_keep)\n",
    "    idx_of_classes_to_keep = set()\n",
    "    name2label = leave_out_loader.dataset.class_to_idx # param\n",
    "    label2name = {label:name for name, label in name2label.items()}\n",
    "    for label in label2name:\n",
    "        # NOTE: Keeping the left out classes here\n",
    "        if label2name[label] in classes_to_keep:\n",
    "            idx_of_classes_to_keep.add(label)\n",
    "    \n",
    "    target_indices = []\n",
    "    for i in range(len(leave_out_loader.dataset)):\n",
    "        *_, label = leave_out_loader.dataset[i]\n",
    "        if label in idx_of_classes_to_keep:\n",
    "            target_indices.append(i)\n",
    "    sampler = SubsetRandomSampler(target_indices)\n",
    "    to_shuffle = False\n",
    "        \n",
    "    leave_out_loader = create_filtered_dataloader(leave_out_loader, sampler, batch_size=256) # dataloader.batch_size\n",
    "    unique_labels = set()\n",
    "    total_samples = 0\n",
    "    for xs, ys in leave_out_loader:\n",
    "        unique_labels.update(ys.tolist())\n",
    "        total_samples += xs.shape[0]\n",
    "    print(\"leave_out_loader Unique Labels:\", unique_labels)\n",
    "    print(\"leave_out_loader total_samples:\", total_samples)\n",
    "    \n",
    "    name2label = leave_out_loader.dataset.class_to_idx\n",
    "    label2name = {label:name for name, label in name2label.items()}\n",
    "    \n",
    "    # print('Leave out classes', args.leave_out_classes)\n",
    "\n",
    "    # testloader\n",
    "\n",
    "    test_info, log_dict = test_pipnet(net, leave_out_loader, optimizer_net, optimizer_classifier, \\\n",
    "                                        scheduler_net, scheduler_classifier, criterion, 0, \\\n",
    "                                            args.epochs, device, pretrain=False, finetune=False, \\\n",
    "                                            test_loader_OOD=testloader_OOD, kernel_orth=args.kernel_orth == 'y', \\\n",
    "                                                tanh_desc=args.tanh_desc == 'y', align=args.align == 'y', uni=args.uni == 'y', align_pf=args.align_pf == 'y',\\\n",
    "                                                minmaximize=args.minmaximize == 'y', wandb_run=wandb_run, pretrain_epochs=args.epochs_pretrain, log=log, \\\n",
    "                                              args=args, apply_overspecificity_mask=False, leave_out_classes=classes_to_keep, path_prob_softmax_tau=1)\n",
    "    print(left_out_class, round(test_info['fine_accuracy'], 2), '-'*10)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de034c67-9ede-412b-be53-789559a47892",
   "metadata": {},
   "source": [
    "## Leave out with mask pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c9c7f4-6a58-408f-88f4-410a0bd10f1e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# testloader\n",
    "\n",
    "test_info, log_dict = test_pipnet(net, leave_out_loader, optimizer_net, optimizer_classifier, \\\n",
    "                                    scheduler_net, scheduler_classifier, criterion, 0, \\\n",
    "                                        args.epochs, device, pretrain=False, finetune=False, \\\n",
    "                                        test_loader_OOD=testloader_OOD, kernel_orth=args.kernel_orth == 'y', \\\n",
    "                                            tanh_desc=args.tanh_desc == 'y', align=args.align == 'y', uni=args.uni == 'y', align_pf=args.align_pf == 'y',\\\n",
    "                                            minmaximize=args.minmaximize == 'y', wandb_run=wandb_run, pretrain_epochs=args.epochs_pretrain, log=log, \\\n",
    "                                          args=args, apply_overspecificity_mask=True, leave_out_classes=leave_out_classes, path_prob_softmax_tau=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77dfced7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(net.module._root_proto_presence)\n",
    "# print(net.module._root_proto_presence.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c939568-6ca9-45c9-a4d8-b444ffafd662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# net.module._root_proto_presence[:, 0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f15b657-6768-4f92-95ac-1219edecec44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cat([net.module._root_proto_presence[:, 1].unsqueeze(-1), net.module._root_proto_presence[:, 0].unsqueeze(-1)], dim=-1)#.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024d55a8-209d-4325-af85-66afbe199ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.no_grad():\n",
    "#     for node in root.nodes_with_children():\n",
    "#         net.module._root_proto_presence\n",
    "#         proto_presence = getattr(net.module, '_'+node.name+'_proto_presence')\n",
    "#         reversed_proto_presence = torch.cat([proto_presence[:, 1].unsqueeze(-1), proto_presence[:, 0].unsqueeze(-1)], dim=-1)\n",
    "#         setattr(net.module, '_'+node.name+'_proto_presence', \\\n",
    "#                 nn.Parameter(reversed_proto_presence))\n",
    "\n",
    "#     test_info, log_dict = test_pipnet(net, leave_out_loader, optimizer_net, optimizer_classifier, \\\n",
    "#                                     scheduler_net, scheduler_classifier, criterion, 0, \\\n",
    "#                                         args.epochs, device, pretrain=False, finetune=False, \\\n",
    "#                                         test_loader_OOD=testloader_OOD, kernel_orth=args.kernel_orth == 'y', \\\n",
    "#                                             tanh_desc=args.tanh_desc == 'y', align=args.align == 'y', uni=args.uni == 'y', align_pf=args.align_pf == 'y',\\\n",
    "#                                             minmaximize=args.minmaximize == 'y', wandb_run=wandb_run, pretrain_epochs=args.epochs_pretrain, log=log, \\\n",
    "#                                           args=args, apply_overspecificity_mask=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43dc275-1062-4aa9-9628-e5a9ce35c1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor([False, True, True]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46aa7a0a-f573-4cca-932a-40340dcb3954",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
