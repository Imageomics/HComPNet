{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bf3473e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "import sys, os\n",
    "import random\n",
    "import numpy as np\n",
    "from shutil import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "import shutil\n",
    "import pickle\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torchvision.datasets.folder import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "# from skimage.filters import threshold_local, gaussian\n",
    "\n",
    "from datetime import datetime\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2ecb88ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runs/208-PruningBF=1.1NaiveHPIPNetMaskL1=0.5MaskTrainExtra=05epsEps=60Cl=2.0TanhDesc=0.05MinCont=0.1_cnext26_FISH-38-224_WeightedCE_with-equalize-aug_img=224_nprotos=10pc\n"
     ]
    }
   ],
   "source": [
    "# 154 pruning CUB-18\n",
    "# run_path = \"runs/154-PruningNaiveHPIPNet_cnext13_CUB-18-imgnet-224_with-equalize-aug_img=224_nprotos=20\"\n",
    "\n",
    "# 155 pruning CUB-190\n",
    "# run_path = \"runs/155-PruningNaiveHPIPNet_cnext13_CUB-190-imgnet-224_with-equalize-aug_img=224_nprotos=20\"\n",
    "\n",
    "# 153 pruning CUB-29\n",
    "# run_path = \"runs/153-PruningNaiveHPIPNet_cnext13_CUB-29-imgnet-224_with-equalize-aug_img=224_nprotos=20\"\n",
    "\n",
    "# 156 pruning CUB-18 LOU3\n",
    "# run_path = \"runs/156-PruningNaiveHPIPNet_LOU3_cnext13_CUB-18-imgnet-224_with-equalize-aug_img=224_nprotos=20\"\n",
    "\n",
    "# 157 pruning CUB-29 LOU3\n",
    "# run_path = \"runs/157-PruningNaiveHPIPNet_LOU3_cnext13_CUB-29-imgnet-224_with-equalize-aug_img=224_nprotos=20\"\n",
    "\n",
    "# run_path = \"runs/178-PruningNaiveHPIPNetMaskL1=0.5MaskTrainExtra=15epsEps=60TanhDesc=0.05MinCont=0.1_cnext26_CUB-190-imgnet-224_WeightedCE_with-equalize-aug_img=224_nprotos=20\"\n",
    "# ckpt_file_name = 'net_trained_last'\n",
    "\n",
    "# run_path = \"runs/179-PruningNaiveHPIPNetMaskL1=0.5MaskTrainExtra=15epsEps=60TanhDesc=0.05MinCont=0.1_cnext26_CUB-190-224_WeightedCE_with-equalize-aug_img=224_nprotos=20\"\n",
    "\n",
    "# run_path = \"runs/180-PruningNaiveHPIPNetMaskL1=0.5MaskTrainExtra=15epsEps=60TanhDesc=2.0MinCont=0.1_cnext26_CUB-190-imgnet-224_WeightedCE_with-equalize-aug_img=224_nprotos=20\"\n",
    "\n",
    "# run_path = \"runs/181-PruningNaiveHPIPNetMaskL1=0.5MaskTrainExtra=15epsEps=60TanhDesc=2.0MinCont=0.1_cnext26_CUB-190-224_WeightedCE_with-equalize-aug_img=224_nprotos=20\"\n",
    "\n",
    "# run_path = \"runs/182-PruningNaiveHPIPNetMaskL1=0.5MaskTrainExtra=15epsEps=60TanhDesc=0.1MinCont=0.1_cnext26_CUB-190-imgnet-224_WeightedCE_with-equalize-aug_img=224_nprotos=20\"\n",
    "\n",
    "# run_path = \"runs/183-PruningBF=1.1NaiveHPIPNetMaskL1=0.5MaskTrainExtra=15epsEps=60TanhDesc=0.05MinCont=0.1_cnext26_CUB-190-imgnet-224_WeightedCE_with-equalize-aug_img=224_nprotos=20\"\n",
    "\n",
    "# run_path = \"runs/185-LOUSet1-PruningBF=1.1NaiveHPIPNetMaskL1=0.5MaskTrainExtra=15epsEps=60TanhDesc=0.05MinCont=0.1_cnext26_CUB-190-imgnet-224_WeightedCE_with-equalize-aug_img=224_nprotos=20\"\n",
    "\n",
    "# run_path = \"runs/191-LOUSet2-PruningBF=1.1NaiveHPIPNetMaskL1=0.5MaskTrainExtra=05epsEps=85Cl=4.0TanhDesc=0.05MinCont=0.5_cnext26_CUB-190-imgnet-224_WeightedCE_with-equalize-aug_img=224_nprotos=20\"\n",
    "\n",
    "# run_path = \"runs/192-PruningNaiveHPIPNetMaskL1=0.5MaskTrainExtra=05epsEps=85Cl=4.0NoTanhDescMinCont=0.5_cnext26_CUB-190-imgnet-224_WeightedCE_with-equalize-aug_img=224_nprotos=20\"\n",
    "\n",
    "# run_path = \"runs/193-LOUSet3-PruningBF=1.1NaiveHPIPNetMaskL1=0.5MaskTrainExtra=05epsEps=85Cl=4.0TanhDesc=0.05MinCont=0.5_cnext26_CUB-190-imgnet-224_WeightedCE_with-equalize-aug_img=224_nprotos=20\"\n",
    "# ckpt_file_name = 'net_trained_last'\n",
    "\n",
    "# run_path = \"runs/195-LOUSet3-PruningBF=1.1NaiveHPIPNetMaskL1=0.5MaskTrainExtra=05epsEps=85Cl=4.0TanhDesc=0.05MinCont=0.5_cnext26_CUB-190-imgnet-224_WeightedCE_with-equalize-aug_img=224_nprotos=10pcOr1pd\"\n",
    "# ckpt_file_name = 'net_trained_last'\n",
    "\n",
    "# run_path = \"runs/198-rerun-LOUSet1-PruningBF=1.1NaiveHPIPNetMaskL1=0.5MaskTrainExtra=05epsEps=85Cl=4.0TanhDesc=0.05MinCont=0.5_cnext26_CUB-190-imgnet-224_WeightedCE_with-equalize-aug_img=224_nprotos=10pcOr1pd\"\n",
    "# ckpt_file_name = 'net_trained_last'\n",
    "\n",
    "# run_path = \"runs/199-rerun-LOUSet3-PruningBF=1.1NaiveHPIPNetOODEnt=-0.2MaskL1=0.5MaskTrainExtra=05epsEps=85Cl=4.0TanhDesc=0.05MinCont=0.5_cnext26_CUB-190-imgnet-224_WeightedCE_with-equalize-aug_img=224_nprotos=10pc\"\n",
    "# ckpt_file_name = 'net_trained_90'\n",
    "\n",
    "# run_path = \"runs/197-LOUSet3-PruningBF=1.1NaiveHPIPNetOODEnt=-0.2MaskL1=0.5MaskTrainExtra=05epsEps=85Cl=4.0TanhDesc=0.05MinCont=0.5_cnext26_CUB-190-imgnet-224_WeightedCE_with-equalize-aug_img=224_nprotos=10pcOr1pd\"\n",
    "# ckpt_file_name = 'net_trained_90'\n",
    "\n",
    "# run_path = \"runs/200-rerun-LOUSet1-PruningBF=1.1NaiveHPIPNetOODEnt=-0.2MaskL1=0.5MaskTrainExtra=05epsEps=85Cl=4.0TanhDesc=0.05MinCont=0.5_cnext26_CUB-190-imgnet-224_WeightedCE_with-equalize-aug_img=224_nprotos=10pc\"\n",
    "# ckpt_file_name = 'net_trained_last'\n",
    "\n",
    "# run_path = \"runs/201-LOUSet1-PruningBF=1.1NaiveHPIPNetOODEnt=-0.2MaskL1=0.5MaskTrainExtra=05epsEps=85Cl=4.0TanhDesc=0.05MinCont=0.5_cnext26_CUB-190-imgnet-224_WeightedCE_with-equalize-aug_img=224_nprotos=10pcOr1pd\"\n",
    "# ckpt_file_name = 'net_trained_90'\n",
    "\n",
    "# run_path = \"runs/205-LOUSet4-PruningBF=1.1NaiveHPIPNetMaskL1=0.5MaskTrainExtra=05epsEps=60Cl=2.0TanhDesc=0.05MinCont=0.1_cnext26_CUB-190-imgnet-224_WeightedCE_with-equalize-aug_img=224_nprotos=10pc\"\n",
    "# ckpt_file_name = 'net_trained_last'\n",
    "\n",
    "# run_path = \"runs/206-LOUSet5-PruningBF=1.1NaiveHPIPNetMaskL1=0.5MaskTrainExtra=05epsEps=60Cl=2.0TanhDesc=0.05MinCont=0.1_cnext26_CUB-190-imgnet-224_WeightedCE_with-equalize-aug_img=224_nprotos=10pc\"\n",
    "# ckpt_file_name = 'net_trained_last'\n",
    "\n",
    "# run_path = \"runs/207-PruningBF=1.1NaiveHPIPNetMaskL1=0.5MaskTrainExtra=05epsEps=60Cl=2.0TanhDesc=0.05MinCont=0.1_cnext26_BUT-51-224_WeightedCE_with-equalize-aug_img=224_nprotos=10pc\"\n",
    "# ckpt_file_name = 'net_trained_last'\n",
    "\n",
    "run_path = \"runs/208-PruningBF=1.1NaiveHPIPNetMaskL1=0.5MaskTrainExtra=05epsEps=60Cl=2.0TanhDesc=0.05MinCont=0.1_cnext26_FISH-38-224_WeightedCE_with-equalize-aug_img=224_nprotos=10pc\"\n",
    "ckpt_file_name = 'net_trained_last'\n",
    "\n",
    "\n",
    "# try:\n",
    "#     sys.path.remove('/home/harishbabu/projects/PIPNet')\n",
    "# except:\n",
    "#     pass\n",
    "# sys.path.insert(0, os.path.join(run_path, 'source_clone'))\n",
    "\n",
    "print(run_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9d53cfe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipnet.pipnet import PIPNet, get_network\n",
    "from util.log import Log\n",
    "from util.args import get_args, save_args, get_optimizer_nn\n",
    "from util.data import get_dataloaders\n",
    "from util.func import init_weights_xavier\n",
    "from pipnet.train import train_pipnet, test_pipnet\n",
    "# from pipnet.test import eval_pipnet, get_thresholds, eval_ood\n",
    "from util.eval_cub_csv import eval_prototypes_cub_parts_csv, get_topk_cub, get_proto_patches_cub\n",
    "from util.vis_pipnet import visualize, visualize_topk\n",
    "from util.visualize_prediction import vis_pred, vis_pred_experiments\n",
    "from util.node import Node\n",
    "from util.phylo_utils import construct_phylo_tree, construct_discretized_phylo_tree\n",
    "from util.func import get_patch_size\n",
    "from util.data import ModifiedLabelLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e5271b-b798-43a6-86b2-8aef7c52a7ff",
   "metadata": {},
   "source": [
    "# Save leave_out_classes to a variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d5f0fbbc-f837-48cf-a5e2-3cc4a4be71ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "args_file = open(os.path.join(run_path, 'metadata', 'args.pickle'), 'rb')\n",
    "args = pickle.load(args_file)\n",
    "\n",
    "if ('leave_out_classes' in args) and (args.leave_out_classes != ''):\n",
    "        with open(args.leave_out_classes, 'r') as file:\n",
    "            leave_out_classes = [line.strip() for line in file]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5a1043-540f-448c-b94c-ee06ba6bd91f",
   "metadata": {},
   "source": [
    "# Define tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c11d438b",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/harishbabu/projects/PIPNet/configs/fish38_phylogeny.yaml'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m args \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(args_file)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mphylo_config:\n\u001b[0;32m----> 5\u001b[0m     phylo_config \u001b[38;5;241m=\u001b[39m \u001b[43mOmegaConf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mphylo_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mphylo_config:\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# construct the phylo tree\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m phylo_config\u001b[38;5;241m.\u001b[39mphyloDistances_string \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNone\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[0;32m~/.conda/envs/hpnet4/lib/python3.9/site-packages/omegaconf/omegaconf.py:189\u001b[0m, in \u001b[0;36mOmegaConf.load\u001b[0;34m(file_)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_yaml_loader\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(file_, (\u001b[38;5;28mstr\u001b[39m, pathlib\u001b[38;5;241m.\u001b[39mPath)):\n\u001b[0;32m--> 189\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mabspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    190\u001b[0m         obj \u001b[38;5;241m=\u001b[39m yaml\u001b[38;5;241m.\u001b[39mload(f, Loader\u001b[38;5;241m=\u001b[39mget_yaml_loader())\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(file_, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/harishbabu/projects/PIPNet/configs/fish38_phylogeny.yaml'"
     ]
    }
   ],
   "source": [
    "args_file = open(os.path.join(run_path, 'metadata', 'args.pickle'), 'rb')\n",
    "args = pickle.load(args_file)\n",
    "\n",
    "if args.phylo_config:\n",
    "    phylo_config = OmegaConf.load(args.phylo_config)\n",
    "\n",
    "if args.phylo_config:\n",
    "    # construct the phylo tree\n",
    "    if phylo_config.phyloDistances_string == 'None':\n",
    "        if '031' in run_path: # this run uses a different phylogeny file that had an extra root node which is a mistake\n",
    "            root = construct_phylo_tree('/home/harishbabu/data/phlyogenyCUB/18Species-with-extra-root-node/1_tree-consensus-Hacket-18Species-modified_cub-names_v1.phy')\n",
    "        else:\n",
    "            root = construct_phylo_tree(phylo_config.phylogeny_path)\n",
    "        print('-'*25 + ' No discretization ' + '-'*25)\n",
    "    else:\n",
    "        root = construct_discretized_phylo_tree(phylo_config.phylogeny_path, phylo_config.phyloDistances_string)\n",
    "        print('-'*25 + ' Discretized ' + '-'*25)\n",
    "else:\n",
    "    # construct the tree (original hierarchy as described in the paper)\n",
    "    root = Node(\"root\")\n",
    "    root.add_children(['animal','vehicle','everyday_object','weapon','scuba_diver'])\n",
    "    root.add_children_to('animal',['non_primate','primate'])\n",
    "    root.add_children_to('non_primate',['African_elephant','giant_panda','lion'])\n",
    "    root.add_children_to('primate',['capuchin','gibbon','orangutan'])\n",
    "    root.add_children_to('vehicle',['ambulance','pickup','sports_car'])\n",
    "    root.add_children_to('everyday_object',['laptop','sandal','wine_bottle'])\n",
    "    root.add_children_to('weapon',['assault_rifle','rifle'])\n",
    "    # flat root\n",
    "    # root.add_children(['scuba_diver','African_elephant','giant_panda','lion','capuchin','gibbon','orangutan','ambulance','pickup','sports_car','laptop','sandal','wine_bottle','assault_rifle','rifle'])\n",
    "root.assign_all_descendents()\n",
    "\n",
    "exp_no = int(os.path.basename(run_path)[:3])\n",
    "\n",
    "if exp_no < 77:\n",
    "    if ('num_protos_per_descendant' in args) and (args.num_protos_per_descendant > 0):\n",
    "        for node in root.nodes_with_children():\n",
    "            node.set_num_protos(args.num_protos_per_descendant)\n",
    "if exp_no == 77:\n",
    "    # update num of protos per node based on num_protos_per_descendant\n",
    "    if args.num_features == 0 and args.num_protos_per_descendant == 0:\n",
    "        raise Exception('Either of num_features or num_protos_per_descendant must be greater than zero')\n",
    "    for node in root.nodes_with_children():\n",
    "        node.set_num_protos(num_protos_per_descendant=args.num_protos_per_descendant,\\\n",
    "                                                            min_protos=args.num_features)\n",
    "\n",
    "elif 'num_protos_per_child' in args:\n",
    "    if ('num_protos_per_descendant' in args):\n",
    "        # update num of protos per node based on num_protos_per_descendant\n",
    "        if args.num_features == 0 and args.num_protos_per_descendant == 0 and args.num_protos_per_child == 0:\n",
    "            raise Exception('Either of num_features or num_protos_per_descendant must be greater than zero')\n",
    "        for node in root.nodes_with_children():\n",
    "            node.set_num_protos(num_protos_per_descendant=args.num_protos_per_descendant,\\\n",
    "                                num_protos_per_child=args.num_protos_per_child,\\\n",
    "                                min_protos=args.num_features,\\\n",
    "                                split_protos=('protopool' in args) and (args.protopool == 'n'))\n",
    "else:\n",
    "    if ('num_protos_per_descendant' in args):\n",
    "        # update num of protos per node based on num_protos_per_descendant\n",
    "        if args.num_features == 0 and args.num_protos_per_descendant == 0:\n",
    "            raise Exception('Either of num_features or num_protos_per_descendant must be greater than zero')\n",
    "        for node in root.nodes_with_children():\n",
    "            node.set_num_protos(num_protos_per_descendant=args.num_protos_per_descendant,\\\n",
    "                                num_protos_per_child=0,\\\n",
    "                                min_protos=args.num_features,\\\n",
    "                                split_protos=('protopool' in args) and (args.protopool == 'n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751cc10c-e33b-472c-b904-36e94ab2c189",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7e3ed910",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num classes (k) =  38 ['fis_001_Alosa_chrysochloris', 'fis_002_Carassius_auratus', 'fis_003_Cyprinus_carpio', 'fis_004_Esox_americanus', 'fis_005_Gambusia_affinis'] etc.\n",
      "64 64\n",
      "Number of prototypes:  768\n",
      "----------Prototypes per descendant: 0----------\n",
      "Assigned 20 protos to node root\n",
      "Assigned 20 protos to node 129+024+067\n",
      "Assigned 20 protos to node 089+046\n",
      "Assigned 20 protos to node 129+065\n",
      "Assigned 20 protos to node 024+051\n",
      "Assigned 20 protos to node 067+070\n",
      "Assigned 20 protos to node 089+090\n",
      "Assigned 20 protos to node 046+087\n",
      "Assigned 20 protos to node 129+192\n",
      "Assigned 20 protos to node 065+006\n",
      "Assigned 20 protos to node 024+031\n",
      "Assigned 20 protos to node 051+052\n",
      "Assigned 20 protos to node 067+068\n",
      "Assigned 20 protos to node 129+043\n",
      "Assigned 20 protos to node 192+081\n",
      "Assigned 20 protos to node 065+144\n",
      "Assigned 20 protos to node 006+071\n",
      "Assigned 20 protos to node 024+086\n",
      "Assigned 20 protos to node 031+004\n",
      "Assigned 20 protos to node 051+053\n",
      "Assigned 20 protos to node 067+069\n",
      "Assigned 20 protos to node 129+018\n",
      "Assigned 20 protos to node 043+078\n",
      "Assigned 20 protos to node 192+036\n",
      "Assigned 20 protos to node 081+083\n",
      "Assigned 20 protos to node 065+084\n",
      "Assigned 20 protos to node 144+147\n",
      "Assigned 20 protos to node 006+058\n",
      "Assigned 20 protos to node 071+072\n",
      "Assigned 20 protos to node 024+001\n",
      "Assigned 20 protos to node 031+032\n",
      "Assigned 20 protos to node 051+050\n",
      "Assigned 20 protos to node 129+107\n",
      "Assigned 20 protos to node 043+042\n",
      "Assigned 20 protos to node 078+038\n",
      "Assigned 20 protos to node 192+191\n",
      "Assigned 20 protos to node 036+188\n",
      "Assigned 20 protos to node 081+082\n",
      "Assigned 20 protos to node 065+061\n",
      "Assigned 20 protos to node 084+063\n",
      "Assigned 20 protos to node 144+143\n",
      "Assigned 20 protos to node 006+008\n",
      "Assigned 20 protos to node 024+100\n",
      "Assigned 20 protos to node 001+045\n",
      "Assigned 20 protos to node 031+033\n",
      "Assigned 20 protos to node 129+136\n",
      "Assigned 20 protos to node 107+151\n",
      "Assigned 20 protos to node 043+040\n",
      "Assigned 20 protos to node 078+041\n",
      "Assigned 20 protos to node 192+187\n",
      "Assigned 20 protos to node 191+189\n",
      "Assigned 20 protos to node 081+080\n",
      "Assigned 20 protos to node 082+079\n",
      "Assigned 20 protos to node 065+066\n",
      "Assigned 20 protos to node 061+064\n",
      "Assigned 20 protos to node 144+142\n",
      "Assigned 20 protos to node 006+005\n",
      "Assigned 20 protos to node 008+106\n",
      "Assigned 20 protos to node 024+023\n",
      "Assigned 20 protos to node 100+101\n",
      "Assigned 20 protos to node 001+003\n",
      "Assigned 20 protos to node 129+199\n",
      "Assigned 20 protos to node 136+085\n",
      "Assigned 20 protos to node 107+111\n",
      "Assigned 20 protos to node 151+153\n",
      "Assigned 20 protos to node 043+037\n",
      "Assigned 20 protos to node 040+102\n",
      "Assigned 20 protos to node 078+077\n",
      "Assigned 20 protos to node 192+190\n",
      "Assigned 20 protos to node 065+062\n",
      "Assigned 20 protos to node 144+145\n",
      "Assigned 20 protos to node 006+007\n",
      "Assigned 20 protos to node 024+025\n",
      "Assigned 20 protos to node 001+002\n",
      "Assigned 20 protos to node 129+118\n",
      "Assigned 20 protos to node 199+186\n",
      "Assigned 20 protos to node 136+138\n",
      "Assigned 20 protos to node 107+073\n",
      "Assigned 20 protos to node 111+112\n",
      "Assigned 20 protos to node 151+157\n",
      "Assigned 20 protos to node 153+154\n",
      "Assigned 20 protos to node 043+039\n",
      "Assigned 20 protos to node 065+059\n",
      "Assigned 20 protos to node 144+146\n",
      "Assigned 20 protos to node 129+104\n",
      "Assigned 20 protos to node 199+150\n",
      "Assigned 20 protos to node 186+185\n",
      "Assigned 20 protos to node 136+137\n",
      "Assigned 20 protos to node 107+093\n",
      "Assigned 20 protos to node 073+074\n",
      "Assigned 20 protos to node 151+156\n",
      "Assigned 20 protos to node 157+152\n",
      "Assigned 20 protos to node 153+155\n",
      "Assigned 20 protos to node 065+060\n",
      "Assigned 20 protos to node 144+141\n",
      "Assigned 20 protos to node 129+035\n",
      "Assigned 20 protos to node 199+094\n",
      "Assigned 20 protos to node 150+019\n",
      "Assigned 20 protos to node 107+030\n",
      "Assigned 20 protos to node 129+054\n",
      "Assigned 20 protos to node 035+055\n",
      "Assigned 20 protos to node 199+028\n",
      "Assigned 20 protos to node 150+149\n",
      "Assigned 20 protos to node 107+029\n",
      "Assigned 20 protos to node 129+175\n",
      "Assigned 20 protos to node 054+140\n",
      "Assigned 20 protos to node 035+048\n",
      "Assigned 20 protos to node 199+198\n",
      "Assigned 20 protos to node 150+091\n",
      "Assigned 20 protos to node 107+108\n",
      "Assigned 20 protos to node 129+011\n",
      "Assigned 20 protos to node 175+020\n",
      "Assigned 20 protos to node 054+057\n",
      "Assigned 20 protos to node 140+017\n",
      "Assigned 20 protos to node 035+056\n",
      "Assigned 20 protos to node 048+047\n",
      "Assigned 20 protos to node 199+194\n",
      "Assigned 20 protos to node 129+121\n",
      "Assigned 20 protos to node 011+013\n",
      "Assigned 20 protos to node 175+099\n",
      "Assigned 20 protos to node 054+014\n",
      "Assigned 20 protos to node 140+139\n",
      "Assigned 20 protos to node 035+034\n",
      "Assigned 20 protos to node 199+193\n",
      "Assigned 20 protos to node 129+117\n",
      "Assigned 20 protos to node 011+095\n",
      "Assigned 20 protos to node 013+088\n",
      "Assigned 20 protos to node 175+181\n",
      "Assigned 20 protos to node 054+016\n",
      "Assigned 20 protos to node 199+197\n",
      "Assigned 20 protos to node 193+195\n",
      "Assigned 20 protos to node 129+133\n",
      "Assigned 20 protos to node 117+114\n",
      "Assigned 20 protos to node 011+026\n",
      "Assigned 20 protos to node 095+096\n",
      "Assigned 20 protos to node 013+012\n",
      "Assigned 20 protos to node 175+168+173+183\n",
      "Assigned 20 protos to node 054+015\n",
      "Assigned 20 protos to node 199+196\n",
      "Assigned 20 protos to node 129+021\n",
      "Assigned 20 protos to node 133+130\n",
      "Assigned 20 protos to node 117+115\n",
      "Assigned 20 protos to node 011+049\n",
      "Assigned 20 protos to node 026+010\n",
      "Assigned 20 protos to node 095+098\n",
      "Assigned 20 protos to node 096+097\n",
      "Assigned 20 protos to node 175+162\n",
      "Assigned 20 protos to node 168+177\n",
      "Assigned 20 protos to node 173+161\n",
      "Assigned 20 protos to node 183+159\n",
      "Assigned 20 protos to node 129+128\n",
      "Assigned 20 protos to node 021+148\n",
      "Assigned 20 protos to node 133+076\n",
      "Assigned 20 protos to node 130+120\n",
      "Assigned 20 protos to node 117+116\n",
      "Assigned 20 protos to node 115+119\n",
      "Assigned 20 protos to node 011+009\n",
      "Assigned 20 protos to node 026+027\n",
      "Assigned 20 protos to node 175+167\n",
      "Assigned 20 protos to node 162+180\n",
      "Assigned 20 protos to node 168+200\n",
      "Assigned 20 protos to node 177+178\n",
      "Assigned 20 protos to node 173+179\n",
      "Assigned 20 protos to node 161+166\n",
      "Assigned 20 protos to node 183+184\n",
      "Assigned 20 protos to node 129+127\n",
      "Assigned 20 protos to node 128+131\n",
      "Assigned 20 protos to node 133+132\n",
      "Assigned 20 protos to node 175+169\n",
      "Assigned 20 protos to node 168+170\n",
      "Assigned 20 protos to node 173+172\n",
      "Assigned 20 protos to node 129+123\n",
      "Assigned 20 protos to node 128+124\n",
      "Assigned 20 protos to node 133+122\n",
      "Assigned 20 protos to node 175+165+182\n",
      "Assigned 20 protos to node 129+125\n",
      "Assigned 20 protos to node 123+113\n",
      "Assigned 20 protos to node 128+126\n",
      "Assigned 20 protos to node 175+160\n",
      "Assigned 20 protos to node 165+164+163\n",
      "Assigned 20 protos to node 175+176\n",
      "Assigned 20 protos to node 160+109\n",
      "Assigned 20 protos to node 165+158\n",
      "Assigned 20 protos to node 175+174\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for DataParallel:\n\tMissing key(s) in state_dict: \"module._129+024+067_proto_presence\", \"module._089+046_proto_presence\", \"module._129+065_proto_presence\", \"module._024+051_proto_presence\", \"module._067+070_proto_presence\", \"module._089+090_proto_presence\", \"module._046+087_proto_presence\", \"module._129+192_proto_presence\", \"module._065+006_proto_presence\", \"module._024+031_proto_presence\", \"module._051+052_proto_presence\", \"module._067+068_proto_presence\", \"module._129+043_proto_presence\", \"module._192+081_proto_presence\", \"module._065+144_proto_presence\", \"module._006+071_proto_presence\", \"module._024+086_proto_presence\", \"module._031+004_proto_presence\", \"module._051+053_proto_presence\", \"module._067+069_proto_presence\", \"module._129+018_proto_presence\", \"module._043+078_proto_presence\", \"module._192+036_proto_presence\", \"module._081+083_proto_presence\", \"module._065+084_proto_presence\", \"module._144+147_proto_presence\", \"module._006+058_proto_presence\", \"module._071+072_proto_presence\", \"module._024+001_proto_presence\", \"module._031+032_proto_presence\", \"module._051+050_proto_presence\", \"module._129+107_proto_presence\", \"module._043+042_proto_presence\", \"module._078+038_proto_presence\", \"module._192+191_proto_presence\", \"module._036+188_proto_presence\", \"module._081+082_proto_presence\", \"module._065+061_proto_presence\", \"module._084+063_proto_presence\", \"module._144+143_proto_presence\", \"module._006+008_proto_presence\", \"module._024+100_proto_presence\", \"module._001+045_proto_presence\", \"module._031+033_proto_presence\", \"module._129+136_proto_presence\", \"module._107+151_proto_presence\", \"module._043+040_proto_presence\", \"module._078+041_proto_presence\", \"module._192+187_proto_presence\", \"module._191+189_proto_presence\", \"module._081+080_proto_presence\", \"module._082+079_proto_presence\", \"module._065+066_proto_presence\", \"module._061+064_proto_presence\", \"module._144+142_proto_presence\", \"module._006+005_proto_presence\", \"module._008+106_proto_presence\", \"module._024+023_proto_presence\", \"module._100+101_proto_presence\", \"module._001+003_proto_presence\", \"module._129+199_proto_presence\", \"module._136+085_proto_presence\", \"module._107+111_proto_presence\", \"module._151+153_proto_presence\", \"module._043+037_proto_presence\", \"module._040+102_proto_presence\", \"module._078+077_proto_presence\", \"module._192+190_proto_presence\", \"module._065+062_proto_presence\", \"module._144+145_proto_presence\", \"module._024+025_proto_presence\", \"module._001+002_proto_presence\", \"module._129+118_proto_presence\", \"module._199+186_proto_presence\", \"module._136+138_proto_presence\", \"module._107+073_proto_presence\", \"module._111+112_proto_presence\", \"module._151+157_proto_presence\", \"module._153+154_proto_presence\", \"module._043+039_proto_presence\", \"module._065+059_proto_presence\", \"module._144+146_proto_presence\", \"module._129+104_proto_presence\", \"module._199+150_proto_presence\", \"module._186+185_proto_presence\", \"module._136+137_proto_presence\", \"module._107+093_proto_presence\", \"module._073+074_proto_presence\", \"module._151+156_proto_presence\", \"module._157+152_proto_presence\", \"module._153+155_proto_presence\", \"module._065+060_proto_presence\", \"module._144+141_proto_presence\", \"module._129+035_proto_presence\", \"module._199+094_proto_presence\", \"module._150+019_proto_presence\", \"module._107+030_proto_presence\", \"module._129+054_proto_presence\", \"module._035+055_proto_presence\", \"module._199+028_proto_presence\", \"module._150+149_proto_presence\", \"module._107+029_proto_presence\", \"module._129+175_proto_presence\", \"module._054+140_proto_presence\", \"module._035+048_proto_presence\", \"module._199+198_proto_presence\", \"module._150+091_proto_presence\", \"module._107+108_proto_presence\", \"module._129+011_proto_presence\", \"module._175+020_proto_presence\", \"module._054+057_proto_presence\", \"module._140+017_proto_presence\", \"module._035+056_proto_presence\", \"module._048+047_proto_presence\", \"module._199+194_proto_presence\", \"module._129+121_proto_presence\", \"module._011+013_proto_presence\", \"module._175+099_proto_presence\", \"module._054+014_proto_presence\", \"module._140+139_proto_presence\", \"module._035+034_proto_presence\", \"module._199+193_proto_presence\", \"module._129+117_proto_presence\", \"module._011+095_proto_presence\", \"module._013+088_proto_presence\", \"module._175+181_proto_presence\", \"module._054+016_proto_presence\", \"module._199+197_proto_presence\", \"module._193+195_proto_presence\", \"module._129+133_proto_presence\", \"module._117+114_proto_presence\", \"module._011+026_proto_presence\", \"module._095+096_proto_presence\", \"module._013+012_proto_presence\", \"module._175+168+173+183_proto_presence\", \"module._054+015_proto_presence\", \"module._199+196_proto_presence\", \"module._129+021_proto_presence\", \"module._133+130_proto_presence\", \"module._117+115_proto_presence\", \"module._011+049_proto_presence\", \"module._026+010_proto_presence\", \"module._095+098_proto_presence\", \"module._096+097_proto_presence\", \"module._175+162_proto_presence\", \"module._168+177_proto_presence\", \"module._173+161_proto_presence\", \"module._183+159_proto_presence\", \"module._129+128_proto_presence\", \"module._021+148_proto_presence\", \"module._133+076_proto_presence\", \"module._130+120_proto_presence\", \"module._117+116_proto_presence\", \"module._115+119_proto_presence\", \"module._011+009_proto_presence\", \"module._026+027_proto_presence\", \"module._175+167_proto_presence\", \"module._162+180_proto_presence\", \"module._168+200_proto_presence\", \"module._177+178_proto_presence\", \"module._173+179_proto_presence\", \"module._161+166_proto_presence\", \"module._183+184_proto_presence\", \"module._129+127_proto_presence\", \"module._128+131_proto_presence\", \"module._133+132_proto_presence\", \"module._175+169_proto_presence\", \"module._168+170_proto_presence\", \"module._173+172_proto_presence\", \"module._129+123_proto_presence\", \"module._128+124_proto_presence\", \"module._133+122_proto_presence\", \"module._175+165+182_proto_presence\", \"module._129+125_proto_presence\", \"module._123+113_proto_presence\", \"module._128+126_proto_presence\", \"module._175+160_proto_presence\", \"module._165+164+163_proto_presence\", \"module._175+176_proto_presence\", \"module._160+109_proto_presence\", \"module._165+158_proto_presence\", \"module._175+174_proto_presence\", \"module._129+024+067_add_on.weight\", \"module._089+046_add_on.weight\", \"module._129+065_add_on.weight\", \"module._024+051_add_on.weight\", \"module._067+070_add_on.weight\", \"module._089+090_add_on.weight\", \"module._046+087_add_on.weight\", \"module._129+192_add_on.weight\", \"module._065+006_add_on.weight\", \"module._024+031_add_on.weight\", \"module._051+052_add_on.weight\", \"module._067+068_add_on.weight\", \"module._129+043_add_on.weight\", \"module._192+081_add_on.weight\", \"module._065+144_add_on.weight\", \"module._006+071_add_on.weight\", \"module._024+086_add_on.weight\", \"module._031+004_add_on.weight\", \"module._051+053_add_on.weight\", \"module._067+069_add_on.weight\", \"module._129+018_add_on.weight\", \"module._043+078_add_on.weight\", \"module._192+036_add_on.weight\", \"module._081+083_add_on.weight\", \"module._065+084_add_on.weight\", \"module._144+147_add_on.weight\", \"module._006+058_add_on.weight\", \"module._071+072_add_on.weight\", \"module._024+001_add_on.weight\", \"module._031+032_add_on.weight\", \"module._051+050_add_on.weight\", \"module._129+107_add_on.weight\", \"module._043+042_add_on.weight\", \"module._078+038_add_on.weight\", \"module._192+191_add_on.weight\", \"module._036+188_add_on.weight\", \"module._081+082_add_on.weight\", \"module._065+061_add_on.weight\", \"module._084+063_add_on.weight\", \"module._144+143_add_on.weight\", \"module._006+008_add_on.weight\", \"module._024+100_add_on.weight\", \"module._001+045_add_on.weight\", \"module._031+033_add_on.weight\", \"module._129+136_add_on.weight\", \"module._107+151_add_on.weight\", \"module._043+040_add_on.weight\", \"module._078+041_add_on.weight\", \"module._192+187_add_on.weight\", \"module._191+189_add_on.weight\", \"module._081+080_add_on.weight\", \"module._082+079_add_on.weight\", \"module._065+066_add_on.weight\", \"module._061+064_add_on.weight\", \"module._144+142_add_on.weight\", \"module._006+005_add_on.weight\", \"module._008+106_add_on.weight\", \"module._024+023_add_on.weight\", \"module._100+101_add_on.weight\", \"module._001+003_add_on.weight\", \"module._129+199_add_on.weight\", \"module._136+085_add_on.weight\", \"module._107+111_add_on.weight\", \"module._151+153_add_on.weight\", \"module._043+037_add_on.weight\", \"module._040+102_add_on.weight\", \"module._078+077_add_on.weight\", \"module._192+190_add_on.weight\", \"module._065+062_add_on.weight\", \"module._144+145_add_on.weight\", \"module._024+025_add_on.weight\", \"module._001+002_add_on.weight\", \"module._129+118_add_on.weight\", \"module._199+186_add_on.weight\", \"module._136+138_add_on.weight\", \"module._107+073_add_on.weight\", \"module._111+112_add_on.weight\", \"module._151+157_add_on.weight\", \"module._153+154_add_on.weight\", \"module._043+039_add_on.weight\", \"module._065+059_add_on.weight\", \"module._144+146_add_on.weight\", \"module._129+104_add_on.weight\", \"module._199+150_add_on.weight\", \"module._186+185_add_on.weight\", \"module._136+137_add_on.weight\", \"module._107+093_add_on.weight\", \"module._073+074_add_on.weight\", \"module._151+156_add_on.weight\", \"module._157+152_add_on.weight\", \"module._153+155_add_on.weight\", \"module._065+060_add_on.weight\", \"module._144+141_add_on.weight\", \"module._129+035_add_on.weight\", \"module._199+094_add_on.weight\", \"module._150+019_add_on.weight\", \"module._107+030_add_on.weight\", \"module._129+054_add_on.weight\", \"module._035+055_add_on.weight\", \"module._199+028_add_on.weight\", \"module._150+149_add_on.weight\", \"module._107+029_add_on.weight\", \"module._129+175_add_on.weight\", \"module._054+140_add_on.weight\", \"module._035+048_add_on.weight\", \"module._199+198_add_on.weight\", \"module._150+091_add_on.weight\", \"module._107+108_add_on.weight\", \"module._129+011_add_on.weight\", \"module._175+020_add_on.weight\", \"module._054+057_add_on.weight\", \"module._140+017_add_on.weight\", \"module._035+056_add_on.weight\", \"module._048+047_add_on.weight\", \"module._199+194_add_on.weight\", \"module._129+121_add_on.weight\", \"module._011+013_add_on.weight\", \"module._175+099_add_on.weight\", \"module._054+014_add_on.weight\", \"module._140+139_add_on.weight\", \"module._035+034_add_on.weight\", \"module._199+193_add_on.weight\", \"module._129+117_add_on.weight\", \"module._011+095_add_on.weight\", \"module._013+088_add_on.weight\", \"module._175+181_add_on.weight\", \"module._054+016_add_on.weight\", \"module._199+197_add_on.weight\", \"module._193+195_add_on.weight\", \"module._129+133_add_on.weight\", \"module._117+114_add_on.weight\", \"module._011+026_add_on.weight\", \"module._095+096_add_on.weight\", \"module._013+012_add_on.weight\", \"module._175+168+173+183_add_on.weight\", \"module._054+015_add_on.weight\", \"module._199+196_add_on.weight\", \"module._129+021_add_on.weight\", \"module._133+130_add_on.weight\", \"module._117+115_add_on.weight\", \"module._011+049_add_on.weight\", \"module._026+010_add_on.weight\", \"module._095+098_add_on.weight\", \"module._096+097_add_on.weight\", \"module._175+162_add_on.weight\", \"module._168+177_add_on.weight\", \"module._173+161_add_on.weight\", \"module._183+159_add_on.weight\", \"module._129+128_add_on.weight\", \"module._021+148_add_on.weight\", \"module._133+076_add_on.weight\", \"module._130+120_add_on.weight\", \"module._117+116_add_on.weight\", \"module._115+119_add_on.weight\", \"module._011+009_add_on.weight\", \"module._026+027_add_on.weight\", \"module._175+167_add_on.weight\", \"module._162+180_add_on.weight\", \"module._168+200_add_on.weight\", \"module._177+178_add_on.weight\", \"module._173+179_add_on.weight\", \"module._161+166_add_on.weight\", \"module._183+184_add_on.weight\", \"module._129+127_add_on.weight\", \"module._128+131_add_on.weight\", \"module._133+132_add_on.weight\", \"module._175+169_add_on.weight\", \"module._168+170_add_on.weight\", \"module._173+172_add_on.weight\", \"module._129+123_add_on.weight\", \"module._128+124_add_on.weight\", \"module._133+122_add_on.weight\", \"module._175+165+182_add_on.weight\", \"module._129+125_add_on.weight\", \"module._123+113_add_on.weight\", \"module._128+126_add_on.weight\", \"module._175+160_add_on.weight\", \"module._165+164+163_add_on.weight\", \"module._175+176_add_on.weight\", \"module._160+109_add_on.weight\", \"module._165+158_add_on.weight\", \"module._175+174_add_on.weight\", \"module._129+024+067_classification.weight\", \"module._129+024+067_classification.normalization_multiplier\", \"module._089+046_classification.weight\", \"module._089+046_classification.normalization_multiplier\", \"module._129+065_classification.weight\", \"module._129+065_classification.normalization_multiplier\", \"module._024+051_classification.weight\", \"module._024+051_classification.normalization_multiplier\", \"module._067+070_classification.weight\", \"module._067+070_classification.normalization_multiplier\", \"module._089+090_classification.weight\", \"module._089+090_classification.normalization_multiplier\", \"module._046+087_classification.weight\", \"module._046+087_classification.normalization_multiplier\", \"module._129+192_classification.weight\", \"module._129+192_classification.normalization_multiplier\", \"module._065+006_classification.weight\", \"module._065+006_classification.normalization_multiplier\", \"module._024+031_classification.weight\", \"module._024+031_classification.normalization_multiplier\", \"module._051+052_classification.weight\", \"module._051+052_classification.normalization_multiplier\", \"module._067+068_classification.weight\", \"module._067+068_classification.normalization_multiplier\", \"module._129+043_classification.weight\", \"module._129+043_classification.normalization_multiplier\", \"module._192+081_classification.weight\", \"module._192+081_classification.normalization_multiplier\", \"module._065+144_classification.weight\", \"module._065+144_classification.normalization_multiplier\", \"module._006+071_classification.weight\", \"module._006+071_classification.normalization_multiplier\", \"module._024+086_classification.weight\", \"module._024+086_classification.normalization_multiplier\", \"module._031+004_classification.weight\", \"module._031+004_classification.normalization_multiplier\", \"module._051+053_classification.weight\", \"module._051+053_classification.normalization_multiplier\", \"module._067+069_classification.weight\", \"module._067+069_classification.normalization_multiplier\", \"module._129+018_classification.weight\", \"module._129+018_classification.normalization_multiplier\", \"module._043+078_classification.weight\", \"module._043+078_classification.normalization_multiplier\", \"module._192+036_classification.weight\", \"module._192+036_classification.normalization_multiplier\", \"module._081+083_classification.weight\", \"module._081+083_classification.normalization_multiplier\", \"module._065+084_classification.weight\", \"module._065+084_classification.normalization_multiplier\", \"module._144+147_classification.weight\", \"module._144+147_classification.normalization_multiplier\", \"module._006+058_classification.weight\", \"module._006+058_classification.normalization_multiplier\", \"module._071+072_classification.weight\", \"module._071+072_classification.normalization_multiplier\", \"module._024+001_classification.weight\", \"module._024+001_classification.normalization_multiplier\", \"module._031+032_classification.weight\", \"module._031+032_classification.normalization_multiplier\", \"module._051+050_classification.weight\", \"module._051+050_classification.normalization_multiplier\", \"module._129+107_classification.weight\", \"module._129+107_classification.normalization_multiplier\", \"module._043+042_classification.weight\", \"module._043+042_classification.normalization_multiplier\", \"module._078+038_classification.weight\", \"module._078+038_classification.normalization_multiplier\", \"module._192+191_classification.weight\", \"module._192+191_classification.normalization_multiplier\", \"module._036+188_classification.weight\", \"module._036+188_classification.normalization_multiplier\", \"module._081+082_classification.weight\", \"module._081+082_classification.normalization_multiplier\", \"module._065+061_classification.weight\", \"module._065+061_classification.normalization_multiplier\", \"module._084+063_classification.weight\", \"module._084+063_classification.normalization_multiplier\", \"module._144+143_classification.weight\", \"module._144+143_classification.normalization_multiplier\", \"module._006+008_classification.weight\", \"module._006+008_classification.normalization_multiplier\", \"module._024+100_classification.weight\", \"module._024+100_classification.normalization_multiplier\", \"module._001+045_classification.weight\", \"module._001+045_classification.normalization_multiplier\", \"module._031+033_classification.weight\", \"module._031+033_classification.normalization_multiplier\", \"module._129+136_classification.weight\", \"module._129+136_classification.normalization_multiplier\", \"module._107+151_classification.weight\", \"module._107+151_classification.normalization_multiplier\", \"module._043+040_classification.weight\", \"module._043+040_classification.normalization_multiplier\", \"module._078+041_classification.weight\", \"module._078+041_classification.normalization_multiplier\", \"module._192+187_classification.weight\", \"module._192+187_classification.normalization_multiplier\", \"module._191+189_classification.weight\", \"module._191+189_classification.normalization_multiplier\", \"module._081+080_classification.weight\", \"module._081+080_classification.normalization_multiplier\", \"module._082+079_classification.weight\", \"module._082+079_classification.normalization_multiplier\", \"module._065+066_classification.weight\", \"module._065+066_classification.normalization_multiplier\", \"module._061+064_classification.weight\", \"module._061+064_classification.normalization_multiplier\", \"module._144+142_classification.weight\", \"module._144+142_classification.normalization_multiplier\", \"module._006+005_classification.weight\", \"module._006+005_classification.normalization_multiplier\", \"module._008+106_classification.weight\", \"module._008+106_classification.normalization_multiplier\", \"module._024+023_classification.weight\", \"module._024+023_classification.normalization_multiplier\", \"module._100+101_classification.weight\", \"module._100+101_classification.normalization_multiplier\", \"module._001+003_classification.weight\", \"module._001+003_classification.normalization_multiplier\", \"module._129+199_classification.weight\", \"module._129+199_classification.normalization_multiplier\", \"module._136+085_classification.weight\", \"module._136+085_classification.normalization_multiplier\", \"module._107+111_classification.weight\", \"module._107+111_classification.normalization_multiplier\", \"module._151+153_classification.weight\", \"module._151+153_classification.normalization_multiplier\", \"module._043+037_classification.weight\", \"module._043+037_classification.normalization_multiplier\", \"module._040+102_classification.weight\", \"module._040+102_classification.normalization_multiplier\", \"module._078+077_classification.weight\", \"module._078+077_classification.normalization_multiplier\", \"module._192+190_classification.weight\", \"module._192+190_classification.normalization_multiplier\", \"module._065+062_classification.weight\", \"module._065+062_classification.normalization_multiplier\", \"module._144+145_classification.weight\", \"module._144+145_classification.normalization_multiplier\", \"module._024+025_classification.weight\", \"module._024+025_classification.normalization_multiplier\", \"module._001+002_classification.weight\", \"module._001+002_classification.normalization_multiplier\", \"module._129+118_classification.weight\", \"module._129+118_classification.normalization_multiplier\", \"module._199+186_classification.weight\", \"module._199+186_classification.normalization_multiplier\", \"module._136+138_classification.weight\", \"module._136+138_classification.normalization_multiplier\", \"module._107+073_classification.weight\", \"module._107+073_classification.normalization_multiplier\", \"module._111+112_classification.weight\", \"module._111+112_classification.normalization_multiplier\", \"module._151+157_classification.weight\", \"module._151+157_classification.normalization_multiplier\", \"module._153+154_classification.weight\", \"module._153+154_classification.normalization_multiplier\", \"module._043+039_classification.weight\", \"module._043+039_classification.normalization_multiplier\", \"module._065+059_classification.weight\", \"module._065+059_classification.normalization_multiplier\", \"module._144+146_classification.weight\", \"module._144+146_classification.normalization_multiplier\", \"module._129+104_classification.weight\", \"module._129+104_classification.normalization_multiplier\", \"module._199+150_classification.weight\", \"module._199+150_classification.normalization_multiplier\", \"module._186+185_classification.weight\", \"module._186+185_classification.normalization_multiplier\", \"module._136+137_classification.weight\", \"module._136+137_classification.normalization_multiplier\", \"module._107+093_classification.weight\", \"module._107+093_classification.normalization_multiplier\", \"module._073+074_classification.weight\", \"module._073+074_classification.normalization_multiplier\", \"module._151+156_classification.weight\", \"module._151+156_classification.normalization_multiplier\", \"module._157+152_classification.weight\", \"module._157+152_classification.normalization_multiplier\", \"module._153+155_classification.weight\", \"module._153+155_classification.normalization_multiplier\", \"module._065+060_classification.weight\", \"module._065+060_classification.normalization_multiplier\", \"module._144+141_classification.weight\", \"module._144+141_classification.normalization_multiplier\", \"module._129+035_classification.weight\", \"module._129+035_classification.normalization_multiplier\", \"module._199+094_classification.weight\", \"module._199+094_classification.normalization_multiplier\", \"module._150+019_classification.weight\", \"module._150+019_classification.normalization_multiplier\", \"module._107+030_classification.weight\", \"module._107+030_classification.normalization_multiplier\", \"module._129+054_classification.weight\", \"module._129+054_classification.normalization_multiplier\", \"module._035+055_classification.weight\", \"module._035+055_classification.normalization_multiplier\", \"module._199+028_classification.weight\", \"module._199+028_classification.normalization_multiplier\", \"module._150+149_classification.weight\", \"module._150+149_classification.normalization_multiplier\", \"module._107+029_classification.weight\", \"module._107+029_classification.normalization_multiplier\", \"module._129+175_classification.weight\", \"module._129+175_classification.normalization_multiplier\", \"module._054+140_classification.weight\", \"module._054+140_classification.normalization_multiplier\", \"module._035+048_classification.weight\", \"module._035+048_classification.normalization_multiplier\", \"module._199+198_classification.weight\", \"module._199+198_classification.normalization_multiplier\", \"module._150+091_classification.weight\", \"module._150+091_classification.normalization_multiplier\", \"module._107+108_classification.weight\", \"module._107+108_classification.normalization_multiplier\", \"module._129+011_classification.weight\", \"module._129+011_classification.normalization_multiplier\", \"module._175+020_classification.weight\", \"module._175+020_classification.normalization_multiplier\", \"module._054+057_classification.weight\", \"module._054+057_classification.normalization_multiplier\", \"module._140+017_classification.weight\", \"module._140+017_classification.normalization_multiplier\", \"module._035+056_classification.weight\", \"module._035+056_classification.normalization_multiplier\", \"module._048+047_classification.weight\", \"module._048+047_classification.normalization_multiplier\", \"module._199+194_classification.weight\", \"module._199+194_classification.normalization_multiplier\", \"module._129+121_classification.weight\", \"module._129+121_classification.normalization_multiplier\", \"module._011+013_classification.weight\", \"module._011+013_classification.normalization_multiplier\", \"module._175+099_classification.weight\", \"module._175+099_classification.normalization_multiplier\", \"module._054+014_classification.weight\", \"module._054+014_classification.normalization_multiplier\", \"module._140+139_classification.weight\", \"module._140+139_classification.normalization_multiplier\", \"module._035+034_classification.weight\", \"module._035+034_classification.normalization_multiplier\", \"module._199+193_classification.weight\", \"module._199+193_classification.normalization_multiplier\", \"module._129+117_classification.weight\", \"module._129+117_classification.normalization_multiplier\", \"module._011+095_classification.weight\", \"module._011+095_classification.normalization_multiplier\", \"module._013+088_classification.weight\", \"module._013+088_classification.normalization_multiplier\", \"module._175+181_classification.weight\", \"module._175+181_classification.normalization_multiplier\", \"module._054+016_classification.weight\", \"module._054+016_classification.normalization_multiplier\", \"module._199+197_classification.weight\", \"module._199+197_classification.normalization_multiplier\", \"module._193+195_classification.weight\", \"module._193+195_classification.normalization_multiplier\", \"module._129+133_classification.weight\", \"module._129+133_classification.normalization_multiplier\", \"module._117+114_classification.weight\", \"module._117+114_classification.normalization_multiplier\", \"module._011+026_classification.weight\", \"module._011+026_classification.normalization_multiplier\", \"module._095+096_classification.weight\", \"module._095+096_classification.normalization_multiplier\", \"module._013+012_classification.weight\", \"module._013+012_classification.normalization_multiplier\", \"module._175+168+173+183_classification.weight\", \"module._175+168+173+183_classification.normalization_multiplier\", \"module._054+015_classification.weight\", \"module._054+015_classification.normalization_multiplier\", \"module._199+196_classification.weight\", \"module._199+196_classification.normalization_multiplier\", \"module._129+021_classification.weight\", \"module._129+021_classification.normalization_multiplier\", \"module._133+130_classification.weight\", \"module._133+130_classification.normalization_multiplier\", \"module._117+115_classification.weight\", \"module._117+115_classification.normalization_multiplier\", \"module._011+049_classification.weight\", \"module._011+049_classification.normalization_multiplier\", \"module._026+010_classification.weight\", \"module._026+010_classification.normalization_multiplier\", \"module._095+098_classification.weight\", \"module._095+098_classification.normalization_multiplier\", \"module._096+097_classification.weight\", \"module._096+097_classification.normalization_multiplier\", \"module._175+162_classification.weight\", \"module._175+162_classification.normalization_multiplier\", \"module._168+177_classification.weight\", \"module._168+177_classification.normalization_multiplier\", \"module._173+161_classification.weight\", \"module._173+161_classification.normalization_multiplier\", \"module._183+159_classification.weight\", \"module._183+159_classification.normalization_multiplier\", \"module._129+128_classification.weight\", \"module._129+128_classification.normalization_multiplier\", \"module._021+148_classification.weight\", \"module._021+148_classification.normalization_multiplier\", \"module._133+076_classification.weight\", \"module._133+076_classification.normalization_multiplier\", \"module._130+120_classification.weight\", \"module._130+120_classification.normalization_multiplier\", \"module._117+116_classification.weight\", \"module._117+116_classification.normalization_multiplier\", \"module._115+119_classification.weight\", \"module._115+119_classification.normalization_multiplier\", \"module._011+009_classification.weight\", \"module._011+009_classification.normalization_multiplier\", \"module._026+027_classification.weight\", \"module._026+027_classification.normalization_multiplier\", \"module._175+167_classification.weight\", \"module._175+167_classification.normalization_multiplier\", \"module._162+180_classification.weight\", \"module._162+180_classification.normalization_multiplier\", \"module._168+200_classification.weight\", \"module._168+200_classification.normalization_multiplier\", \"module._177+178_classification.weight\", \"module._177+178_classification.normalization_multiplier\", \"module._173+179_classification.weight\", \"module._173+179_classification.normalization_multiplier\", \"module._161+166_classification.weight\", \"module._161+166_classification.normalization_multiplier\", \"module._183+184_classification.weight\", \"module._183+184_classification.normalization_multiplier\", \"module._129+127_classification.weight\", \"module._129+127_classification.normalization_multiplier\", \"module._128+131_classification.weight\", \"module._128+131_classification.normalization_multiplier\", \"module._133+132_classification.weight\", \"module._133+132_classification.normalization_multiplier\", \"module._175+169_classification.weight\", \"module._175+169_classification.normalization_multiplier\", \"module._168+170_classification.weight\", \"module._168+170_classification.normalization_multiplier\", \"module._173+172_classification.weight\", \"module._173+172_classification.normalization_multiplier\", \"module._129+123_classification.weight\", \"module._129+123_classification.normalization_multiplier\", \"module._128+124_classification.weight\", \"module._128+124_classification.normalization_multiplier\", \"module._133+122_classification.weight\", \"module._133+122_classification.normalization_multiplier\", \"module._175+165+182_classification.weight\", \"module._175+165+182_classification.normalization_multiplier\", \"module._129+125_classification.weight\", \"module._129+125_classification.normalization_multiplier\", \"module._123+113_classification.weight\", \"module._123+113_classification.normalization_multiplier\", \"module._128+126_classification.weight\", \"module._128+126_classification.normalization_multiplier\", \"module._175+160_classification.weight\", \"module._175+160_classification.normalization_multiplier\", \"module._165+164+163_classification.weight\", \"module._165+164+163_classification.normalization_multiplier\", \"module._175+176_classification.weight\", \"module._175+176_classification.normalization_multiplier\", \"module._160+109_classification.weight\", \"module._160+109_classification.normalization_multiplier\", \"module._165+158_classification.weight\", \"module._165+158_classification.normalization_multiplier\", \"module._175+174_classification.weight\", \"module._175+174_classification.normalization_multiplier\". \n\tUnexpected key(s) in state_dict: \"module._021+004_proto_presence\", \"module._021+001_proto_presence\", \"module._004+005_proto_presence\", \"module._021+034_proto_presence\", \"module._005+016_proto_presence\", \"module._021+002_proto_presence\", \"module._034+036+037+035+033_proto_presence\", \"module._005+015_proto_presence\", \"module._016+017_proto_presence\", \"module._021+024+025+031+027+019+020+026+022+032+018_proto_presence\", \"module._002+003_proto_presence\", \"module._015+011_proto_presence\", \"module._021+038_proto_presence\", \"module._015+014+009+010+013+012+008_proto_presence\", \"module._021+023_proto_presence\", \"module._021+029_proto_presence\", \"module._021+028_proto_presence\", \"module._021+030_proto_presence\", \"module._021+004_add_on.weight\", \"module._021+001_add_on.weight\", \"module._004+005_add_on.weight\", \"module._021+034_add_on.weight\", \"module._005+016_add_on.weight\", \"module._021+002_add_on.weight\", \"module._034+036+037+035+033_add_on.weight\", \"module._005+015_add_on.weight\", \"module._016+017_add_on.weight\", \"module._021+024+025+031+027+019+020+026+022+032+018_add_on.weight\", \"module._002+003_add_on.weight\", \"module._015+011_add_on.weight\", \"module._021+038_add_on.weight\", \"module._015+014+009+010+013+012+008_add_on.weight\", \"module._021+023_add_on.weight\", \"module._021+029_add_on.weight\", \"module._021+028_add_on.weight\", \"module._021+030_add_on.weight\", \"module._021+004_classification.weight\", \"module._021+004_classification.normalization_multiplier\", \"module._021+001_classification.weight\", \"module._021+001_classification.normalization_multiplier\", \"module._004+005_classification.weight\", \"module._004+005_classification.normalization_multiplier\", \"module._021+034_classification.weight\", \"module._021+034_classification.normalization_multiplier\", \"module._005+016_classification.weight\", \"module._005+016_classification.normalization_multiplier\", \"module._021+002_classification.weight\", \"module._021+002_classification.normalization_multiplier\", \"module._034+036+037+035+033_classification.weight\", \"module._034+036+037+035+033_classification.normalization_multiplier\", \"module._005+015_classification.weight\", \"module._005+015_classification.normalization_multiplier\", \"module._016+017_classification.weight\", \"module._016+017_classification.normalization_multiplier\", \"module._021+024+025+031+027+019+020+026+022+032+018_classification.weight\", \"module._021+024+025+031+027+019+020+026+022+032+018_classification.normalization_multiplier\", \"module._002+003_classification.weight\", \"module._002+003_classification.normalization_multiplier\", \"module._015+011_classification.weight\", \"module._015+011_classification.normalization_multiplier\", \"module._021+038_classification.weight\", \"module._021+038_classification.normalization_multiplier\", \"module._015+014+009+010+013+012+008_classification.weight\", \"module._015+014+009+010+013+012+008_classification.normalization_multiplier\", \"module._021+023_classification.weight\", \"module._021+023_classification.normalization_multiplier\", \"module._021+029_classification.weight\", \"module._021+029_classification.normalization_multiplier\", \"module._021+028_classification.weight\", \"module._021+028_classification.normalization_multiplier\", \"module._021+030_classification.weight\", \"module._021+030_classification.normalization_multiplier\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 53\u001b[0m\n\u001b[1;32m     51\u001b[0m net \u001b[38;5;241m=\u001b[39m net\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m     52\u001b[0m net \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mDataParallel(net, device_ids \u001b[38;5;241m=\u001b[39m device_ids)    \n\u001b[0;32m---> 53\u001b[0m \u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel_state_dict\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# print(net.eval())\u001b[39;00m\n\u001b[1;32m     55\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mNLLLoss(reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/.conda/envs/hpnet4/lib/python3.9/site-packages/torch/nn/modules/module.py:2041\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   2036\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2037\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2038\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(k) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2040\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2041\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2042\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2043\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for DataParallel:\n\tMissing key(s) in state_dict: \"module._129+024+067_proto_presence\", \"module._089+046_proto_presence\", \"module._129+065_proto_presence\", \"module._024+051_proto_presence\", \"module._067+070_proto_presence\", \"module._089+090_proto_presence\", \"module._046+087_proto_presence\", \"module._129+192_proto_presence\", \"module._065+006_proto_presence\", \"module._024+031_proto_presence\", \"module._051+052_proto_presence\", \"module._067+068_proto_presence\", \"module._129+043_proto_presence\", \"module._192+081_proto_presence\", \"module._065+144_proto_presence\", \"module._006+071_proto_presence\", \"module._024+086_proto_presence\", \"module._031+004_proto_presence\", \"module._051+053_proto_presence\", \"module._067+069_proto_presence\", \"module._129+018_proto_presence\", \"module._043+078_proto_presence\", \"module._192+036_proto_presence\", \"module._081+083_proto_presence\", \"module._065+084_proto_presence\", \"module._144+147_proto_presence\", \"module._006+058_proto_presence\", \"module._071+072_proto_presence\", \"module._024+001_proto_presence\", \"module._031+032_proto_presence\", \"module._051+050_proto_presence\", \"module._129+107_proto_presence\", \"module._043+042_proto_presence\", \"module._078+038_proto_presence\", \"module._192+191_proto_presence\", \"module._036+188_proto_presence\", \"module._081+082_proto_presence\", \"module._065+061_proto_presence\", \"module._084+063_proto_presence\", \"module._144+143_proto_presence\", \"module._006+008_proto_presence\", \"module._024+100_proto_presence\", \"module._001+045_proto_presence\", \"module._031+033_proto_presence\", \"module._129+136_proto_presence\", \"module._107+151_proto_presence\", \"module._043+040_proto_presence\", \"module._078+041_proto_presence\", \"module._192+187_proto_presence\", \"module._191+189_proto_presence\", \"module._081+080_proto_presence\", \"module._082+079_proto_presence\", \"module._065+066_proto_presence\", \"module._061+064_proto_presence\", \"module._144+142_proto_presence\", \"module._006+005_proto_presence\", \"module._008+106_proto_presence\", \"module._024+023_proto_presence\", \"module._100+101_proto_presence\", \"module._001+003_proto_presence\", \"module._129+199_proto_presence\", \"module._136+085_proto_presence\", \"module._107+111_proto_presence\", \"module._151+153_proto_presence\", \"module._043+037_proto_presence\", \"module._040+102_proto_presence\", \"module._078+077_proto_presence\", \"module._192+190_proto_presence\", \"module._065+062_proto_presence\", \"module._144+145_proto_presence\", \"module._024+025_proto_presence\", \"module._001+002_proto_presence\", \"module._129+118_proto_presence\", \"module._199+186_proto_presence\", \"module._136+138_proto_presence\", \"module._107+073_proto_presence\", \"module._111+112_proto_presence\", \"module._151+157_proto_presence\", \"module._153+154_proto_presence\", \"module._043+039_proto_presence\", \"module._065+059_proto_presence\", \"module._144+146_proto_presence\", \"module._129+104_proto_presence\", \"module._199+150_proto_presence\", \"module._186+185_proto_presence\", \"module._136+137_proto_presence\", \"module._107+093_proto_presence\", \"module._073+074_proto_presence\", \"module._151+156_proto_presence\", \"module._157+152_proto_presence\", \"module._153+155_proto_presence\", \"module._065+060_proto_presence\", \"module._144+141_proto_presence\", \"module._129+035_proto_presence\", \"module._199+094_proto_presence\", \"module._150+019_proto_presence\", \"module._107+030_proto_presence\", \"module._129+054_proto_presence\", \"module._035+055_proto_presence\", \"module._199+028_proto_presence\", \"module._150+149_proto_presence\", \"module._107+029_proto_presence\", \"module._129+175_proto_presence\", \"module._054+140_proto_presence\", \"module._035+048_proto_presence\", \"module._199+198_proto_presence\", \"module._150+091_proto_presence\", \"module._107+108_proto_presence\", \"module._129+011_proto_presence\", \"module._175+020_proto_presence\", \"module._054+057_proto_presence\", \"module._140+017_proto_presence\", \"module._035+056_proto_presence\", \"module._048+047_proto_presence\", \"module._199+194_proto_presence\", \"module._129+121_proto_presence\", \"module._011+013_proto_presence\", \"module._175+099_proto_presence\", \"module._054+014_proto_presence\", \"module._140+139_proto_presence\", \"module._035+034_proto_presence\", \"module._199+193_proto_presence\", \"module._129+117_proto_presence\", \"module._011+095_proto_presence\", \"module._013+088_proto_presence\", \"module._175+181_proto_presence\", \"module._054+016_proto_presence\", \"module._199+197_proto_presence\", \"module._193+195_proto_presence\", \"module._129+133_proto_presence\", \"module._117+114_proto_presence\", \"module._011+026_proto_presence\", \"module._095+096_proto_presence\", \"module._013+012_proto_presence\", \"module._175+168+173+183_proto_presence\", \"module._054+015_proto_presence\", \"module._199+196_proto_presence\", \"module._129+021_proto_presence\", \"module._133+130_proto_presence\", \"module._117+115_proto_presence\", \"module._011+049_proto_presence\", \"module._026+010_proto_presence\", \"module._095+098_proto_presence\", \"module._096+097_proto_presence\", \"module._175+162_proto_presence\", \"module._168+177_proto_presence\", \"module._173+161_proto_presence\", \"module._183+159_proto_presence\", \"module._129+128_proto_presence\", \"module._021+148_proto_presence\", \"module._133+076_proto_presence\", \"module._130+120_proto_presence\", \"module._117+116_proto_presence\", \"module._115+119_proto_presence\", \"module._011+009_proto_presence\", \"module._026+027_proto_presence\", \"module._175+167_proto_presence\", \"module._162+180_proto_presence\", \"module._168+200_proto_presence\", \"module._177+178_proto_presence\", \"module._173+179_proto_presence\", \"module._161+166_proto_presence\", \"module._183+184_proto_presence\", \"module._129+127_proto_presence\", \"module._128+131_proto_presence\", \"module._133+132_proto_presence\", \"module._175+169_proto_presence\", \"module._168+170_proto_presence\", \"module._173+172_proto_presence\", \"module._129+123_proto_presence\", \"module._128+124_proto_presence\", \"module._133+122_proto_presence\", \"module._175+165+182_proto_presence\", \"module._129+125_proto_presence\", \"module._123+113_proto_presence\", \"module._128+126_proto_presence\", \"module._175+160_proto_presence\", \"module._165+164+163_proto_presence\", \"module._175+176_proto_presence\", \"module._160+109_proto_presence\", \"module._165+158_proto_presence\", \"module._175+174_proto_presence\", \"module._129+024+067_add_on.weight\", \"module._089+046_add_on.weight\", \"module._129+065_add_on.weight\", \"module._024+051_add_on.weight\", \"module._067+070_add_on.weight\", \"module._089+090_add_on.weight\", \"module._046+087_add_on.weight\", \"module._129+192_add_on.weight\", \"module._065+006_add_on.weight\", \"module._024+031_add_on.weight\", \"module._051+052_add_on.weight\", \"module._067+068_add_on.weight\", \"module._129+043_add_on.weight\", \"module._192+081_add_on.weight\", \"module._065+144_add_on.weight\", \"module._006+071_add_on.weight\", \"module._024+086_add_on.weight\", \"module._031+004_add_on.weight\", \"module._051+053_add_on.weight\", \"module._067+069_add_on.weight\", \"module._129+018_add_on.weight\", \"module._043+078_add_on.weight\", \"module._192+036_add_on.weight\", \"module._081+083_add_on.weight\", \"module._065+084_add_on.weight\", \"module._144+147_add_on.weight\", \"module._006+058_add_on.weight\", \"module._071+072_add_on.weight\", \"module._024+001_add_on.weight\", \"module._031+032_add_on.weight\", \"module._051+050_add_on.weight\", \"module._129+107_add_on.weight\", \"module._043+042_add_on.weight\", \"module._078+038_add_on.weight\", \"module._192+191_add_on.weight\", \"module._036+188_add_on.weight\", \"module._081+082_add_on.weight\", \"module._065+061_add_on.weight\", \"module._084+063_add_on.weight\", \"module._144+143_add_on.weight\", \"module._006+008_add_on.weight\", \"module._024+100_add_on.weight\", \"module._001+045_add_on.weight\", \"module._031+033_add_on.weight\", \"module._129+136_add_on.weight\", \"module._107+151_add_on.weight\", \"module._043+040_add_on.weight\", \"module._078+041_add_on.weight\", \"module._192+187_add_on.weight\", \"module._191+189_add_on.weight\", \"module._081+080_add_on.weight\", \"module._082+079_add_on.weight\", \"module._065+066_add_on.weight\", \"module._061+064_add_on.weight\", \"module._144+142_add_on.weight\", \"module._006+005_add_on.weight\", \"module._008+106_add_on.weight\", \"module._024+023_add_on.weight\", \"module._100+101_add_on.weight\", \"module._001+003_add_on.weight\", \"module._129+199_add_on.weight\", \"module._136+085_add_on.weight\", \"module._107+111_add_on.weight\", \"module._151+153_add_on.weight\", \"module._043+037_add_on.weight\", \"module._040+102_add_on.weight\", \"module._078+077_add_on.weight\", \"module._192+190_add_on.weight\", \"module._065+062_add_on.weight\", \"module._144+145_add_on.weight\", \"module._024+025_add_on.weight\", \"module._001+002_add_on.weight\", \"module._129+118_add_on.weight\", \"module._199+186_add_on.weight\", \"module._136+138_add_on.weight\", \"module._107+073_add_on.weight\", \"module._111+112_add_on.weight\", \"module._151+157_add_on.weight\", \"module._153+154_add_on.weight\", \"module._043+039_add_on.weight\", \"module._065+059_add_on.weight\", \"module._144+146_add_on.weight\", \"module._129+104_add_on.weight\", \"module._199+150_add_on.weight\", \"module._186+185_add_on.weight\", \"module._136+137_add_on.weight\", \"module._107+093_add_on.weight\", \"module._073+074_add_on.weight\", \"module._151+156_add_on.weight\", \"module._157+152_add_on.weight\", \"module._153+155_add_on.weight\", \"module._065+060_add_on.weight\", \"module._144+141_add_on.weight\", \"module._129+035_add_on.weight\", \"module._199+094_add_on.weight\", \"module._150+019_add_on.weight\", \"module._107+030_add_on.weight\", \"module._129+054_add_on.weight\", \"module._035+055_add_on.weight\", \"module._199+028_add_on.weight\", \"module._150+149_add_on.weight\", \"module._107+029_add_on.weight\", \"module._129+175_add_on.weight\", \"module._054+140_add_on.weight\", \"module._035+048_add_on.weight\", \"module._199+198_add_on.weight\", \"module._150+091_add_on.weight\", \"module._107+108_add_on.weight\", \"module._129+011_add_on.weight\", \"module._175+020_add_on.weight\", \"module._054+057_add_on.weight\", \"module._140+017_add_on.weight\", \"module._035+056_add_on.weight\", \"module._048+047_add_on.weight\", \"module._199+194_add_on.weight\", \"module._129+121_add_on.weight\", \"module._011+013_add_on.weight\", \"module._175+099_add_on.weight\", \"module._054+014_add_on.weight\", \"module._140+139_add_on.weight\", \"module._035+034_add_on.weight\", \"module._199+193_add_on.weight\", \"module._129+117_add_on.weight\", \"module._011+095_add_on.weight\", \"module._013+088_add_on.weight\", \"module._175+181_add_on.weight\", \"module._054+016_add_on.weight\", \"module._199+197_add_on.weight\", \"module._193+195_add_on.weight\", \"module._129+133_add_on.weight\", \"module._117+114_add_on.weight\", \"module._011+026_add_on.weight\", \"module._095+096_add_on.weight\", \"module._013+012_add_on.weight\", \"module._175+168+173+183_add_on.weight\", \"module._054+015_add_on.weight\", \"module._199+196_add_on.weight\", \"module._129+021_add_on.weight\", \"module._133+130_add_on.weight\", \"module._117+115_add_on.weight\", \"module._011+049_add_on.weight\", \"module._026+010_add_on.weight\", \"module._095+098_add_on.weight\", \"module._096+097_add_on.weight\", \"module._175+162_add_on.weight\", \"module._168+177_add_on.weight\", \"module._173+161_add_on.weight\", \"module._183+159_add_on.weight\", \"module._129+128_add_on.weight\", \"module._021+148_add_on.weight\", \"module._133+076_add_on.weight\", \"module._130+120_add_on.weight\", \"module._117+116_add_on.weight\", \"module._115+119_add_on.weight\", \"module._011+009_add_on.weight\", \"module._026+027_add_on.weight\", \"module._175+167_add_on.weight\", \"module._162+180_add_on.weight\", \"module._168+200_add_on.weight\", \"module._177+178_add_on.weight\", \"module._173+179_add_on.weight\", \"module._161+166_add_on.weight\", \"module._183+184_add_on.weight\", \"module._129+127_add_on.weight\", \"module._128+131_add_on.weight\", \"module._133+132_add_on.weight\", \"module._175+169_add_on.weight\", \"module._168+170_add_on.weight\", \"module._173+172_add_on.weight\", \"module._129+123_add_on.weight\", \"module._128+124_add_on.weight\", \"module._133+122_add_on.weight\", \"module._175+165+182_add_on.weight\", \"module._129+125_add_on.weight\", \"module._123+113_add_on.weight\", \"module._128+126_add_on.weight\", \"module._175+160_add_on.weight\", \"module._165+164+163_add_on.weight\", \"module._175+176_add_on.weight\", \"module._160+109_add_on.weight\", \"module._165+158_add_on.weight\", \"module._175+174_add_on.weight\", \"module._129+024+067_classification.weight\", \"module._129+024+067_classification.normalization_multiplier\", \"module._089+046_classification.weight\", \"module._089+046_classification.normalization_multiplier\", \"module._129+065_classification.weight\", \"module._129+065_classification.normalization_multiplier\", \"module._024+051_classification.weight\", \"module._024+051_classification.normalization_multiplier\", \"module._067+070_classification.weight\", \"module._067+070_classification.normalization_multiplier\", \"module._089+090_classification.weight\", \"module._089+090_classification.normalization_multiplier\", \"module._046+087_classification.weight\", \"module._046+087_classification.normalization_multiplier\", \"module._129+192_classification.weight\", \"module._129+192_classification.normalization_multiplier\", \"module._065+006_classification.weight\", \"module._065+006_classification.normalization_multiplier\", \"module._024+031_classification.weight\", \"module._024+031_classification.normalization_multiplier\", \"module._051+052_classification.weight\", \"module._051+052_classification.normalization_multiplier\", \"module._067+068_classification.weight\", \"module._067+068_classification.normalization_multiplier\", \"module._129+043_classification.weight\", \"module._129+043_classification.normalization_multiplier\", \"module._192+081_classification.weight\", \"module._192+081_classification.normalization_multiplier\", \"module._065+144_classification.weight\", \"module._065+144_classification.normalization_multiplier\", \"module._006+071_classification.weight\", \"module._006+071_classification.normalization_multiplier\", \"module._024+086_classification.weight\", \"module._024+086_classification.normalization_multiplier\", \"module._031+004_classification.weight\", \"module._031+004_classification.normalization_multiplier\", \"module._051+053_classification.weight\", \"module._051+053_classification.normalization_multiplier\", \"module._067+069_classification.weight\", \"module._067+069_classification.normalization_multiplier\", \"module._129+018_classification.weight\", \"module._129+018_classification.normalization_multiplier\", \"module._043+078_classification.weight\", \"module._043+078_classification.normalization_multiplier\", \"module._192+036_classification.weight\", \"module._192+036_classification.normalization_multiplier\", \"module._081+083_classification.weight\", \"module._081+083_classification.normalization_multiplier\", \"module._065+084_classification.weight\", \"module._065+084_classification.normalization_multiplier\", \"module._144+147_classification.weight\", \"module._144+147_classification.normalization_multiplier\", \"module._006+058_classification.weight\", \"module._006+058_classification.normalization_multiplier\", \"module._071+072_classification.weight\", \"module._071+072_classification.normalization_multiplier\", \"module._024+001_classification.weight\", \"module._024+001_classification.normalization_multiplier\", \"module._031+032_classification.weight\", \"module._031+032_classification.normalization_multiplier\", \"module._051+050_classification.weight\", \"module._051+050_classification.normalization_multiplier\", \"module._129+107_classification.weight\", \"module._129+107_classification.normalization_multiplier\", \"module._043+042_classification.weight\", \"module._043+042_classification.normalization_multiplier\", \"module._078+038_classification.weight\", \"module._078+038_classification.normalization_multiplier\", \"module._192+191_classification.weight\", \"module._192+191_classification.normalization_multiplier\", \"module._036+188_classification.weight\", \"module._036+188_classification.normalization_multiplier\", \"module._081+082_classification.weight\", \"module._081+082_classification.normalization_multiplier\", \"module._065+061_classification.weight\", \"module._065+061_classification.normalization_multiplier\", \"module._084+063_classification.weight\", \"module._084+063_classification.normalization_multiplier\", \"module._144+143_classification.weight\", \"module._144+143_classification.normalization_multiplier\", \"module._006+008_classification.weight\", \"module._006+008_classification.normalization_multiplier\", \"module._024+100_classification.weight\", \"module._024+100_classification.normalization_multiplier\", \"module._001+045_classification.weight\", \"module._001+045_classification.normalization_multiplier\", \"module._031+033_classification.weight\", \"module._031+033_classification.normalization_multiplier\", \"module._129+136_classification.weight\", \"module._129+136_classification.normalization_multiplier\", \"module._107+151_classification.weight\", \"module._107+151_classification.normalization_multiplier\", \"module._043+040_classification.weight\", \"module._043+040_classification.normalization_multiplier\", \"module._078+041_classification.weight\", \"module._078+041_classification.normalization_multiplier\", \"module._192+187_classification.weight\", \"module._192+187_classification.normalization_multiplier\", \"module._191+189_classification.weight\", \"module._191+189_classification.normalization_multiplier\", \"module._081+080_classification.weight\", \"module._081+080_classification.normalization_multiplier\", \"module._082+079_classification.weight\", \"module._082+079_classification.normalization_multiplier\", \"module._065+066_classification.weight\", \"module._065+066_classification.normalization_multiplier\", \"module._061+064_classification.weight\", \"module._061+064_classification.normalization_multiplier\", \"module._144+142_classification.weight\", \"module._144+142_classification.normalization_multiplier\", \"module._006+005_classification.weight\", \"module._006+005_classification.normalization_multiplier\", \"module._008+106_classification.weight\", \"module._008+106_classification.normalization_multiplier\", \"module._024+023_classification.weight\", \"module._024+023_classification.normalization_multiplier\", \"module._100+101_classification.weight\", \"module._100+101_classification.normalization_multiplier\", \"module._001+003_classification.weight\", \"module._001+003_classification.normalization_multiplier\", \"module._129+199_classification.weight\", \"module._129+199_classification.normalization_multiplier\", \"module._136+085_classification.weight\", \"module._136+085_classification.normalization_multiplier\", \"module._107+111_classification.weight\", \"module._107+111_classification.normalization_multiplier\", \"module._151+153_classification.weight\", \"module._151+153_classification.normalization_multiplier\", \"module._043+037_classification.weight\", \"module._043+037_classification.normalization_multiplier\", \"module._040+102_classification.weight\", \"module._040+102_classification.normalization_multiplier\", \"module._078+077_classification.weight\", \"module._078+077_classification.normalization_multiplier\", \"module._192+190_classification.weight\", \"module._192+190_classification.normalization_multiplier\", \"module._065+062_classification.weight\", \"module._065+062_classification.normalization_multiplier\", \"module._144+145_classification.weight\", \"module._144+145_classification.normalization_multiplier\", \"module._024+025_classification.weight\", \"module._024+025_classification.normalization_multiplier\", \"module._001+002_classification.weight\", \"module._001+002_classification.normalization_multiplier\", \"module._129+118_classification.weight\", \"module._129+118_classification.normalization_multiplier\", \"module._199+186_classification.weight\", \"module._199+186_classification.normalization_multiplier\", \"module._136+138_classification.weight\", \"module._136+138_classification.normalization_multiplier\", \"module._107+073_classification.weight\", \"module._107+073_classification.normalization_multiplier\", \"module._111+112_classification.weight\", \"module._111+112_classification.normalization_multiplier\", \"module._151+157_classification.weight\", \"module._151+157_classification.normalization_multiplier\", \"module._153+154_classification.weight\", \"module._153+154_classification.normalization_multiplier\", \"module._043+039_classification.weight\", \"module._043+039_classification.normalization_multiplier\", \"module._065+059_classification.weight\", \"module._065+059_classification.normalization_multiplier\", \"module._144+146_classification.weight\", \"module._144+146_classification.normalization_multiplier\", \"module._129+104_classification.weight\", \"module._129+104_classification.normalization_multiplier\", \"module._199+150_classification.weight\", \"module._199+150_classification.normalization_multiplier\", \"module._186+185_classification.weight\", \"module._186+185_classification.normalization_multiplier\", \"module._136+137_classification.weight\", \"module._136+137_classification.normalization_multiplier\", \"module._107+093_classification.weight\", \"module._107+093_classification.normalization_multiplier\", \"module._073+074_classification.weight\", \"module._073+074_classification.normalization_multiplier\", \"module._151+156_classification.weight\", \"module._151+156_classification.normalization_multiplier\", \"module._157+152_classification.weight\", \"module._157+152_classification.normalization_multiplier\", \"module._153+155_classification.weight\", \"module._153+155_classification.normalization_multiplier\", \"module._065+060_classification.weight\", \"module._065+060_classification.normalization_multiplier\", \"module._144+141_classification.weight\", \"module._144+141_classification.normalization_multiplier\", \"module._129+035_classification.weight\", \"module._129+035_classification.normalization_multiplier\", \"module._199+094_classification.weight\", \"module._199+094_classification.normalization_multiplier\", \"module._150+019_classification.weight\", \"module._150+019_classification.normalization_multiplier\", \"module._107+030_classification.weight\", \"module._107+030_classification.normalization_multiplier\", \"module._129+054_classification.weight\", \"module._129+054_classification.normalization_multiplier\", \"module._035+055_classification.weight\", \"module._035+055_classification.normalization_multiplier\", \"module._199+028_classification.weight\", \"module._199+028_classification.normalization_multiplier\", \"module._150+149_classification.weight\", \"module._150+149_classification.normalization_multiplier\", \"module._107+029_classification.weight\", \"module._107+029_classification.normalization_multiplier\", \"module._129+175_classification.weight\", \"module._129+175_classification.normalization_multiplier\", \"module._054+140_classification.weight\", \"module._054+140_classification.normalization_multiplier\", \"module._035+048_classification.weight\", \"module._035+048_classification.normalization_multiplier\", \"module._199+198_classification.weight\", \"module._199+198_classification.normalization_multiplier\", \"module._150+091_classification.weight\", \"module._150+091_classification.normalization_multiplier\", \"module._107+108_classification.weight\", \"module._107+108_classification.normalization_multiplier\", \"module._129+011_classification.weight\", \"module._129+011_classification.normalization_multiplier\", \"module._175+020_classification.weight\", \"module._175+020_classification.normalization_multiplier\", \"module._054+057_classification.weight\", \"module._054+057_classification.normalization_multiplier\", \"module._140+017_classification.weight\", \"module._140+017_classification.normalization_multiplier\", \"module._035+056_classification.weight\", \"module._035+056_classification.normalization_multiplier\", \"module._048+047_classification.weight\", \"module._048+047_classification.normalization_multiplier\", \"module._199+194_classification.weight\", \"module._199+194_classification.normalization_multiplier\", \"module._129+121_classification.weight\", \"module._129+121_classification.normalization_multiplier\", \"module._011+013_classification.weight\", \"module._011+013_classification.normalization_multiplier\", \"module._175+099_classification.weight\", \"module._175+099_classification.normalization_multiplier\", \"module._054+014_classification.weight\", \"module._054+014_classification.normalization_multiplier\", \"module._140+139_classification.weight\", \"module._140+139_classification.normalization_multiplier\", \"module._035+034_classification.weight\", \"module._035+034_classification.normalization_multiplier\", \"module._199+193_classification.weight\", \"module._199+193_classification.normalization_multiplier\", \"module._129+117_classification.weight\", \"module._129+117_classification.normalization_multiplier\", \"module._011+095_classification.weight\", \"module._011+095_classification.normalization_multiplier\", \"module._013+088_classification.weight\", \"module._013+088_classification.normalization_multiplier\", \"module._175+181_classification.weight\", \"module._175+181_classification.normalization_multiplier\", \"module._054+016_classification.weight\", \"module._054+016_classification.normalization_multiplier\", \"module._199+197_classification.weight\", \"module._199+197_classification.normalization_multiplier\", \"module._193+195_classification.weight\", \"module._193+195_classification.normalization_multiplier\", \"module._129+133_classification.weight\", \"module._129+133_classification.normalization_multiplier\", \"module._117+114_classification.weight\", \"module._117+114_classification.normalization_multiplier\", \"module._011+026_classification.weight\", \"module._011+026_classification.normalization_multiplier\", \"module._095+096_classification.weight\", \"module._095+096_classification.normalization_multiplier\", \"module._013+012_classification.weight\", \"module._013+012_classification.normalization_multiplier\", \"module._175+168+173+183_classification.weight\", \"module._175+168+173+183_classification.normalization_multiplier\", \"module._054+015_classification.weight\", \"module._054+015_classification.normalization_multiplier\", \"module._199+196_classification.weight\", \"module._199+196_classification.normalization_multiplier\", \"module._129+021_classification.weight\", \"module._129+021_classification.normalization_multiplier\", \"module._133+130_classification.weight\", \"module._133+130_classification.normalization_multiplier\", \"module._117+115_classification.weight\", \"module._117+115_classification.normalization_multiplier\", \"module._011+049_classification.weight\", \"module._011+049_classification.normalization_multiplier\", \"module._026+010_classification.weight\", \"module._026+010_classification.normalization_multiplier\", \"module._095+098_classification.weight\", \"module._095+098_classification.normalization_multiplier\", \"module._096+097_classification.weight\", \"module._096+097_classification.normalization_multiplier\", \"module._175+162_classification.weight\", \"module._175+162_classification.normalization_multiplier\", \"module._168+177_classification.weight\", \"module._168+177_classification.normalization_multiplier\", \"module._173+161_classification.weight\", \"module._173+161_classification.normalization_multiplier\", \"module._183+159_classification.weight\", \"module._183+159_classification.normalization_multiplier\", \"module._129+128_classification.weight\", \"module._129+128_classification.normalization_multiplier\", \"module._021+148_classification.weight\", \"module._021+148_classification.normalization_multiplier\", \"module._133+076_classification.weight\", \"module._133+076_classification.normalization_multiplier\", \"module._130+120_classification.weight\", \"module._130+120_classification.normalization_multiplier\", \"module._117+116_classification.weight\", \"module._117+116_classification.normalization_multiplier\", \"module._115+119_classification.weight\", \"module._115+119_classification.normalization_multiplier\", \"module._011+009_classification.weight\", \"module._011+009_classification.normalization_multiplier\", \"module._026+027_classification.weight\", \"module._026+027_classification.normalization_multiplier\", \"module._175+167_classification.weight\", \"module._175+167_classification.normalization_multiplier\", \"module._162+180_classification.weight\", \"module._162+180_classification.normalization_multiplier\", \"module._168+200_classification.weight\", \"module._168+200_classification.normalization_multiplier\", \"module._177+178_classification.weight\", \"module._177+178_classification.normalization_multiplier\", \"module._173+179_classification.weight\", \"module._173+179_classification.normalization_multiplier\", \"module._161+166_classification.weight\", \"module._161+166_classification.normalization_multiplier\", \"module._183+184_classification.weight\", \"module._183+184_classification.normalization_multiplier\", \"module._129+127_classification.weight\", \"module._129+127_classification.normalization_multiplier\", \"module._128+131_classification.weight\", \"module._128+131_classification.normalization_multiplier\", \"module._133+132_classification.weight\", \"module._133+132_classification.normalization_multiplier\", \"module._175+169_classification.weight\", \"module._175+169_classification.normalization_multiplier\", \"module._168+170_classification.weight\", \"module._168+170_classification.normalization_multiplier\", \"module._173+172_classification.weight\", \"module._173+172_classification.normalization_multiplier\", \"module._129+123_classification.weight\", \"module._129+123_classification.normalization_multiplier\", \"module._128+124_classification.weight\", \"module._128+124_classification.normalization_multiplier\", \"module._133+122_classification.weight\", \"module._133+122_classification.normalization_multiplier\", \"module._175+165+182_classification.weight\", \"module._175+165+182_classification.normalization_multiplier\", \"module._129+125_classification.weight\", \"module._129+125_classification.normalization_multiplier\", \"module._123+113_classification.weight\", \"module._123+113_classification.normalization_multiplier\", \"module._128+126_classification.weight\", \"module._128+126_classification.normalization_multiplier\", \"module._175+160_classification.weight\", \"module._175+160_classification.normalization_multiplier\", \"module._165+164+163_classification.weight\", \"module._165+164+163_classification.normalization_multiplier\", \"module._175+176_classification.weight\", \"module._175+176_classification.normalization_multiplier\", \"module._160+109_classification.weight\", \"module._160+109_classification.normalization_multiplier\", \"module._165+158_classification.weight\", \"module._165+158_classification.normalization_multiplier\", \"module._175+174_classification.weight\", \"module._175+174_classification.normalization_multiplier\". \n\tUnexpected key(s) in state_dict: \"module._021+004_proto_presence\", \"module._021+001_proto_presence\", \"module._004+005_proto_presence\", \"module._021+034_proto_presence\", \"module._005+016_proto_presence\", \"module._021+002_proto_presence\", \"module._034+036+037+035+033_proto_presence\", \"module._005+015_proto_presence\", \"module._016+017_proto_presence\", \"module._021+024+025+031+027+019+020+026+022+032+018_proto_presence\", \"module._002+003_proto_presence\", \"module._015+011_proto_presence\", \"module._021+038_proto_presence\", \"module._015+014+009+010+013+012+008_proto_presence\", \"module._021+023_proto_presence\", \"module._021+029_proto_presence\", \"module._021+028_proto_presence\", \"module._021+030_proto_presence\", \"module._021+004_add_on.weight\", \"module._021+001_add_on.weight\", \"module._004+005_add_on.weight\", \"module._021+034_add_on.weight\", \"module._005+016_add_on.weight\", \"module._021+002_add_on.weight\", \"module._034+036+037+035+033_add_on.weight\", \"module._005+015_add_on.weight\", \"module._016+017_add_on.weight\", \"module._021+024+025+031+027+019+020+026+022+032+018_add_on.weight\", \"module._002+003_add_on.weight\", \"module._015+011_add_on.weight\", \"module._021+038_add_on.weight\", \"module._015+014+009+010+013+012+008_add_on.weight\", \"module._021+023_add_on.weight\", \"module._021+029_add_on.weight\", \"module._021+028_add_on.weight\", \"module._021+030_add_on.weight\", \"module._021+004_classification.weight\", \"module._021+004_classification.normalization_multiplier\", \"module._021+001_classification.weight\", \"module._021+001_classification.normalization_multiplier\", \"module._004+005_classification.weight\", \"module._004+005_classification.normalization_multiplier\", \"module._021+034_classification.weight\", \"module._021+034_classification.normalization_multiplier\", \"module._005+016_classification.weight\", \"module._005+016_classification.normalization_multiplier\", \"module._021+002_classification.weight\", \"module._021+002_classification.normalization_multiplier\", \"module._034+036+037+035+033_classification.weight\", \"module._034+036+037+035+033_classification.normalization_multiplier\", \"module._005+015_classification.weight\", \"module._005+015_classification.normalization_multiplier\", \"module._016+017_classification.weight\", \"module._016+017_classification.normalization_multiplier\", \"module._021+024+025+031+027+019+020+026+022+032+018_classification.weight\", \"module._021+024+025+031+027+019+020+026+022+032+018_classification.normalization_multiplier\", \"module._002+003_classification.weight\", \"module._002+003_classification.normalization_multiplier\", \"module._015+011_classification.weight\", \"module._015+011_classification.normalization_multiplier\", \"module._021+038_classification.weight\", \"module._021+038_classification.normalization_multiplier\", \"module._015+014+009+010+013+012+008_classification.weight\", \"module._015+014+009+010+013+012+008_classification.normalization_multiplier\", \"module._021+023_classification.weight\", \"module._021+023_classification.normalization_multiplier\", \"module._021+029_classification.weight\", \"module._021+029_classification.normalization_multiplier\", \"module._021+028_classification.weight\", \"module._021+028_classification.normalization_multiplier\", \"module._021+030_classification.weight\", \"module._021+030_classification.normalization_multiplier\". "
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    device_ids = [torch.cuda.current_device()]\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    device_ids = []\n",
    "\n",
    "# args_file = open(os.path.join(run_path, 'metadata', 'args.pickle'), 'rb')\n",
    "# args = pickle.load(args_file)\n",
    "\n",
    "# ckpt_file_name = 'net_overspecific_pruned_replaced_thresh=0.5_last'\n",
    "# ckpt_file_name = 'net_trained_last'\n",
    "# ckpt_file_name = 'net_trained_10'\n",
    "# ckpt_file_name = 'net_pretrained'\n",
    "epoch = ckpt_file_name.split('_')[-1]\n",
    "\n",
    "ckpt_path = os.path.join(run_path, 'checkpoints', ckpt_file_name)\n",
    "checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "\n",
    "if ckpt_file_name != 'net_trained_last':\n",
    "    print('\\n', (10*'-')+'WARNING: Not using the final trained model'+(10*'-'), '\\n')\n",
    "\n",
    "# Obtain the dataset and dataloaders\n",
    "temp = args.leave_out_classes\n",
    "args.leave_out_classes = '' # NOTE: because here we need to load the entire dataloader, and filter out the classes later\n",
    "trainloader, trainloader_pretraining, trainloader_normal, trainloader_normal_augment, projectloader, testloader, test_projectloader, classes = get_dataloaders(args, device)\n",
    "args.leave_out_classes = temp\n",
    "\n",
    "print(args.batch_size, trainloader.batch_size)\n",
    "\n",
    "if len(classes)<=20:\n",
    "    if args.validation_size == 0.:\n",
    "        print(\"Classes: \", testloader.dataset.class_to_idx, flush=True)\n",
    "    else:\n",
    "        print(\"Classes: \", str(classes), flush=True)\n",
    "\n",
    "# Create a convolutional network based on arguments and add 1x1 conv layer\n",
    "feature_net, add_on_layers, pool_layer, classification_layers, num_prototypes = get_network(len(classes), args, root=root)\n",
    "   \n",
    "# Create a PIP-Net\n",
    "net = PIPNet(num_classes=len(classes),\n",
    "                    num_prototypes=num_prototypes,\n",
    "                    feature_net = feature_net,\n",
    "                    args = args,\n",
    "                    add_on_layers = add_on_layers,\n",
    "                    pool_layer = pool_layer,\n",
    "                    classification_layers = classification_layers,\n",
    "                    num_parent_nodes = len(root.nodes_with_children()),\n",
    "                    root = root\n",
    "                    )\n",
    "net = net.to(device=device)\n",
    "net = nn.DataParallel(net, device_ids = device_ids)    \n",
    "net.load_state_dict(checkpoint['model_state_dict'],strict=True)\n",
    "# print(net.eval())\n",
    "criterion = nn.NLLLoss(reduction='mean').to(device)\n",
    "\n",
    "# Forward one batch through the backbone to get the latent output size\n",
    "with torch.no_grad():\n",
    "    xs1, _, _ = next(iter(trainloader))\n",
    "    xs1 = xs1.to(device)\n",
    "    _, proto_features, _, _ = net(xs1)\n",
    "    wshape = proto_features['root'].shape[-1]\n",
    "    args.wshape = wshape #needed for calculating image patch size\n",
    "    print(\"Output shape: \", proto_features['root'].shape, flush=True)\n",
    "    \n",
    "print(args.wshape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9b5b65-35ae-441d-939f-76897b2dad35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6339634b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from util.log import Log\n",
    "os.environ['WANDB_DISABLED'] = 'true'\n",
    "wandb_run = wandb.init(project=\"pipnet\", name=os.path.basename(args.log_dir), config=vars(args), reinit=False)\n",
    "log = Log(args.log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "47e8df17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chosen network is convnext\n",
      "-------------------------Not using OOD data-------------------------\n"
     ]
    }
   ],
   "source": [
    "optimizer_net, optimizer_classifier, params_to_freeze, params_to_train, params_backbone = get_optimizer_nn(net, args)            \n",
    "scheduler_net = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_net, T_max=len(trainloader)*args.epochs, eta_min=args.lr_net/100.)\n",
    "if args.epochs<=30:\n",
    "    scheduler_classifier = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer_classifier, T_0=5, eta_min=0.001, T_mult=1, verbose=False)\n",
    "else:\n",
    "    scheduler_classifier = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer_classifier, T_0=10, eta_min=0.001, T_mult=1, verbose=False)\n",
    "\n",
    "    \n",
    "if args.OOD_dataset:\n",
    "    trainloader_OOD, trainloader_pretraining_OOD, trainloader_normal_OOD, trainloader_normal_augment_OOD, projectloader_OOD, testloader_OOD, test_projectloader_OOD, _ = get_dataloaders(args, device, OOD=True)\n",
    "    print('-'*25 + 'Using OOD data' + '-'*25)\n",
    "else:\n",
    "    trainloader_OOD = trainloader_pretraining_OOD = trainloader_normal_OOD = trainloader_normal_augment_OOD = projectloader_OOD = testloader_OOD = test_projectloader_OOD = None\n",
    "    print('-'*25 + 'Not using OOD data' + '-'*25)\n",
    "    \n",
    "if ('focal_loss' in args) and (args.focal_loss == 'y'):\n",
    "    from util.custom_losses import WeightedCrossEntropyLoss, WeightedNLLLoss, FocalLossWrapper\n",
    "    criterion = FocalLossWrapper(device=device, alpha=1, gamma=args.focal_loss_gamma, reduction='mean').to(device)\n",
    "else:\n",
    "    from util.custom_losses import WeightedCrossEntropyLoss, WeightedNLLLoss, FocalLossWrapper\n",
    "    criterion = WeightedNLLLoss(device=device).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb43785a-cf12-4460-bae5-efe9cf9d332d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b84a48d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_heatmap(latent_activation, input_image):\n",
    "    image_a = latent_activation.cpu().numpy()\n",
    "    image_a = (image_a - image_a.min()) / (image_a.max() - image_a.min())\n",
    "\n",
    "    input_image = (input_image - input_image.min()) / (input_image.max() - input_image.min())\n",
    "    image_b = input_image.permute(1, 2, 0).cpu().numpy()\n",
    "    \n",
    "    reshaped_image_a = np.array(Image.fromarray((image_a[0] * 255).astype('uint8')).resize((input_image.shape[-1], input_image.shape[-1])))\n",
    "    normalized_heatmap = (reshaped_image_a - np.min(reshaped_image_a)) / (np.max(reshaped_image_a) - np.min(reshaped_image_a))\n",
    "    \n",
    "    heatmap_colormap = plt.get_cmap('jet')\n",
    "    heatmap_colored = heatmap_colormap(normalized_heatmap)\n",
    "    \n",
    "    heatmap_colored_uint8 = (heatmap_colored[:, :, :3] * 255).astype(np.uint8)\n",
    "    image_a_heatmap_pillow = Image.fromarray(heatmap_colored_uint8)\n",
    "    image_b_pillow = Image.fromarray((image_b * 255).astype('uint8'))\n",
    "    \n",
    "    result_image = Image.blend(image_b_pillow, image_a_heatmap_pillow, alpha=0.3)\n",
    "    \n",
    "    return np.array(result_image)\n",
    "\n",
    "\n",
    "def get_heatmap_uninterpolated(latent_activation, input_image):\n",
    "    image_a = latent_activation.cpu().numpy()\n",
    "    image_a = (image_a - image_a.min()) / (image_a.max() - image_a.min())\n",
    "\n",
    "    input_image = (input_image - input_image.min()) / (input_image.max() - input_image.min())\n",
    "    image_b = input_image.permute(1, 2, 0).cpu().numpy()\n",
    "    \n",
    "    reshaped_image_a = np.array(Image.fromarray((image_a[0] * 255).astype('uint8')).resize((input_image.shape[-1], input_image.shape[-1]), \\\n",
    "                                                                                          resample=Image.NEAREST ))\n",
    "    normalized_heatmap = (reshaped_image_a - np.min(reshaped_image_a)) / (np.max(reshaped_image_a) - np.min(reshaped_image_a))\n",
    "    \n",
    "    heatmap_colormap = plt.get_cmap('jet')\n",
    "    heatmap_colored = heatmap_colormap(normalized_heatmap)\n",
    "    \n",
    "    heatmap_colored_uint8 = (heatmap_colored[:, :, :3] * 255).astype(np.uint8)\n",
    "    image_a_heatmap_pillow = Image.fromarray(heatmap_colored_uint8)\n",
    "    image_b_pillow = Image.fromarray((image_b * 255).astype('uint8'))\n",
    "    \n",
    "    result_image = Image.blend(image_b_pillow, image_a_heatmap_pillow, alpha=0.3)\n",
    "    \n",
    "    return np.array(result_image)\n",
    "\n",
    "def get_bb_gaussian_threshold(latent_activation, sigma=1.0, percentile=97, extend_h=0, extend_w=0):\n",
    "    # latent_activation -> []\n",
    "    upscaled_similarity = get_upscaled_activation_uninterpolated(latent_activation, \\\n",
    "                                                                 image_size=(args.image_size, args.image_size))\n",
    "    upscaled_similarity = minmaxscale(upscaled_similarity)\n",
    "    upscaled_similarity = gaussian(upscaled_similarity, sigma=sigma)\n",
    "    upscaled_similarity = threshold_local(upscaled_similarity, block_size=15, method='mean')\n",
    "    h_min, h_max, w_min, w_max = find_top_percentile_bbox(upscaled_similarity ,percentile=97)\n",
    "    h_min = max(0, h_min-extend_h)\n",
    "    h_max = min(upscaled_similarity.shape[0], h_max+extend_h)\n",
    "    w_min = max(0, w_min-extend_w)\n",
    "    w_max = min(upscaled_similarity.shape[1], w_max+extend_w)\n",
    "    return h_min, h_max, w_min, w_max\n",
    "\n",
    "\n",
    "def minmaxscale(tensor):\n",
    "    return (tensor - tensor.min()) / (tensor.max() - tensor.min())\n",
    "\n",
    "from torch.utils.data import DataLoader, SequentialSampler\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def unshuffle_dataloader(dataloader):\n",
    "    if type(dataloader.dataset) == ImageFolder:\n",
    "        dataset = dataloader.dataset\n",
    "    else:\n",
    "        dataset = dataloader.dataset.dataset.dataset\n",
    "    new_dataloader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=dataloader.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=dataloader.num_workers,\n",
    "        pin_memory=dataloader.pin_memory,\n",
    "        drop_last=dataloader.drop_last,\n",
    "        timeout=dataloader.timeout,\n",
    "        worker_init_fn=dataloader.worker_init_fn,\n",
    "        multiprocessing_context=dataloader.multiprocessing_context,\n",
    "        generator=dataloader.generator,\n",
    "        prefetch_factor=dataloader.prefetch_factor,\n",
    "        persistent_workers=dataloader.persistent_workers\n",
    "    )\n",
    "    return new_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d7da75-8319-4629-8865-668f55c0a8f0",
   "metadata": {},
   "source": [
    "# Fine accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0c293507-c348-4fec-9669-938143926a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'cl_weight' not in args:\n",
    "    args.cl_weight = 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1414fe0c-accf-4289-9b2f-9dac08326ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Epoch0:   0% 0/21 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'item'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# testloader\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m test_info, log_dict \u001b[38;5;241m=\u001b[39m \u001b[43mtest_pipnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtestloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_net\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_classifier\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mscheduler_net\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler_classifier\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpretrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinetune\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mtest_loader_OOD\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtestloader_OOD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_orth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel_orth\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43my\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mtanh_desc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtanh_desc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43my\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malign\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malign\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43my\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muni\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muni\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43my\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malign_pf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malign_pf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43my\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mminmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mminmaximize\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43my\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwandb_run\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwandb_run\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpretrain_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepochs_pretrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m                                          \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapply_overspecificity_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath_prob_softmax_tau\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/PIPNet/pipnet/train.py:632\u001b[0m, in \u001b[0;36mtest_pipnet\u001b[0;34m(net, test_loader, optimizer_net, optimizer_classifier, scheduler_net, scheduler_classifier, criterion, epoch, nr_epochs, device, pretrain, finetune, progress_prefix, wandb_logging, test_loader_OOD, kernel_orth, tanh_desc, align, uni, align_pf, tanh, minmaximize, cluster_desc, sep_desc, subspace_sep, byol, byol_tau_base, step_info, wandb_run, pretrain_epochs, log, args, apply_overspecificity_mask, leave_out_classes, path_prob_softmax_tau)\u001b[0m\n\u001b[1;32m    626\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    627\u001b[0m     features, proto_features, pooled, out \u001b[38;5;241m=\u001b[39m net(xs, apply_overspecificity_mask\u001b[38;5;241m=\u001b[39mapply_overspecificity_mask)\n\u001b[1;32m    629\u001b[0m loss, class_loss_dict, a_loss, tanh_loss_dict, minmaximize_loss_dict, OOD_loss_dict, kernel_orth_loss_dict, \\\n\u001b[1;32m    630\u001b[0m uni_loss, avg_class_loss, avg_a_loss_pf, avg_tanh_loss, avg_minmaximize_loss, avg_OOD_loss, avg_kernel_orth_loss, \\\n\u001b[1;32m    631\u001b[0m byol_loss, avg_cluster_desc_loss, avg_sep_desc_loss, avg_tanh_desc_loss, avg_subspace_sep_loss, avg_conc_log_ip_loss, acc \u001b[38;5;241m=\u001b[39m \\\n\u001b[0;32m--> 632\u001b[0m     \u001b[43mcalculate_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madditional_network_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproto_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpooled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malign_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malign_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malign_pf_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malign_pf_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mt_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mt_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmm_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmm_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munif_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munif_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcl_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcl_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mOOD_loss_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mOOD_loss_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morth_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morth_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mcluster_desc_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcluster_desc_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msep_desc_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msep_desc_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubspace_sep_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubspace_sep_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbyol_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbyol_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mnet_normalization_multiplier\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_multiplier\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpretrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpretrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinetune\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfinetune\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m    636\u001b[0m \u001b[43m                \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEPS\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel2name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel2name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode_accuracy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnode_accuracy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m    637\u001b[0m \u001b[43m                \u001b[49m\u001b[43mOOD_loss_required\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mOOD_loss_required\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_orth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkernel_orth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtanh_desc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtanh_desc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malign\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malign\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muni\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muni\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malign_pf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malign_pf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtanh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtanh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mminmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mminmaximize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m    638\u001b[0m \u001b[43m                \u001b[49m\u001b[43mcluster_desc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcluster_desc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msep_desc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msep_desc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubspace_sep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubspace_sep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbyol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbyol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    640\u001b[0m \u001b[38;5;66;03m# print(f\"GPU Memory Usage: 0:{torch.cuda.memory_allocated(0) / 1024**2:.2f} MB, 1:{torch.cuda.memory_allocated(1) / 1024**2:.2f} MB\")\u001b[39;00m\n\u001b[1;32m    642\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m node_name, loss_value \u001b[38;5;129;01min\u001b[39;00m class_loss_dict\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m~/projects/PIPNet/pipnet/train.py:1738\u001b[0m, in \u001b[0;36mcalculate_loss\u001b[0;34m(epoch, net, additional_network_outputs, features, proto_features, pooled, out, ys, align_weight, align_pf_weight, t_weight, mm_weight, unif_weight, cl_weight, OOD_loss_weight, orth_weight, cluster_desc_weight, sep_desc_weight, subspace_sep_weight, byol_weight, net_normalization_multiplier, pretrain, finetune, criterion, train_iter, print, EPS, root, label2name, node_accuracy, OOD_loss_required, kernel_orth, tanh_desc, align, uni, align_pf, tanh, minmaximize, cluster_desc, sep_desc, subspace_sep, byol, train, args, device)\u001b[0m\n\u001b[1;32m   1734\u001b[0m                 train_iter\u001b[38;5;241m.\u001b[39mset_postfix_str(\n\u001b[1;32m   1735\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mL:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,LC:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_class_loss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, L_OVSP:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_overspecifity_loss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, L_MASKL1:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_mask_l1_loss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, L_ORTH:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_kernel_orth_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, LT_DESC:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_tanh_desc_loss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, L_OOD_ENT:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_OOD_ent_loss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, losses_used:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(losses_used)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, refresh\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1736\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1737\u001b[0m                 train_iter\u001b[38;5;241m.\u001b[39mset_postfix_str(\n\u001b[0;32m-> 1738\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mL:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,LC:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_class_loss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, L_OVSP:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_overspecifity_loss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, L_MASKL1:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_mask_l1_loss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, LA_PF:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_a_loss_pf\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, LT:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_tanh_loss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, L_ORTH:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_kernel_orth_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, LT_DESC:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_tanh_desc_loss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, L_OOD_ENT:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_OOD_ent_loss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, losses_used:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(losses_used)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, refresh\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)            \n\u001b[1;32m   1740\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss, class_loss, a_loss, tanh_loss, minmaximize_loss, OOD_loss, kernel_orth_loss, uni_loss, avg_class_loss, avg_a_loss_pf, avg_tanh_loss, avg_minmaximize_loss, avg_OOD_loss, avg_kernel_orth_loss, byol_loss\u001b[38;5;241m.\u001b[39mitem(), avg_cluster_desc_loss\u001b[38;5;241m.\u001b[39mitem(), avg_sep_desc_loss\u001b[38;5;241m.\u001b[39mitem(), avg_tanh_desc_loss\u001b[38;5;241m.\u001b[39mitem(), avg_subspace_sep_loss\u001b[38;5;241m.\u001b[39mitem(), avg_conc_log_ip_loss\u001b[38;5;241m.\u001b[39mitem(), acc\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'item'"
     ]
    }
   ],
   "source": [
    "# testloader\n",
    "\n",
    "test_info, log_dict = test_pipnet(net, testloader, optimizer_net, optimizer_classifier, \\\n",
    "                                    scheduler_net, scheduler_classifier, criterion, 0, \\\n",
    "                                        args.epochs, device, pretrain=False, finetune=False, \\\n",
    "                                        test_loader_OOD=testloader_OOD, kernel_orth=args.kernel_orth == 'y', \\\n",
    "                                            tanh_desc=args.tanh_desc == 'y', align=args.align == 'y', uni=args.uni == 'y', align_pf=args.align_pf == 'y',\\\n",
    "                                            minmaximize=args.minmaximize == 'y', wandb_run=wandb_run, pretrain_epochs=args.epochs_pretrain, log=log, \\\n",
    "                                          args=args, apply_overspecificity_mask=False, path_prob_softmax_tau=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179f3017-a4a7-418a-8a25-496de314ceca",
   "metadata": {},
   "outputs": [],
   "source": [
    "round(test_info['fine_accuracy'], 4)\n",
    "test_info['fine_accuracy']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10f6a5d-d5b6-41f3-9da2-9ca115fa5855",
   "metadata": {},
   "source": [
    "# Geometric mean accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c4bff115-cb1e-44af-b08e-8126a4e40d97",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22/22 [07:48<00:00, 21.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: tensor(0.7003)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "leafname_to_depth = {}\n",
    "leafname_to_idx = testloader.dataset.class_to_idx\n",
    "\n",
    "\n",
    "\n",
    "def get_node_depth(node):\n",
    "    depth = 0\n",
    "    while node.parent:\n",
    "        depth += 1\n",
    "        node = node.parent\n",
    "    return depth\n",
    "\n",
    "for leaf_name in root.leaf_descendents:\n",
    "    leaf_node = root.get_node(leaf_name)\n",
    "    leafname_to_depth[leaf_name] = get_node_depth(leaf_node)\n",
    "\n",
    "def get_path_prob_list(out_probs, data_idx, root, leaf_node):\n",
    "    node = leaf_node\n",
    "    path_prob_list = []\n",
    "    while node.parent:\n",
    "        parent = node.parent\n",
    "        child_class_idx = parent.children_to_labels[node.name]\n",
    "        path_prob_list.append(out_probs[parent.name][data_idx, child_class_idx])\n",
    "        node = parent\n",
    "    return path_prob_list\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (xs, ys) in tqdm(enumerate(testloader), total=len(testloader)):\n",
    "\n",
    "        features, proto_features, pooled, out = net(xs)\n",
    "        out_probs = {}\n",
    "        for node_name, output_logits in out.items():\n",
    "            out_probs[node_name] = torch.softmax(output_logits, dim=1)\n",
    "\n",
    "        batch_leaf_probs_list = []\n",
    "        for data_idx in range(xs.shape[0]):\n",
    "            leaf_probs = [0 for _ in range(len(leafname_to_idx))]\n",
    "            for leafname, class_idx in leafname_to_idx.items():\n",
    "                leaf_node = root.get_node(leafname)\n",
    "                path_prob_list = get_path_prob_list(out_probs, data_idx, root, leaf_node)\n",
    "                depth = leafname_to_depth[leafname]\n",
    "                leaf_probs[class_idx] = torch.tensor([prob**(1/depth) for prob in path_prob_list]).prod()\n",
    "            batch_leaf_probs_list.append(torch.tensor(leaf_probs))\n",
    "        batch_leaf_probs = torch.stack(batch_leaf_probs_list)\n",
    "\n",
    "        correct += (batch_leaf_probs.argmax(dim=1) == ys).sum()\n",
    "        total += ys.shape[0]\n",
    "\n",
    "print('Accuracy:', correct / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "01c792a6-0923-4dc6-8b39-b3e92d47d71e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(testloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449a56f4-b599-437e-b216-bd9ff22b2a61",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Leave one out accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de75a068-908d-454a-b5a4-1b5b3e441cae",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Remove classes from dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "caed7bac-9b1a-4f96-8945-75427c659692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testloader Unique Labels: {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189}\n",
      "testloader total_samples: 5512\n",
      "classes_to_keep ['cub_112_Great_Grey_Shrike']\n",
      "leave_out_loader Unique Labels: {104}\n",
      "leave_out_loader total_samples: 30\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Sampler, SubsetRandomSampler\n",
    "\n",
    "def create_filtered_dataloader(dataloader, new_sampler, batch_size):\n",
    "    if type(dataloader.dataset) == ImageFolder:\n",
    "        dataset = dataloader.dataset\n",
    "    else:\n",
    "        dataset = dataloader.dataset.dataset.dataset\n",
    "    new_dataloader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        sampler=new_sampler,\n",
    "        num_workers=dataloader.num_workers,\n",
    "        pin_memory=dataloader.pin_memory,\n",
    "        drop_last=dataloader.drop_last,\n",
    "        timeout=dataloader.timeout,\n",
    "        worker_init_fn=dataloader.worker_init_fn,\n",
    "        multiprocessing_context=dataloader.multiprocessing_context,\n",
    "        generator=dataloader.generator,\n",
    "        prefetch_factor=dataloader.prefetch_factor,\n",
    "        persistent_workers=dataloader.persistent_workers\n",
    "    )\n",
    "    return new_dataloader\n",
    "\n",
    "leave_out_loader = testloader\n",
    "# leave_out_loader = test_projectloader\n",
    "unique_labels = set()\n",
    "total_samples = 0\n",
    "for xs, ys in testloader:\n",
    "    unique_labels.update(ys.tolist())\n",
    "    total_samples += xs.shape[0]\n",
    "print(\"testloader Unique Labels:\", unique_labels)\n",
    "print(\"testloader total_samples:\", total_samples)\n",
    "\n",
    "# leave_out_classes = args.leave_out_classes.split(',')[2]\n",
    "\n",
    "class_of_interest = 7\n",
    "classes_to_keep = [leave_out_classes[class_of_interest]]\n",
    "\n",
    "# classes_to_keep = [leave_out_classes[1], leave_out_classes[6], leave_out_classes[8]]\n",
    "# classes_to_keep = [leave_out_classes[0], leave_out_classes[1], leave_out_classes[2], leave_out_classes[3], leave_out_classes[6], leave_out_classes[7]]\n",
    "\n",
    "\n",
    "# classes_to_keep = leave_out_classes\n",
    "\n",
    "# leave_out_classes = ['cub_093_Clark_Nutcracker']\n",
    "\n",
    "print('classes_to_keep', classes_to_keep)\n",
    "idx_of_classes_to_keep = set()\n",
    "name2label = leave_out_loader.dataset.class_to_idx # param\n",
    "label2name = {label:name for name, label in name2label.items()}\n",
    "for label in label2name:\n",
    "    # NOTE: Keeping the left out classes here\n",
    "    if label2name[label] in classes_to_keep:\n",
    "        idx_of_classes_to_keep.add(label)\n",
    "\n",
    "target_indices = []\n",
    "for i in range(len(leave_out_loader.dataset)):\n",
    "    *_, label = leave_out_loader.dataset[i]\n",
    "    if label in idx_of_classes_to_keep:\n",
    "        target_indices.append(i)\n",
    "sampler = SubsetRandomSampler(target_indices)\n",
    "to_shuffle = False\n",
    "    \n",
    "leave_out_loader = create_filtered_dataloader(leave_out_loader, sampler, batch_size=256) # dataloader.batch_size\n",
    "unique_labels = set()\n",
    "total_samples = 0\n",
    "for xs, ys in leave_out_loader:\n",
    "    unique_labels.update(ys.tolist())\n",
    "    total_samples += xs.shape[0]\n",
    "print(\"leave_out_loader Unique Labels:\", unique_labels)\n",
    "print(\"leave_out_loader total_samples:\", total_samples)\n",
    "\n",
    "name2label = leave_out_loader.dataset.class_to_idx\n",
    "label2name = {label:name for name, label in name2label.items()}\n",
    "\n",
    "# print('Leave out classes', args.leave_out_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2b4aae4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# leave_out_loader.dataset.class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b79ee1fb-5bdf-44d1-ad64-bdeafa2b449f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'cl_weight' not in args:\n",
    "    args.cl_weight = 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d131c4ec-83d8-451a-b02c-42ed8841130a",
   "metadata": {},
   "source": [
    "## Leave out without mask pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e724d2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Epoch0: 100% 1/1 [00:00<00:00,  1.32it/s, L:0.146,LC:0.760, L_OVSP:-0.004, L_MASKL1:0.001, LA_PF:0.011, LT:-5.000, L_ORTH:0.001, LT_DESC:-5.000, L_OOD_ENT:-5.000, losses_used:MASK_PRUNING+MIN_CONT+AL_PF+KO+CL]\n",
      "/home/harishbabu/.conda/envs/hpnet4/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/harishbabu/.conda/envs/hpnet4/lib/python3.9/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tFine accuracy: 0.57\n",
      "\tNode name: root, acc: 100.0, f1:100.0, samples: 60, 129+024+067=60/60=1.0, 089+046=0/0=nan\n",
      "\tNode name: 129+024+067, acc: 86.67, f1:92.86, samples: 60, 129+065=52/60=0.87, 024+051=0/0=nan, 067+070=0/0=nan\n",
      "\tNode name: 129+065, acc: 100.0, f1:100.0, samples: 60, 129+192=60/60=1.0, 065+006=0/0=nan\n",
      "\tNode name: 129+192, acc: 100.0, f1:100.0, samples: 60, 129+043=60/60=1.0, 192+081=0/0=nan\n",
      "\tNode name: 129+043, acc: 70.0, f1:82.35, samples: 60, 129+018=42/60=0.7, 043+078=0/0=nan\n",
      "\tNode name: 129+018, acc: 100.0, f1:100.0, samples: 60, 129+107=60/60=1.0, cub_018_Spotted_Catbird=0/0=nan\n",
      "\tNode name: 129+107, acc: 96.67, f1:98.31, samples: 60, 129+136=0/0=nan, 107+151=58/60=0.97\n",
      "\tNode name: 107+151, acc: 100.0, f1:100.0, samples: 60, 107+111=60/60=1.0, 151+153=0/0=nan\n",
      "\tNode name: 107+111, acc: 96.67, f1:98.31, samples: 60, 107+073=0/0=nan, 111+112=58/60=0.97\n",
      "\tNode name: 111+112, acc: 0.0, f1:0.0, samples: 60, cub_111_Loggerhead_Shrike=0/0=nan, cub_112_Great_Grey_Shrike=0/60=0.0\n"
     ]
    }
   ],
   "source": [
    "# testloader\n",
    "\n",
    "test_info, log_dict = test_pipnet(net, leave_out_loader, optimizer_net, optimizer_classifier, \\\n",
    "                                    scheduler_net, scheduler_classifier, criterion, 0, \\\n",
    "                                        args.epochs, device, pretrain=False, finetune=False, \\\n",
    "                                        test_loader_OOD=testloader_OOD, kernel_orth=args.kernel_orth == 'y', \\\n",
    "                                            tanh_desc=args.tanh_desc == 'y', align=args.align == 'y', uni=args.uni == 'y', align_pf=args.align_pf == 'y',\\\n",
    "                                            minmaximize=args.minmaximize == 'y', wandb_run=wandb_run, pretrain_epochs=args.epochs_pretrain, log=log, \\\n",
    "                                          args=args, apply_overspecificity_mask=False, leave_out_classes=leave_out_classes, path_prob_softmax_tau=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0562137d-d40c-453a-b950-87626d5abcf1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cub_030_Fish_Crow',\n",
       " 'cub_198_Rock_Wren',\n",
       " 'cub_147_Least_Tern',\n",
       " 'cub_014_Indigo_Bunting',\n",
       " 'cub_083_White_breasted_Kingfisher',\n",
       " 'cub_138_Tree_Swallow',\n",
       " 'cub_185_Bohemian_Waxwing',\n",
       " 'cub_112_Great_Grey_Shrike']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(test_info['fine_accuracy'], 2)\n",
    "leave_out_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0ea6aab4-9851-4c8c-a7ed-60b0ff34c3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test_info['node_accuracy'].values()\n",
    "\n",
    "# # print(leave_out_classes[class_of_interest])\n",
    "\n",
    "# # for key in test_info['node_accuracy']:\n",
    "# #     if leave_out_classes[class_of_interest] in modified_root.get_node(key).leaf_descendents:\n",
    "# #         print(key, test_info['node_accuracy'][key]['accuracy'])\n",
    "\n",
    "# print(leave_out_classes[class_of_interest])\n",
    "# leave_out_accuracy = 1\n",
    "# for key in test_info['node_accuracy']:\n",
    "#     if leave_out_classes[class_of_interest] in root.get_node(key).leaf_descendents:\n",
    "#         if not (leave_out_classes[class_of_interest] in [child.name for child in root.get_node(key).children]):\n",
    "#             leave_out_accuracy *= test_info['node_accuracy'][key]['accuracy'] / 100\n",
    "#         print(key, test_info['node_accuracy'][key]['accuracy'])\n",
    "\n",
    "# print(leave_out_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dec387d2-56a5-4046-b426-2721ed40d729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classes_to_keep ['cub_030_Fish_Crow']\n",
      "leave_out_loader Unique Labels: {28}\n",
      "leave_out_loader total_samples: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Epoch0: 100% 1/1 [00:00<00:00,  1.82it/s, L:0.039,LC:0.626, L_OVSP:-0.006, L_MASKL1:0.001, LA_PF:0.012, LT:-5.000, L_ORTH:0.005, LT_DESC:-5.000, L_OOD_ENT:-5.000, losses_used:MASK_PRUNING+MIN_CONT+AL_PF+KO+CL]\n",
      "/home/harishbabu/.conda/envs/hpnet4/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/harishbabu/.conda/envs/hpnet4/lib/python3.9/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tFine accuracy: 0.5333\n",
      "\tNode name: root, acc: 96.67, f1:98.31, samples: 60, 129+024+067=58/60=0.97, 089+046=0/0=nan\n",
      "\tNode name: 129+024+067, acc: 63.33, f1:77.55, samples: 60, 129+065=38/60=0.63, 024+051=0/0=nan, 067+070=0/0=nan\n",
      "\tNode name: 129+065, acc: 86.67, f1:92.86, samples: 60, 129+192=52/60=0.87, 065+006=0/0=nan\n",
      "\tNode name: 129+192, acc: 100.0, f1:100.0, samples: 60, 129+043=60/60=1.0, 192+081=0/0=nan\n",
      "\tNode name: 129+043, acc: 100.0, f1:100.0, samples: 60, 129+018=60/60=1.0, 043+078=0/0=nan\n",
      "\tNode name: 129+018, acc: 96.67, f1:98.31, samples: 60, 129+107=58/60=0.97, cub_018_Spotted_Catbird=0/0=nan\n",
      "\tNode name: 129+107, acc: 83.33, f1:90.91, samples: 60, 129+136=0/0=nan, 107+151=50/60=0.83\n",
      "\tNode name: 107+151, acc: 100.0, f1:100.0, samples: 60, 107+111=60/60=1.0, 151+153=0/0=nan\n",
      "\tNode name: 107+111, acc: 100.0, f1:100.0, samples: 60, 107+073=60/60=1.0, 111+112=0/0=nan\n",
      "\tNode name: 107+073, acc: 100.0, f1:100.0, samples: 60, 107+093=60/60=1.0, 073+074=0/0=nan\n",
      "\tNode name: 107+093, acc: 100.0, f1:100.0, samples: 60, 107+030=60/60=1.0, cub_093_Clark_Nutcracker=0/0=nan\n",
      "\tNode name: 107+030, acc: 0.0, f1:0.0, samples: 60, 107+029=0/0=nan, cub_030_Fish_Crow=0/60=0.0\n",
      "cub_030_Fish_Crow 0.53 ----------\n",
      "\n",
      "\n",
      "classes_to_keep ['cub_198_Rock_Wren']\n",
      "leave_out_loader Unique Labels: {187}\n",
      "leave_out_loader total_samples: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Epoch0: 100% 1/1 [00:00<00:00,  1.84it/s, L:0.044,LC:0.554, L_OVSP:-0.005, L_MASKL1:0.001, LA_PF:0.010, LT:-5.000, L_ORTH:0.005, LT_DESC:-5.000, L_OOD_ENT:-5.000, losses_used:MASK_PRUNING+MIN_CONT+AL_PF+KO+CL]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tFine accuracy: 0.5333\n",
      "\tNode name: root, acc: 100.0, f1:100.0, samples: 60, 129+024+067=60/60=1.0, 089+046=0/0=nan\n",
      "\tNode name: 129+024+067, acc: 83.33, f1:90.91, samples: 60, 129+065=50/60=0.83, 024+051=0/0=nan, 067+070=0/0=nan\n",
      "\tNode name: 129+065, acc: 100.0, f1:100.0, samples: 60, 129+192=60/60=1.0, 065+006=0/0=nan\n",
      "\tNode name: 129+192, acc: 100.0, f1:100.0, samples: 60, 129+043=60/60=1.0, 192+081=0/0=nan\n",
      "\tNode name: 129+043, acc: 90.0, f1:94.74, samples: 60, 129+018=54/60=0.9, 043+078=0/0=nan\n",
      "\tNode name: 129+018, acc: 100.0, f1:100.0, samples: 60, 129+107=60/60=1.0, cub_018_Spotted_Catbird=0/0=nan\n",
      "\tNode name: 129+107, acc: 100.0, f1:100.0, samples: 60, 129+136=60/60=1.0, 107+151=0/0=nan\n",
      "\tNode name: 129+136, acc: 100.0, f1:100.0, samples: 60, 129+199=60/60=1.0, 136+085=0/0=nan\n",
      "\tNode name: 129+199, acc: 80.0, f1:88.89, samples: 60, 129+118=0/0=nan, 199+186=48/60=0.8\n",
      "\tNode name: 199+186, acc: 100.0, f1:100.0, samples: 60, 199+150=60/60=1.0, 186+185=0/0=nan\n",
      "\tNode name: 199+150, acc: 76.67, f1:86.79, samples: 60, 199+094=46/60=0.77, 150+019=0/0=nan\n",
      "\tNode name: 199+094, acc: 100.0, f1:100.0, samples: 60, 199+028=60/60=1.0, cub_094_White_breasted_Nuthatch=0/0=nan\n",
      "\tNode name: 199+028, acc: 100.0, f1:100.0, samples: 60, 199+198=60/60=1.0, cub_028_Brown_Creeper=0/0=nan\n",
      "\tNode name: 199+198, acc: 0.0, f1:0.0, samples: 60, 199+194=0/0=nan, cub_198_Rock_Wren=0/60=0.0\n",
      "cub_198_Rock_Wren 0.53 ----------\n",
      "\n",
      "\n",
      "classes_to_keep ['cub_014_Indigo_Bunting']\n",
      "leave_out_loader Unique Labels: {13}\n",
      "leave_out_loader total_samples: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Epoch0: 100% 1/1 [00:00<00:00,  1.84it/s, L:0.025,LC:0.409, L_OVSP:-0.004, L_MASKL1:0.001, LA_PF:0.008, LT:-5.000, L_ORTH:0.004, LT_DESC:-5.000, L_OOD_ENT:-5.000, losses_used:MASK_PRUNING+MIN_CONT+AL_PF+KO+CL]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tFine accuracy: 0.9667\n",
      "\tNode name: root, acc: 100.0, f1:100.0, samples: 60, 129+024+067=60/60=1.0, 089+046=0/0=nan\n",
      "\tNode name: 129+024+067, acc: 100.0, f1:100.0, samples: 60, 129+065=60/60=1.0, 024+051=0/0=nan, 067+070=0/0=nan\n",
      "\tNode name: 129+065, acc: 100.0, f1:100.0, samples: 60, 129+192=60/60=1.0, 065+006=0/0=nan\n",
      "\tNode name: 129+192, acc: 100.0, f1:100.0, samples: 60, 129+043=60/60=1.0, 192+081=0/0=nan\n",
      "\tNode name: 129+043, acc: 100.0, f1:100.0, samples: 60, 129+018=60/60=1.0, 043+078=0/0=nan\n",
      "\tNode name: 129+018, acc: 100.0, f1:100.0, samples: 60, 129+107=60/60=1.0, cub_018_Spotted_Catbird=0/0=nan\n",
      "\tNode name: 129+107, acc: 100.0, f1:100.0, samples: 60, 129+136=60/60=1.0, 107+151=0/0=nan\n",
      "\tNode name: 129+136, acc: 100.0, f1:100.0, samples: 60, 129+199=60/60=1.0, 136+085=0/0=nan\n",
      "\tNode name: 129+199, acc: 100.0, f1:100.0, samples: 60, 129+118=60/60=1.0, 199+186=0/0=nan\n",
      "\tNode name: 129+118, acc: 100.0, f1:100.0, samples: 60, 129+104=60/60=1.0, cub_118_House_Sparrow=0/0=nan\n",
      "\tNode name: 129+104, acc: 100.0, f1:100.0, samples: 60, 129+035=60/60=1.0, cub_104_American_Pipit=0/0=nan\n",
      "\tNode name: 129+035, acc: 100.0, f1:100.0, samples: 60, 129+054=60/60=1.0, 035+055=0/0=nan\n",
      "\tNode name: 129+054, acc: 90.0, f1:94.74, samples: 60, 129+175=0/0=nan, 054+140=54/60=0.9\n",
      "\tNode name: 054+140, acc: 100.0, f1:100.0, samples: 60, 054+057=60/60=1.0, 140+017=0/0=nan\n",
      "\tNode name: 054+057, acc: 100.0, f1:100.0, samples: 60, 054+014=60/60=1.0, cub_057_Rose_breasted_Grosbeak=0/0=nan\n",
      "\tNode name: 054+014, acc: 0.0, f1:0.0, samples: 60, 054+016=0/0=nan, cub_014_Indigo_Bunting=0/60=0.0\n",
      "cub_014_Indigo_Bunting 0.97 ----------\n",
      "\n",
      "\n",
      "classes_to_keep ['cub_185_Bohemian_Waxwing']\n",
      "leave_out_loader Unique Labels: {174}\n",
      "leave_out_loader total_samples: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Epoch0: 100% 1/1 [00:00<00:00,  1.92it/s, L:0.055,LC:0.665, L_OVSP:-0.004, L_MASKL1:0.001, LA_PF:0.010, LT:-5.000, L_ORTH:0.005, LT_DESC:-5.000, L_OOD_ENT:-5.000, losses_used:MASK_PRUNING+MIN_CONT+AL_PF+KO+CL]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tFine accuracy: 0.7\n",
      "\tNode name: root, acc: 90.0, f1:94.74, samples: 60, 129+024+067=54/60=0.9, 089+046=0/0=nan\n",
      "\tNode name: 129+024+067, acc: 90.0, f1:94.74, samples: 60, 129+065=54/60=0.9, 024+051=0/0=nan, 067+070=0/0=nan\n",
      "\tNode name: 129+065, acc: 100.0, f1:100.0, samples: 60, 129+192=60/60=1.0, 065+006=0/0=nan\n",
      "\tNode name: 129+192, acc: 100.0, f1:100.0, samples: 60, 129+043=60/60=1.0, 192+081=0/0=nan\n",
      "\tNode name: 129+043, acc: 93.33, f1:96.55, samples: 60, 129+018=56/60=0.93, 043+078=0/0=nan\n",
      "\tNode name: 129+018, acc: 93.33, f1:96.55, samples: 60, 129+107=56/60=0.93, cub_018_Spotted_Catbird=0/0=nan\n",
      "\tNode name: 129+107, acc: 93.33, f1:96.55, samples: 60, 129+136=56/60=0.93, 107+151=0/0=nan\n",
      "\tNode name: 129+136, acc: 80.0, f1:88.89, samples: 60, 129+199=48/60=0.8, 136+085=0/0=nan\n",
      "\tNode name: 129+199, acc: 90.0, f1:94.74, samples: 60, 129+118=0/0=nan, 199+186=54/60=0.9\n",
      "\tNode name: 199+186, acc: 96.67, f1:98.31, samples: 60, 199+150=0/0=nan, 186+185=58/60=0.97\n",
      "\tNode name: 186+185, acc: 0.0, f1:0.0, samples: 60, cub_186_Cedar_Waxwing=0/0=nan, cub_185_Bohemian_Waxwing=0/60=0.0\n",
      "cub_185_Bohemian_Waxwing 0.7 ----------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Sampler, SubsetRandomSampler\n",
    "\n",
    "def create_filtered_dataloader(dataloader, new_sampler, batch_size):\n",
    "    if type(dataloader.dataset) == ImageFolder:\n",
    "        dataset = dataloader.dataset\n",
    "    else:\n",
    "        dataset = dataloader.dataset.dataset.dataset\n",
    "    new_dataloader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        sampler=new_sampler,\n",
    "        num_workers=dataloader.num_workers,\n",
    "        pin_memory=dataloader.pin_memory,\n",
    "        drop_last=dataloader.drop_last,\n",
    "        timeout=dataloader.timeout,\n",
    "        worker_init_fn=dataloader.worker_init_fn,\n",
    "        multiprocessing_context=dataloader.multiprocessing_context,\n",
    "        generator=dataloader.generator,\n",
    "        prefetch_factor=dataloader.prefetch_factor,\n",
    "        persistent_workers=dataloader.persistent_workers\n",
    "    )\n",
    "    return new_dataloader\n",
    "\n",
    "leave_out_loader = testloader\n",
    "\n",
    "for left_out_class in leave_out_classes:\n",
    "\n",
    "    classes_to_keep = [left_out_class]\n",
    "    \n",
    "    print('classes_to_keep', classes_to_keep)\n",
    "    idx_of_classes_to_keep = set()\n",
    "    name2label = leave_out_loader.dataset.class_to_idx # param\n",
    "    label2name = {label:name for name, label in name2label.items()}\n",
    "    for label in label2name:\n",
    "        # NOTE: Keeping the left out classes here\n",
    "        if label2name[label] in classes_to_keep:\n",
    "            idx_of_classes_to_keep.add(label)\n",
    "    \n",
    "    target_indices = []\n",
    "    for i in range(len(leave_out_loader.dataset)):\n",
    "        *_, label = leave_out_loader.dataset[i]\n",
    "        if label in idx_of_classes_to_keep:\n",
    "            target_indices.append(i)\n",
    "    sampler = SubsetRandomSampler(target_indices)\n",
    "    to_shuffle = False\n",
    "        \n",
    "    leave_out_loader = create_filtered_dataloader(leave_out_loader, sampler, batch_size=256) # dataloader.batch_size\n",
    "    unique_labels = set()\n",
    "    total_samples = 0\n",
    "    for xs, ys in leave_out_loader:\n",
    "        unique_labels.update(ys.tolist())\n",
    "        total_samples += xs.shape[0]\n",
    "    print(\"leave_out_loader Unique Labels:\", unique_labels)\n",
    "    print(\"leave_out_loader total_samples:\", total_samples)\n",
    "    \n",
    "    name2label = leave_out_loader.dataset.class_to_idx\n",
    "    label2name = {label:name for name, label in name2label.items()}\n",
    "    \n",
    "    # print('Leave out classes', args.leave_out_classes)\n",
    "\n",
    "    # testloader\n",
    "\n",
    "    test_info, log_dict = test_pipnet(net, leave_out_loader, optimizer_net, optimizer_classifier, \\\n",
    "                                        scheduler_net, scheduler_classifier, criterion, 0, \\\n",
    "                                            args.epochs, device, pretrain=False, finetune=False, \\\n",
    "                                            test_loader_OOD=testloader_OOD, kernel_orth=args.kernel_orth == 'y', \\\n",
    "                                                tanh_desc=args.tanh_desc == 'y', align=args.align == 'y', uni=args.uni == 'y', align_pf=args.align_pf == 'y',\\\n",
    "                                                minmaximize=args.minmaximize == 'y', wandb_run=wandb_run, pretrain_epochs=args.epochs_pretrain, log=log, \\\n",
    "                                              args=args, apply_overspecificity_mask=False, leave_out_classes=classes_to_keep, path_prob_softmax_tau=1)\n",
    "    print(left_out_class, round(test_info['fine_accuracy'], 2), '-'*10)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de034c67-9ede-412b-be53-789559a47892",
   "metadata": {},
   "source": [
    "## Leave out with mask pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a6c9c7f4-6a58-408f-88f4-410a0bd10f1e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Epoch0: 100% 2/2 [00:03<00:00,  1.62s/it, L:1.455,LC:1.571, L_OVSP:-0.006, L_MASKL1:0.002, LA_PF:0.019, LT:-5.000, L_ORTH:0.002, LT_DESC:-5.000, L_OOD_ENT:0.550, losses_used:MASK_PRUNING+MIN_CONT+AL_PF+KO+CL+OOD_ENT]\n",
      "/home/harishbabu/.conda/envs/hpnet4/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/harishbabu/.conda/envs/hpnet4/lib/python3.9/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tFine accuracy: 0.0\n",
      "\tNode name: root, acc: 0.0, f1:0.0, samples: 580, 129+024+067=0/580=0.0, 089+046=0/0=nan\n",
      "\tNode name: 129+024+067, acc: 24.48, f1:11.8, samples: 580, 129+065=0/418=0.0, 024+051=142/162=0.88, 067+070=0/0=nan\n",
      "\tNode name: 089+046, acc: inf, f1:inf, samples: 0, 089+090=0/0=nan, 046+087=0/0=nan\n",
      "\tNode name: 129+065, acc: 71.29, f1:59.34, samples: 418, 129+192=298/298=1.0, 065+006=0/120=0.0\n",
      "\tNode name: 024+051, acc: 37.04, f1:20.02, samples: 162, 024+031=0/102=0.0, 051+052=60/60=1.0\n",
      "\tNode name: 067+070, acc: inf, f1:inf, samples: 0, 067+068=0/0=nan, cub_070_Green_Violetear=0/0=nan\n",
      "\tNode name: 089+090, acc: inf, f1:inf, samples: 0, cub_089_Hooded_Merganser=0/0=nan, cub_090_Red_breasted_Merganser=0/0=nan\n",
      "\tNode name: 046+087, acc: inf, f1:inf, samples: 0, cub_046_Gadwall=0/0=nan, cub_087_Mallard=0/0=nan\n",
      "\tNode name: 129+192, acc: 20.13, f1:6.75, samples: 298, 129+043=0/238=0.0, 192+081=60/60=1.0\n",
      "\tNode name: 065+006, acc: 91.67, f1:95.65, samples: 120, 065+144=110/120=0.92, 006+071=0/0=nan\n",
      "\tNode name: 024+031, acc: 96.08, f1:96.06, samples: 102, 024+086=56/56=1.0, 031+004=42/46=0.91\n",
      "\tNode name: 051+052, acc: 100.0, f1:100.0, samples: 60, 051+053=60/60=1.0, cub_052_Pied_billed_Grebe=0/0=nan\n",
      "\tNode name: 067+068, acc: inf, f1:inf, samples: 0, 067+069=0/0=nan, cub_068_Ruby_throated_Hummingbird=0/0=nan\n",
      "\tNode name: 129+043, acc: 0.0, f1:0.0, samples: 238, 129+018=0/238=0.0, 043+078=0/0=nan\n",
      "\tNode name: 192+081, acc: 53.33, f1:69.57, samples: 60, 192+036=32/60=0.53, 081+083=0/0=nan\n",
      "\tNode name: 065+144, acc: 100.0, f1:100.0, samples: 120, 065+084=60/60=1.0, 144+147=60/60=1.0\n",
      "\tNode name: 006+071, acc: inf, f1:inf, samples: 0, 006+058=0/0=nan, 071+072=0/0=nan\n",
      "\tNode name: 024+086, acc: 89.29, f1:94.34, samples: 56, 024+001=50/56=0.89, cub_086_Pacific_Loon=0/0=nan\n",
      "\tNode name: 031+004, acc: 95.65, f1:97.78, samples: 46, 031+032=44/46=0.96, cub_004_Groove_billed_Ani=0/0=nan\n",
      "\tNode name: 051+053, acc: 80.0, f1:88.89, samples: 60, 051+050=48/60=0.8, cub_053_Western_Grebe=0/0=nan\n",
      "\tNode name: 067+069, acc: inf, f1:inf, samples: 0, cub_067_Anna_Hummingbird=0/0=nan, cub_069_Rufous_Hummingbird=0/0=nan\n",
      "\tNode name: 129+018, acc: 0.0, f1:0.0, samples: 238, 129+107=0/238=0.0, cub_018_Spotted_Catbird=0/0=nan\n",
      "\tNode name: 043+078, acc: inf, f1:inf, samples: 0, 043+042=0/0=nan, 078+038=0/0=nan\n",
      "\tNode name: 192+036, acc: 56.67, f1:72.34, samples: 60, 192+191=34/60=0.57, 036+188=0/0=nan\n",
      "\tNode name: 081+083, acc: inf, f1:inf, samples: 0, 081+082=0/0=nan, cub_083_White_breasted_Kingfisher=0/0=nan\n",
      "\tNode name: 065+084, acc: 13.33, f1:23.53, samples: 60, 065+061=0/0=nan, 084+063=8/60=0.13\n",
      "\tNode name: 144+147, acc: 0.0, f1:0.0, samples: 60, 144+143=0/0=nan, cub_147_Least_Tern=0/60=0.0\n",
      "\tNode name: 006+058, acc: inf, f1:inf, samples: 0, 006+008=0/0=nan, cub_058_Pigeon_Guillemot=0/0=nan\n",
      "\tNode name: 071+072, acc: inf, f1:inf, samples: 0, cub_071_Long_tailed_Jaeger=0/0=nan, cub_072_Pomarine_Jaeger=0/0=nan\n",
      "\tNode name: 024+001, acc: 92.86, f1:96.3, samples: 56, 024+100=0/0=nan, 001+045=52/56=0.93\n",
      "\tNode name: 031+032, acc: 0.0, f1:0.0, samples: 46, 031+033=0/0=nan, cub_032_Mangrove_Cuckoo=0/46=0.0\n",
      "\tNode name: 051+050, acc: 0.0, f1:0.0, samples: 60, cub_051_Horned_Grebe=0/60=0.0, cub_050_Eared_Grebe=0/0=nan\n",
      "\tNode name: 129+107, acc: 25.21, f1:10.15, samples: 238, 129+136=0/178=0.0, 107+151=60/60=1.0\n",
      "\tNode name: 043+042, acc: inf, f1:inf, samples: 0, 043+040=0/0=nan, cub_042_Vermilion_Flycatcher=0/0=nan\n",
      "\tNode name: 078+038, acc: inf, f1:inf, samples: 0, 078+041=0/0=nan, cub_038_Great_Crested_Flycatcher=0/0=nan\n",
      "\tNode name: 192+191, acc: 96.67, f1:98.31, samples: 60, 192+187=0/0=nan, 191+189=58/60=0.97\n",
      "\tNode name: 036+188, acc: inf, f1:inf, samples: 0, cub_036_Northern_Flicker=0/0=nan, cub_188_Pileated_Woodpecker=0/0=nan\n",
      "\tNode name: 081+082, acc: inf, f1:inf, samples: 0, 081+080=0/0=nan, 082+079=0/0=nan\n",
      "\tNode name: 065+061, acc: inf, f1:inf, samples: 0, 065+066=0/0=nan, 061+064=0/0=nan\n",
      "\tNode name: 084+063, acc: 0.0, f1:0.0, samples: 60, cub_084_Red_legged_Kittiwake=0/0=nan, cub_063_Ivory_Gull=0/60=0.0\n",
      "\tNode name: 144+143, acc: inf, f1:inf, samples: 0, 144+142=0/0=nan, cub_143_Caspian_Tern=0/0=nan\n",
      "\tNode name: 006+008, acc: inf, f1:inf, samples: 0, 006+005=0/0=nan, 008+106=0/0=nan\n",
      "\tNode name: 024+100, acc: inf, f1:inf, samples: 0, 024+023=0/0=nan, 100+101=0/0=nan\n",
      "\tNode name: 001+045, acc: 89.29, f1:94.34, samples: 56, 001+003=50/56=0.89, cub_045_Northern_Fulmar=0/0=nan\n",
      "\tNode name: 031+033, acc: inf, f1:inf, samples: 0, cub_031_Black_billed_Cuckoo=0/0=nan, cub_033_Yellow_billed_Cuckoo=0/0=nan\n",
      "\tNode name: 129+136, acc: 0.0, f1:0.0, samples: 178, 129+199=0/178=0.0, 136+085=0/0=nan\n",
      "\tNode name: 107+151, acc: 100.0, f1:100.0, samples: 60, 107+111=60/60=1.0, 151+153=0/0=nan\n",
      "\tNode name: 043+040, acc: inf, f1:inf, samples: 0, 043+037=0/0=nan, 040+102=0/0=nan\n",
      "\tNode name: 078+041, acc: inf, f1:inf, samples: 0, 078+077=0/0=nan, cub_041_Scissor_tailed_Flycatcher=0/0=nan\n",
      "\tNode name: 192+187, acc: inf, f1:inf, samples: 0, 192+190=0/0=nan, cub_187_American_Three_toed_Woodpecker=0/0=nan\n",
      "\tNode name: 191+189, acc: 0.0, f1:0.0, samples: 60, cub_191_Red_headed_Woodpecker=0/60=0.0, cub_189_Red_bellied_Woodpecker=0/0=nan\n",
      "\tNode name: 081+080, acc: inf, f1:inf, samples: 0, cub_081_Pied_Kingfisher=0/0=nan, cub_080_Green_Kingfisher=0/0=nan\n",
      "\tNode name: 082+079, acc: inf, f1:inf, samples: 0, cub_082_Ringed_Kingfisher=0/0=nan, cub_079_Belted_Kingfisher=0/0=nan\n",
      "\tNode name: 065+066, acc: inf, f1:inf, samples: 0, 065+062=0/0=nan, cub_066_Western_Gull=0/0=nan\n",
      "\tNode name: 061+064, acc: inf, f1:inf, samples: 0, cub_061_Heermann_Gull=0/0=nan, cub_064_Ring_billed_Gull=0/0=nan\n",
      "\tNode name: 144+142, acc: inf, f1:inf, samples: 0, 144+145=0/0=nan, cub_142_Black_Tern=0/0=nan\n",
      "\tNode name: 006+005, acc: inf, f1:inf, samples: 0, 006+007=0/0=nan, cub_005_Crested_Auklet=0/0=nan\n",
      "\tNode name: 008+106, acc: inf, f1:inf, samples: 0, cub_008_Rhinoceros_Auklet=0/0=nan, cub_106_Horned_Puffin=0/0=nan\n",
      "\tNode name: 024+023, acc: inf, f1:inf, samples: 0, 024+025=0/0=nan, cub_023_Brandt_Cormorant=0/0=nan\n",
      "\tNode name: 100+101, acc: inf, f1:inf, samples: 0, cub_100_Brown_Pelican=0/0=nan, cub_101_White_Pelican=0/0=nan\n",
      "\tNode name: 001+003, acc: 0.0, f1:0.0, samples: 56, 001+002=0/0=nan, cub_003_Sooty_Albatross=0/56=0.0\n",
      "\tNode name: 129+199, acc: 66.29, f1:52.85, samples: 178, 129+118=0/60=0.0, 199+186=118/118=1.0\n",
      "\tNode name: 136+085, acc: inf, f1:inf, samples: 0, 136+138=0/0=nan, cub_085_Horned_Lark=0/0=nan\n",
      "\tNode name: 107+111, acc: 100.0, f1:100.0, samples: 60, 107+073=60/60=1.0, 111+112=0/0=nan\n",
      "\tNode name: 151+153, acc: inf, f1:inf, samples: 0, 151+157=0/0=nan, 153+154=0/0=nan\n",
      "\tNode name: 043+037, acc: inf, f1:inf, samples: 0, 043+039=0/0=nan, cub_037_Acadian_Flycatcher=0/0=nan\n",
      "\tNode name: 040+102, acc: inf, f1:inf, samples: 0, cub_040_Olive_sided_Flycatcher=0/0=nan, cub_102_Western_Wood_Pewee=0/0=nan\n",
      "\tNode name: 078+077, acc: inf, f1:inf, samples: 0, cub_078_Gray_Kingbird=0/0=nan, cub_077_Tropical_Kingbird=0/0=nan\n",
      "\tNode name: 192+190, acc: inf, f1:inf, samples: 0, cub_192_Downy_Woodpecker=0/0=nan, cub_190_Red_cockaded_Woodpecker=0/0=nan\n",
      "\tNode name: 065+062, acc: inf, f1:inf, samples: 0, 065+059=0/0=nan, cub_062_Herring_Gull=0/0=nan\n",
      "\tNode name: 144+145, acc: inf, f1:inf, samples: 0, 144+146=0/0=nan, cub_145_Elegant_Tern=0/0=nan\n",
      "\tNode name: 006+007, acc: inf, f1:inf, samples: 0, cub_006_Least_Auklet=0/0=nan, cub_007_Parakeet_Auklet=0/0=nan\n",
      "\tNode name: 024+025, acc: inf, f1:inf, samples: 0, cub_024_Red_faced_Cormorant=0/0=nan, cub_025_Pelagic_Cormorant=0/0=nan\n",
      "\tNode name: 001+002, acc: inf, f1:inf, samples: 0, cub_001_Black_footed_Albatross=0/0=nan, cub_002_Laysan_Albatross=0/0=nan\n",
      "\tNode name: 129+118, acc: 0.0, f1:0.0, samples: 60, 129+104=0/60=0.0, cub_118_House_Sparrow=0/0=nan\n",
      "\tNode name: 199+186, acc: 91.53, f1:95.58, samples: 118, 199+150=108/118=0.92, 186+185=0/0=nan\n",
      "\tNode name: 136+138, acc: inf, f1:inf, samples: 0, 136+137=0/0=nan, cub_138_Tree_Swallow=0/0=nan\n",
      "\tNode name: 107+073, acc: 100.0, f1:100.0, samples: 60, 107+093=60/60=1.0, 073+074=0/0=nan\n",
      "\tNode name: 111+112, acc: inf, f1:inf, samples: 0, cub_111_Loggerhead_Shrike=0/0=nan, cub_112_Great_Grey_Shrike=0/0=nan\n",
      "\tNode name: 151+157, acc: inf, f1:inf, samples: 0, 151+156=0/0=nan, 157+152=0/0=nan\n",
      "\tNode name: 153+154, acc: inf, f1:inf, samples: 0, 153+155=0/0=nan, cub_154_Red_eyed_Vireo=0/0=nan\n",
      "\tNode name: 043+039, acc: inf, f1:inf, samples: 0, cub_043_Yellow_bellied_Flycatcher=0/0=nan, cub_039_Least_Flycatcher=0/0=nan\n",
      "\tNode name: 065+059, acc: inf, f1:inf, samples: 0, 065+060=0/0=nan, cub_059_California_Gull=0/0=nan\n",
      "\tNode name: 144+146, acc: inf, f1:inf, samples: 0, 144+141=0/0=nan, cub_146_Forsters_Tern=0/0=nan\n",
      "\tNode name: 129+104, acc: 0.0, f1:0.0, samples: 60, 129+035=0/60=0.0, cub_104_American_Pipit=0/0=nan\n",
      "\tNode name: 199+150, acc: 83.05, f1:83.04, samples: 118, 199+094=48/60=0.8, 150+019=50/58=0.86\n",
      "\tNode name: 186+185, acc: inf, f1:inf, samples: 0, cub_186_Cedar_Waxwing=0/0=nan, cub_185_Bohemian_Waxwing=0/0=nan\n",
      "\tNode name: 136+137, acc: inf, f1:inf, samples: 0, cub_136_Barn_Swallow=0/0=nan, cub_137_Cliff_Swallow=0/0=nan\n",
      "\tNode name: 107+093, acc: 100.0, f1:100.0, samples: 60, 107+030=60/60=1.0, cub_093_Clark_Nutcracker=0/0=nan\n",
      "\tNode name: 073+074, acc: inf, f1:inf, samples: 0, cub_073_Blue_Jay=0/0=nan, cub_074_Florida_Jay=0/0=nan\n",
      "\tNode name: 151+156, acc: inf, f1:inf, samples: 0, cub_151_Black_capped_Vireo=0/0=nan, cub_156_White_eyed_Vireo=0/0=nan\n",
      "\tNode name: 157+152, acc: inf, f1:inf, samples: 0, cub_157_Yellow_throated_Vireo=0/0=nan, cub_152_Blue_headed_Vireo=0/0=nan\n",
      "\tNode name: 153+155, acc: inf, f1:inf, samples: 0, cub_153_Philadelphia_Vireo=0/0=nan, cub_155_Warbling_Vireo=0/0=nan\n",
      "\tNode name: 065+060, acc: inf, f1:inf, samples: 0, cub_065_Slaty_backed_Gull=0/0=nan, cub_060_Glaucous_winged_Gull=0/0=nan\n",
      "\tNode name: 144+141, acc: inf, f1:inf, samples: 0, cub_144_Common_Tern=0/0=nan, cub_141_Artic_Tern=0/0=nan\n",
      "\tNode name: 129+035, acc: 0.0, f1:0.0, samples: 60, 129+054=0/60=0.0, 035+055=0/0=nan\n",
      "\tNode name: 199+094, acc: 96.67, f1:98.31, samples: 60, 199+028=58/60=0.97, cub_094_White_breasted_Nuthatch=0/0=nan\n",
      "\tNode name: 150+019, acc: 100.0, f1:100.0, samples: 58, 150+149=58/58=1.0, cub_019_Gray_Catbird=0/0=nan\n",
      "\tNode name: 107+030, acc: 0.0, f1:0.0, samples: 60, 107+029=0/0=nan, cub_030_Fish_Crow=0/60=0.0\n",
      "\tNode name: 129+054, acc: 100.0, f1:100.0, samples: 60, 129+175=0/0=nan, 054+140=60/60=1.0\n",
      "\tNode name: 035+055, acc: inf, f1:inf, samples: 0, 035+048=0/0=nan, cub_055_Evening_Grosbeak=0/0=nan\n",
      "\tNode name: 199+028, acc: 93.33, f1:96.55, samples: 60, 199+198=56/60=0.93, cub_028_Brown_Creeper=0/0=nan\n",
      "\tNode name: 150+149, acc: 0.0, f1:0.0, samples: 58, 150+091=0/0=nan, cub_149_Brown_Thrasher=0/58=0.0\n",
      "\tNode name: 107+029, acc: inf, f1:inf, samples: 0, 107+108=0/0=nan, cub_029_American_Crow=0/0=nan\n",
      "\tNode name: 129+175, acc: inf, f1:inf, samples: 0, 129+011=0/0=nan, 175+020=0/0=nan\n",
      "\tNode name: 054+140, acc: 100.0, f1:100.0, samples: 60, 054+057=0/0=nan, 140+017=60/60=1.0\n",
      "\tNode name: 035+048, acc: inf, f1:inf, samples: 0, 035+056=0/0=nan, 048+047=0/0=nan\n",
      "\tNode name: 199+198, acc: 3.33, f1:6.45, samples: 60, 199+194=0/0=nan, cub_198_Rock_Wren=2/60=0.03\n",
      "\tNode name: 150+091, acc: inf, f1:inf, samples: 0, cub_150_Sage_Thrasher=0/0=nan, cub_091_Mockingbird=0/0=nan\n",
      "\tNode name: 107+108, acc: inf, f1:inf, samples: 0, cub_107_Common_Raven=0/0=nan, cub_108_White_necked_Raven=0/0=nan\n",
      "\tNode name: 129+011, acc: inf, f1:inf, samples: 0, 129+121=0/0=nan, 011+013=0/0=nan\n",
      "\tNode name: 175+020, acc: inf, f1:inf, samples: 0, 175+099=0/0=nan, cub_020_Yellow_breasted_Chat=0/0=nan\n",
      "\tNode name: 054+057, acc: inf, f1:inf, samples: 0, 054+014=0/0=nan, cub_057_Rose_breasted_Grosbeak=0/0=nan\n",
      "\tNode name: 140+017, acc: 83.33, f1:90.91, samples: 60, 140+139=50/60=0.83, cub_017_Cardinal=0/0=nan\n",
      "\tNode name: 035+056, acc: inf, f1:inf, samples: 0, 035+034=0/0=nan, cub_056_Pine_Grosbeak=0/0=nan\n",
      "\tNode name: 048+047, acc: inf, f1:inf, samples: 0, cub_048_European_Goldfinch=0/0=nan, cub_047_American_Goldfinch=0/0=nan\n",
      "\tNode name: 199+194, acc: inf, f1:inf, samples: 0, 199+193=0/0=nan, cub_194_Cactus_Wren=0/0=nan\n",
      "\tNode name: 129+121, acc: inf, f1:inf, samples: 0, 129+117=0/0=nan, cub_121_Grasshopper_Sparrow=0/0=nan\n",
      "\tNode name: 011+013, acc: inf, f1:inf, samples: 0, 011+095=0/0=nan, 013+088=0/0=nan\n",
      "\tNode name: 175+099, acc: inf, f1:inf, samples: 0, 175+181=0/0=nan, cub_099_Ovenbird=0/0=nan\n",
      "\tNode name: 054+014, acc: inf, f1:inf, samples: 0, 054+016=0/0=nan, cub_014_Indigo_Bunting=0/0=nan\n",
      "\tNode name: 140+139, acc: 0.0, f1:0.0, samples: 60, cub_140_Summer_Tanager=0/0=nan, cub_139_Scarlet_Tanager=0/60=0.0\n",
      "\tNode name: 035+034, acc: inf, f1:inf, samples: 0, cub_035_Purple_Finch=0/0=nan, cub_034_Gray_crowned_Rosy_Finch=0/0=nan\n",
      "\tNode name: 199+193, acc: inf, f1:inf, samples: 0, 199+197=0/0=nan, 193+195=0/0=nan\n",
      "\tNode name: 129+117, acc: inf, f1:inf, samples: 0, 129+133=0/0=nan, 117+114=0/0=nan\n",
      "\tNode name: 011+095, acc: inf, f1:inf, samples: 0, 011+026=0/0=nan, 095+096=0/0=nan\n",
      "\tNode name: 013+088, acc: inf, f1:inf, samples: 0, 013+012=0/0=nan, cub_088_Western_Meadowlark=0/0=nan\n",
      "\tNode name: 175+181, acc: inf, f1:inf, samples: 0, 175+168+173+183=0/0=nan, cub_181_Worm_eating_Warbler=0/0=nan\n",
      "\tNode name: 054+016, acc: inf, f1:inf, samples: 0, 054+015=0/0=nan, cub_016_Painted_Bunting=0/0=nan\n",
      "\tNode name: 199+197, acc: inf, f1:inf, samples: 0, 199+196=0/0=nan, cub_197_Marsh_Wren=0/0=nan\n",
      "\tNode name: 193+195, acc: inf, f1:inf, samples: 0, cub_193_Bewick_Wren=0/0=nan, cub_195_Carolina_Wren=0/0=nan\n",
      "\tNode name: 129+133, acc: inf, f1:inf, samples: 0, 129+021=0/0=nan, 133+130=0/0=nan\n",
      "\tNode name: 117+114, acc: inf, f1:inf, samples: 0, 117+115=0/0=nan, cub_114_Black_throated_Sparrow=0/0=nan\n",
      "\tNode name: 011+026, acc: inf, f1:inf, samples: 0, 011+049=0/0=nan, 026+010=0/0=nan\n",
      "\tNode name: 095+096, acc: inf, f1:inf, samples: 0, 095+098=0/0=nan, 096+097=0/0=nan\n",
      "\tNode name: 013+012, acc: inf, f1:inf, samples: 0, cub_013_Bobolink=0/0=nan, cub_012_Yellow_headed_Blackbird=0/0=nan\n",
      "\tNode name: 175+168+173+183, acc: inf, f1:inf, samples: 0, 175+162=0/0=nan, 168+177=0/0=nan, 173+161=0/0=nan, 183+159=0/0=nan\n",
      "\tNode name: 054+015, acc: inf, f1:inf, samples: 0, cub_054_Blue_Grosbeak=0/0=nan, cub_015_Lazuli_Bunting=0/0=nan\n",
      "\tNode name: 199+196, acc: inf, f1:inf, samples: 0, cub_199_Winter_Wren=0/0=nan, cub_196_House_Wren=0/0=nan\n",
      "\tNode name: 129+021, acc: inf, f1:inf, samples: 0, 129+128=0/0=nan, 021+148=0/0=nan\n",
      "\tNode name: 133+130, acc: inf, f1:inf, samples: 0, 133+076=0/0=nan, 130+120=0/0=nan\n",
      "\tNode name: 117+115, acc: inf, f1:inf, samples: 0, 117+116=0/0=nan, 115+119=0/0=nan\n",
      "\tNode name: 011+049, acc: inf, f1:inf, samples: 0, 011+009=0/0=nan, cub_049_Boat_tailed_Grackle=0/0=nan\n",
      "\tNode name: 026+010, acc: inf, f1:inf, samples: 0, 026+027=0/0=nan, cub_010_Red_winged_Blackbird=0/0=nan\n",
      "\tNode name: 095+098, acc: inf, f1:inf, samples: 0, cub_095_Baltimore_Oriole=0/0=nan, cub_098_Scott_Oriole=0/0=nan\n",
      "\tNode name: 096+097, acc: inf, f1:inf, samples: 0, cub_096_Hooded_Oriole=0/0=nan, cub_097_Orchard_Oriole=0/0=nan\n",
      "\tNode name: 175+162, acc: inf, f1:inf, samples: 0, 175+167=0/0=nan, 162+180=0/0=nan\n",
      "\tNode name: 168+177, acc: inf, f1:inf, samples: 0, 168+200=0/0=nan, 177+178=0/0=nan\n",
      "\tNode name: 173+161, acc: inf, f1:inf, samples: 0, 173+179=0/0=nan, 161+166=0/0=nan\n",
      "\tNode name: 183+159, acc: inf, f1:inf, samples: 0, 183+184=0/0=nan, cub_159_Black_and_white_Warbler=0/0=nan\n",
      "\tNode name: 129+128, acc: inf, f1:inf, samples: 0, 129+127=0/0=nan, 128+131=0/0=nan\n",
      "\tNode name: 021+148, acc: inf, f1:inf, samples: 0, cub_021_Eastern_Towhee=0/0=nan, cub_148_Green_tailed_Towhee=0/0=nan\n",
      "\tNode name: 133+076, acc: inf, f1:inf, samples: 0, 133+132=0/0=nan, cub_076_Dark_eyed_Junco=0/0=nan\n",
      "\tNode name: 130+120, acc: inf, f1:inf, samples: 0, cub_130_Tree_Sparrow=0/0=nan, cub_120_Fox_Sparrow=0/0=nan\n",
      "\tNode name: 117+116, acc: inf, f1:inf, samples: 0, cub_117_Clay_colored_Sparrow=0/0=nan, cub_116_Chipping_Sparrow=0/0=nan\n",
      "\tNode name: 115+119, acc: inf, f1:inf, samples: 0, cub_115_Brewer_Sparrow=0/0=nan, cub_119_Field_Sparrow=0/0=nan\n",
      "\tNode name: 011+009, acc: inf, f1:inf, samples: 0, cub_011_Rusty_Blackbird=0/0=nan, cub_009_Brewer_Blackbird=0/0=nan\n",
      "\tNode name: 026+027, acc: inf, f1:inf, samples: 0, cub_026_Bronzed_Cowbird=0/0=nan, cub_027_Shiny_Cowbird=0/0=nan\n",
      "\tNode name: 175+167, acc: inf, f1:inf, samples: 0, 175+169=0/0=nan, cub_167_Hooded_Warbler=0/0=nan\n",
      "\tNode name: 162+180, acc: inf, f1:inf, samples: 0, cub_162_Canada_Warbler=0/0=nan, cub_180_Wilson_Warbler=0/0=nan\n",
      "\tNode name: 168+200, acc: inf, f1:inf, samples: 0, 168+170=0/0=nan, cub_200_Common_Yellowthroat=0/0=nan\n",
      "\tNode name: 177+178, acc: inf, f1:inf, samples: 0, cub_177_Prothonotary_Warbler=0/0=nan, cub_178_Swainson_Warbler=0/0=nan\n",
      "\tNode name: 173+179, acc: inf, f1:inf, samples: 0, 173+172=0/0=nan, cub_179_Tennessee_Warbler=0/0=nan\n",
      "\tNode name: 161+166, acc: inf, f1:inf, samples: 0, cub_161_Blue_winged_Warbler=0/0=nan, cub_166_Golden_winged_Warbler=0/0=nan\n",
      "\tNode name: 183+184, acc: inf, f1:inf, samples: 0, cub_183_Northern_Waterthrush=0/0=nan, cub_184_Louisiana_Waterthrush=0/0=nan\n",
      "\tNode name: 129+127, acc: inf, f1:inf, samples: 0, 129+123=0/0=nan, cub_127_Savannah_Sparrow=0/0=nan\n",
      "\tNode name: 128+131, acc: inf, f1:inf, samples: 0, 128+124=0/0=nan, cub_131_Vesper_Sparrow=0/0=nan\n",
      "\tNode name: 133+132, acc: inf, f1:inf, samples: 0, 133+122=0/0=nan, cub_132_White_crowned_Sparrow=0/0=nan\n",
      "\tNode name: 175+169, acc: inf, f1:inf, samples: 0, 175+165+182=0/0=nan, cub_169_Magnolia_Warbler=0/0=nan\n",
      "\tNode name: 168+170, acc: inf, f1:inf, samples: 0, cub_168_Kentucky_Warbler=0/0=nan, cub_170_Mourning_Warbler=0/0=nan\n",
      "\tNode name: 173+172, acc: inf, f1:inf, samples: 0, cub_173_Orange_crowned_Warbler=0/0=nan, cub_172_Nashville_Warbler=0/0=nan\n",
      "\tNode name: 129+123, acc: inf, f1:inf, samples: 0, 129+125=0/0=nan, 123+113=0/0=nan\n",
      "\tNode name: 128+124, acc: inf, f1:inf, samples: 0, 128+126=0/0=nan, cub_124_Le_Conte_Sparrow=0/0=nan\n",
      "\tNode name: 133+122, acc: inf, f1:inf, samples: 0, cub_133_White_throated_Sparrow=0/0=nan, cub_122_Harris_Sparrow=0/0=nan\n",
      "\tNode name: 175+165+182, acc: inf, f1:inf, samples: 0, 175+160=0/0=nan, 165+164+163=0/0=nan, cub_182_Yellow_Warbler=0/0=nan\n",
      "\tNode name: 129+125, acc: inf, f1:inf, samples: 0, cub_129_Song_Sparrow=0/0=nan, cub_125_Lincoln_Sparrow=0/0=nan\n",
      "\tNode name: 123+113, acc: inf, f1:inf, samples: 0, cub_123_Henslow_Sparrow=0/0=nan, cub_113_Baird_Sparrow=0/0=nan\n",
      "\tNode name: 128+126, acc: inf, f1:inf, samples: 0, cub_128_Seaside_Sparrow=0/0=nan, cub_126_Nelson_Sharp_tailed_Sparrow=0/0=nan\n",
      "\tNode name: 175+160, acc: inf, f1:inf, samples: 0, 175+176=0/0=nan, 160+109=0/0=nan\n",
      "\tNode name: 165+164+163, acc: inf, f1:inf, samples: 0, 165+158=0/0=nan, cub_164_Cerulean_Warbler=0/0=nan, cub_163_Cape_May_Warbler=0/0=nan\n",
      "\tNode name: 175+176, acc: inf, f1:inf, samples: 0, 175+174=0/0=nan, cub_176_Prairie_Warbler=0/0=nan\n",
      "\tNode name: 160+109, acc: inf, f1:inf, samples: 0, cub_160_Black_throated_Blue_Warbler=0/0=nan, cub_109_American_Redstart=0/0=nan\n",
      "\tNode name: 165+158, acc: inf, f1:inf, samples: 0, cub_165_Chestnut_sided_Warbler=0/0=nan, cub_158_Bay_breasted_Warbler=0/0=nan\n",
      "\tNode name: 175+174, acc: inf, f1:inf, samples: 0, cub_175_Pine_Warbler=0/0=nan, cub_174_Palm_Warbler=0/0=nan\n"
     ]
    }
   ],
   "source": [
    "# testloader\n",
    "\n",
    "test_info, log_dict = test_pipnet(net, leave_out_loader, optimizer_net, optimizer_classifier, \\\n",
    "                                    scheduler_net, scheduler_classifier, criterion, 0, \\\n",
    "                                        args.epochs, device, pretrain=False, finetune=False, \\\n",
    "                                        test_loader_OOD=testloader_OOD, kernel_orth=args.kernel_orth == 'y', \\\n",
    "                                            tanh_desc=args.tanh_desc == 'y', align=args.align == 'y', uni=args.uni == 'y', align_pf=args.align_pf == 'y',\\\n",
    "                                            minmaximize=args.minmaximize == 'y', wandb_run=wandb_run, pretrain_epochs=args.epochs_pretrain, log=log, \\\n",
    "                                          args=args, apply_overspecificity_mask=True, leave_out_classes=leave_out_classes, path_prob_softmax_tau=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "77dfced7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(net.module._root_proto_presence)\n",
    "# print(net.module._root_proto_presence.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1c939568-6ca9-45c9-a4d8-b444ffafd662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# net.module._root_proto_presence[:, 0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1f15b657-6768-4f92-95ac-1219edecec44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cat([net.module._root_proto_presence[:, 1].unsqueeze(-1), net.module._root_proto_presence[:, 0].unsqueeze(-1)], dim=-1)#.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "024d55a8-209d-4325-af85-66afbe199ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.no_grad():\n",
    "#     for node in root.nodes_with_children():\n",
    "#         net.module._root_proto_presence\n",
    "#         proto_presence = getattr(net.module, '_'+node.name+'_proto_presence')\n",
    "#         reversed_proto_presence = torch.cat([proto_presence[:, 1].unsqueeze(-1), proto_presence[:, 0].unsqueeze(-1)], dim=-1)\n",
    "#         setattr(net.module, '_'+node.name+'_proto_presence', \\\n",
    "#                 nn.Parameter(reversed_proto_presence))\n",
    "\n",
    "#     test_info, log_dict = test_pipnet(net, leave_out_loader, optimizer_net, optimizer_classifier, \\\n",
    "#                                     scheduler_net, scheduler_classifier, criterion, 0, \\\n",
    "#                                         args.epochs, device, pretrain=False, finetune=False, \\\n",
    "#                                         test_loader_OOD=testloader_OOD, kernel_orth=args.kernel_orth == 'y', \\\n",
    "#                                             tanh_desc=args.tanh_desc == 'y', align=args.align == 'y', uni=args.uni == 'y', align_pf=args.align_pf == 'y',\\\n",
    "#                                             minmaximize=args.minmaximize == 'y', wandb_run=wandb_run, pretrain_epochs=args.epochs_pretrain, log=log, \\\n",
    "#                                           args=args, apply_overspecificity_mask=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a43dc275-1062-4aa9-9628-e5a9ce35c1e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([False, True, True]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46aa7a0a-f573-4cca-932a-40340dcb3954",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
